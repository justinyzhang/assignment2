{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3811895d-111d-4e29-999c-d4f48ab28bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:69: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:69: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/var/folders/y0/lhz2681516n6nt67mqv7dkpr0000gn/T/ipykernel_6506/123689798.py:69: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  df['price_numeric'] = df['price'].replace('[\\$,]', '', regex=True).astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NYC Main data from ./listingsNYC.csv...\n",
      "NYC Main dataset shape: (37434, 79)\n",
      "Number of listings: 37434\n",
      "Number of features: 79\n",
      "\n",
      "Missing values in key columns for NYC Main:\n",
      "price: 15126 missing values (40.41%)\n",
      "room_type: 0 missing values (0.00%)\n",
      "accommodates: 0 missing values (0.00%)\n",
      "bedrooms: 5911 missing values (15.79%)\n",
      "bathrooms: 14931 missing values (39.89%)\n",
      "review_scores_rating: 11787 missing values (31.49%)\n",
      "\n",
      "Price statistics for NYC Main:\n",
      "count    22308.000000\n",
      "mean       213.835216\n",
      "std        427.599435\n",
      "min          7.000000\n",
      "25%         85.000000\n",
      "50%        140.000000\n",
      "75%        240.000000\n",
      "max      20000.000000\n",
      "Name: price_numeric, dtype: float64\n",
      "Loading NYC 2024 Q1 data from ./listingsNYC2024Q1.csv...\n",
      "NYC 2024 Q1 dataset shape: (38377, 75)\n",
      "Number of listings: 38377\n",
      "Number of features: 75\n",
      "\n",
      "Missing values in key columns for NYC 2024 Q1:\n",
      "price: 14616 missing values (38.09%)\n",
      "room_type: 0 missing values (0.00%)\n",
      "accommodates: 0 missing values (0.00%)\n",
      "bedrooms: 5782 missing values (15.07%)\n",
      "bathrooms: 14621 missing values (38.10%)\n",
      "review_scores_rating: 11610 missing values (30.25%)\n",
      "\n",
      "Price statistics for NYC 2024 Q1:\n",
      "count     23761.000000\n",
      "mean        206.791212\n",
      "std         762.340521\n",
      "min          10.000000\n",
      "25%          80.000000\n",
      "50%         135.000000\n",
      "75%         230.000000\n",
      "max      100000.000000\n",
      "Name: price_numeric, dtype: float64\n",
      "Loading Rhode Island data from ./listingsRI.csv...\n",
      "Rhode Island dataset shape: (5479, 75)\n",
      "Number of listings: 5479\n",
      "Number of features: 75\n",
      "\n",
      "Missing values in key columns for Rhode Island:\n",
      "price: 726 missing values (13.25%)\n",
      "room_type: 0 missing values (0.00%)\n",
      "accommodates: 0 missing values (0.00%)\n",
      "bedrooms: 54 missing values (0.99%)\n",
      "bathrooms: 727 missing values (13.27%)\n",
      "review_scores_rating: 799 missing values (14.58%)\n",
      "\n",
      "Price statistics for Rhode Island:\n",
      "count     4753.000000\n",
      "mean       339.301494\n",
      "std        387.749223\n",
      "min         25.000000\n",
      "25%        132.000000\n",
      "50%        246.000000\n",
      "75%        400.000000\n",
      "max      10000.000000\n",
      "Name: price_numeric, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# For modeling\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up paths to the data files\n",
    "# In a real implementation, these would be GitHub or OSF.io URLs\n",
    "DATA_PATH = \".\"  # Current directory\n",
    "NYC_MAIN = os.path.join(DATA_PATH, \"listingsNYC.csv\")\n",
    "NYC_Q1 = os.path.join(DATA_PATH, \"listingsNYC2024Q1.csv\")\n",
    "RHODE_ISLAND = os.path.join(DATA_PATH, \"listingsRI.csv\")\n",
    "\n",
    "# Function to load and explore the dataset\n",
    "def load_and_explore_data(file_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Load dataset and perform initial exploration\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        dataset_name: Name of the dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"{dataset_name} dataset shape: {df.shape}\")\n",
    "    print(f\"Number of listings: {df.shape[0]}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    key_columns = ['price', 'room_type', 'accommodates', 'bedrooms', \n",
    "                  'bathrooms', 'review_scores_rating']\n",
    "    \n",
    "    missing = df[key_columns].isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nMissing values in key columns for {dataset_name}:\")\n",
    "    for col, count, percent in zip(missing.index, missing.values, missing_percent.values):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "    \n",
    "    # Print basic price statistics if price column exists\n",
    "    if 'price' in df.columns:\n",
    "        # First, clean the price column (remove $ and commas)\n",
    "        df['price_numeric'] = df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        print(f\"\\nPrice statistics for {dataset_name}:\")\n",
    "        print(df['price_numeric'].describe())\n",
    "        \n",
    "        # Create a histogram of prices\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Filter out extreme prices for better visualization\n",
    "        price_filter = df[df['price_numeric'] < df['price_numeric'].quantile(0.95)]\n",
    "        sns.histplot(price_filter['price_numeric'], kde=True)\n",
    "        plt.title(f'{dataset_name} - Price Distribution (excluding top 5%)')\n",
    "        plt.xlabel('Price (USD)')\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_price_distribution.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all three datasets\n",
    "nyc_main = load_and_explore_data(NYC_MAIN, \"NYC Main\")\n",
    "nyc_q1 = load_and_explore_data(NYC_Q1, \"NYC 2024 Q1\")\n",
    "ri_data = load_and_explore_data(RHODE_ISLAND, \"Rhode Island\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54be185-10f5-492d-b18d-799ea2deaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning and preprocessing NYC Main dataset...\n",
      "Removed price outliers outside range: $35.00 - $1184.58\n",
      "Created host_experience_days, median value: 2381.0 days\n",
      "Filled 6 missing values in bathrooms with median: 1.0\n",
      "Filled 36 missing values in bedrooms with median: 1.0\n",
      "Filled 79 missing values in beds with median: 1.0\n",
      "Filled 6561 missing values in review_scores_rating with median: 4.85\n",
      "Converted host_is_superhost to binary (0/1)\n",
      "Converted instant_bookable to binary (0/1)\n",
      "Created log-transformed price feature\n",
      "Completed preprocessing. Final shape: (21891, 82)\n",
      "\n",
      "Cleaning and preprocessing NYC 2024 Q1 dataset...\n",
      "Removed price outliers outside range: $33.00 - $1072.00\n",
      "Created host_experience_days, median value: 2464.0 days\n",
      "Filled 5 missing values in bathrooms with median: 1.0\n",
      "Filled 35 missing values in bedrooms with median: 1.0\n",
      "Filled 145 missing values in beds with median: 1.0\n",
      "Filled 6606 missing values in review_scores_rating with median: 4.84\n",
      "Converted host_is_superhost to binary (0/1)\n",
      "Converted instant_bookable to binary (0/1)\n",
      "Created log-transformed price feature\n",
      "Completed preprocessing. Final shape: (23314, 78)\n",
      "\n",
      "Cleaning and preprocessing Rhode Island dataset...\n",
      "Removed price outliers outside range: $37.00 - $1800.00\n",
      "Created host_experience_days, median value: 2375.0 days\n",
      "Filled 1 missing values in bathrooms with median: 1.0\n",
      "Filled 2 missing values in bedrooms with median: 2.0\n",
      "Filled 2 missing values in beds with median: 3.0\n",
      "Filled 626 missing values in review_scores_rating with median: 4.92\n",
      "Converted host_is_superhost to binary (0/1)\n",
      "Converted instant_bookable to binary (0/1)\n",
      "Created log-transformed price feature\n",
      "Completed preprocessing. Final shape: (4661, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/var/folders/y0/lhz2681516n6nt67mqv7dkpr0000gn/T/ipykernel_6506/4036930876.py:19: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  cleaned_df['price'] = cleaned_df['price'].replace('[\\$,]', '', regex=True).astype(float)\n"
     ]
    }
   ],
   "source": [
    "def clean_and_preprocess_data(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the Airbnb listings data\n",
    "    \n",
    "    Args:\n",
    "        df: Raw DataFrame from load_and_explore_data\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame ready for feature engineering\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning and preprocessing {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Convert price from string to float\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['price'] = cleaned_df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        # Handle extreme outliers in price\n",
    "        q1 = cleaned_df['price'].quantile(0.01)\n",
    "        q3 = cleaned_df['price'].quantile(0.99)\n",
    "        cleaned_df = cleaned_df[(cleaned_df['price'] >= q1) & (cleaned_df['price'] <= q3)]\n",
    "        print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Convert host_since to datetime and calculate host experience in days\n",
    "    if 'host_since' in cleaned_df.columns:\n",
    "        cleaned_df['host_since'] = pd.to_datetime(cleaned_df['host_since'], errors='coerce')\n",
    "        reference_date = pd.to_datetime('2024-01-01')  # Use beginning of 2024 as reference\n",
    "        cleaned_df['host_experience_days'] = (reference_date - cleaned_df['host_since']).dt.days\n",
    "        # Fill missing values with median\n",
    "        median_experience = cleaned_df['host_experience_days'].median()\n",
    "        cleaned_df['host_experience_days'] = cleaned_df['host_experience_days'].fillna(median_experience)\n",
    "        print(f\"Created host_experience_days, median value: {median_experience:.1f} days\")\n",
    "    \n",
    "    # Handle missing values for important numeric features\n",
    "    numeric_features = [\n",
    "        'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "        'review_scores_rating', 'number_of_reviews'\n",
    "    ]\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        if feature in cleaned_df.columns:\n",
    "            missing_count = cleaned_df[feature].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                median_value = cleaned_df[feature].median()\n",
    "                cleaned_df[feature] = cleaned_df[feature].fillna(median_value)\n",
    "                print(f\"Filled {missing_count} missing values in {feature} with median: {median_value}\")\n",
    "    \n",
    "    # Process boolean columns\n",
    "    bool_columns = ['host_is_superhost', 'instant_bookable']\n",
    "    for col in bool_columns:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = cleaned_df[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "            print(f\"Converted {col} to binary (0/1)\")\n",
    "    \n",
    "    # Drop rows where price is missing (shouldn't be many after previous steps)\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        missing_price = cleaned_df['price'].isnull().sum()\n",
    "        if missing_price > 0:\n",
    "            cleaned_df = cleaned_df.dropna(subset=['price'])\n",
    "            print(f\"Dropped {missing_price} rows with missing price\")\n",
    "    \n",
    "    # Log transform price for better model performance\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['log_price'] = np.log1p(cleaned_df['price'])\n",
    "        print(\"Created log-transformed price feature\")\n",
    "    \n",
    "    print(f\"Completed preprocessing. Final shape: {cleaned_df.shape}\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Clean and preprocess all datasets\n",
    "nyc_main_clean = clean_and_preprocess_data(nyc_main, \"NYC Main\")\n",
    "nyc_q1_clean = clean_and_preprocess_data(nyc_q1, \"NYC 2024 Q1\")\n",
    "ri_clean = clean_and_preprocess_data(ri_data, \"Rhode Island\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea6de6d-72ab-4f5c-b26c-71f0936f5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features for NYC Main dataset...\n",
      "\n",
      "Extracting amenities from NYC Main dataset...\n",
      "Found 5555 unique amenities across 21891 listings\n",
      "Top 5 amenities: Smoke alarm, Wifi, Carbon monoxide alarm, Kitchen, Hot water\n",
      "Created binary features for top 20 amenities and amenities_count\n",
      "Created amenity category features: has_essentials, has_luxury, has_safety\n",
      "Created person_per_bedroom feature\n",
      "Created avg_review_score from 6 individual review metrics\n",
      "Created one-hot encoding for room_type with 4 categories\n",
      "Created one-hot encoding for property_type with 31 categories\n",
      "Created one-hot encoding for neighborhoods with 190 categories\n",
      "Created neighborhood_avg_price feature\n",
      "Created distance_to_center feature (km)\n",
      "Created booking flexibility features\n",
      "Completed feature engineering. Final number of features: 344\n",
      "\n",
      "Engineering features for NYC 2024 Q1 dataset...\n",
      "\n",
      "Extracting amenities from NYC 2024 Q1 dataset...\n",
      "Found 6148 unique amenities across 23314 listings\n",
      "Top 5 amenities: Smoke alarm, Wifi, Kitchen, Carbon monoxide alarm, Hot water\n",
      "Created binary features for top 20 amenities and amenities_count\n",
      "Created amenity category features: has_essentials, has_luxury, has_safety\n",
      "Created person_per_bedroom feature\n",
      "Created avg_review_score from 6 individual review metrics\n",
      "Created one-hot encoding for room_type with 4 categories\n",
      "Created one-hot encoding for property_type with 34 categories\n",
      "Created one-hot encoding for neighborhoods with 192 categories\n",
      "Created neighborhood_avg_price feature\n",
      "Created distance_to_center feature (km)\n",
      "Created booking flexibility features\n",
      "Completed feature engineering. Final number of features: 345\n",
      "\n",
      "Engineering features for Rhode Island dataset...\n",
      "\n",
      "Extracting amenities from Rhode Island dataset...\n",
      "Found 2128 unique amenities across 4661 listings\n",
      "Top 5 amenities: Smoke alarm, Wifi, Carbon monoxide alarm, Kitchen, Hot water\n",
      "Created binary features for top 20 amenities and amenities_count\n",
      "Created amenity category features: has_essentials, has_luxury, has_safety\n",
      "Created person_per_bedroom feature\n",
      "Created avg_review_score from 6 individual review metrics\n",
      "Created one-hot encoding for room_type with 3 categories\n",
      "Created one-hot encoding for property_type with 22 categories\n",
      "Created one-hot encoding for neighborhoods with 35 categories\n",
      "Created neighborhood_avg_price feature\n",
      "Created distance_to_center feature (km)\n",
      "Created booking flexibility features\n",
      "Completed feature engineering. Final number of features: 175\n",
      "\n",
      "Creating visualizations for NYC Main dataset...\n"
     ]
    }
   ],
   "source": [
    "def extract_amenities(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Extract amenities from the JSON string and create binary features\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with an 'amenities' column\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with new binary amenity features\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting amenities from {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_with_amenities = df.copy()\n",
    "    \n",
    "    if 'amenities' not in df_with_amenities.columns:\n",
    "        print(\"No 'amenities' column found in dataset\")\n",
    "        return df_with_amenities\n",
    "    \n",
    "    # Parse amenities JSON\n",
    "    def parse_amenities(amenities_str):\n",
    "        if pd.isna(amenities_str) or amenities_str == '[]':\n",
    "            return []\n",
    "        try:\n",
    "            # Clean the JSON string\n",
    "            cleaned_str = amenities_str.replace(\"'\", '\"')\n",
    "            return json.loads(cleaned_str)\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    df_with_amenities['amenities_list'] = df_with_amenities['amenities'].apply(parse_amenities)\n",
    "    \n",
    "    # Get all amenities across the dataset\n",
    "    all_amenities = []\n",
    "    for amenities_list in df_with_amenities['amenities_list']:\n",
    "        all_amenities.extend(amenities_list)\n",
    "    \n",
    "    # Count frequencies\n",
    "    from collections import Counter\n",
    "    amenities_counter = Counter(all_amenities)\n",
    "    total_listings = len(df_with_amenities)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    amenities_freq = pd.DataFrame({\n",
    "        'amenity': list(amenities_counter.keys()),\n",
    "        'count': list(amenities_counter.values()),\n",
    "        'percentage': [count/total_listings*100 for count in amenities_counter.values()]\n",
    "    }).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Found {len(amenities_freq)} unique amenities across {total_listings} listings\")\n",
    "    \n",
    "    # Get top amenities (limit to 20 to avoid creating too many features)\n",
    "    top_amenities = amenities_freq.head(20)['amenity'].tolist()\n",
    "    print(f\"Top 5 amenities: {', '.join(top_amenities[:5])}\")\n",
    "    \n",
    "    # Create binary features for top amenities\n",
    "    for amenity in top_amenities:\n",
    "        # Create a valid column name\n",
    "        column_name = f\"has_{amenity.lower().replace(' ', '_').replace('-', '_').replace('/', '_')}\"\n",
    "        \n",
    "        # Some column names might be too long, truncate if necessary\n",
    "        if len(column_name) > 63:  # PostgreSQL limit, just to be safe\n",
    "            column_name = column_name[:63]\n",
    "        \n",
    "        # Create the binary feature\n",
    "        df_with_amenities[column_name] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: 1 if amenity in x else 0\n",
    "        )\n",
    "    \n",
    "    # Create amenity count feature\n",
    "    df_with_amenities['amenities_count'] = df_with_amenities['amenities_list'].apply(len)\n",
    "    print(f\"Created binary features for top {len(top_amenities)} amenities and amenities_count\")\n",
    "    \n",
    "    # Create amenity category features\n",
    "    # Define categories of amenities\n",
    "    amenity_categories = {\n",
    "        'essentials': ['Wifi', 'Internet', 'Kitchen', 'Heating', 'Air conditioning'],\n",
    "        'luxury': ['Pool', 'Hot tub', 'Gym', 'Doorman', 'Elevator'],\n",
    "        'safety': ['Smoke detector', 'Carbon monoxide detector', 'Fire extinguisher', 'First aid kit']\n",
    "    }\n",
    "    \n",
    "    for category, amenities in amenity_categories.items():\n",
    "        df_with_amenities[f'has_{category}'] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: 1 if any(amenity in x for amenity in amenities) else 0\n",
    "        )\n",
    "    \n",
    "    print(f\"Created amenity category features: {', '.join([f'has_{c}' for c in amenity_categories.keys()])}\")\n",
    "    \n",
    "    return df_with_amenities\n",
    "\n",
    "def engineer_features(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create additional features that might be useful for prediction\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering features for {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Extract amenities\n",
    "    df_engineered = extract_amenities(df_engineered, dataset_name)\n",
    "    \n",
    "    # Create person-per-bedroom ratio\n",
    "    if all(col in df_engineered.columns for col in ['accommodates', 'bedrooms']):\n",
    "        # Avoid division by zero by replacing 0 bedrooms with 1\n",
    "        df_engineered['person_per_bedroom'] = df_engineered['accommodates'] / df_engineered['bedrooms'].replace(0, 1)\n",
    "        print(\"Created person_per_bedroom feature\")\n",
    "    \n",
    "    # Create average review score if multiple review scores are available\n",
    "    review_score_columns = [col for col in df_engineered.columns if col.startswith('review_scores_') \n",
    "                           and col != 'review_scores_rating']\n",
    "    \n",
    "    if len(review_score_columns) >= 2:\n",
    "        df_engineered['avg_review_score'] = df_engineered[review_score_columns].mean(axis=1)\n",
    "        print(f\"Created avg_review_score from {len(review_score_columns)} individual review metrics\")\n",
    "    \n",
    "    # Create room type one-hot encoding\n",
    "    if 'room_type' in df_engineered.columns:\n",
    "        room_type_dummies = pd.get_dummies(df_engineered['room_type'], prefix='room_type')\n",
    "        df_engineered = pd.concat([df_engineered, room_type_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for room_type with {room_type_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create property type features (grouped to avoid too many categories)\n",
    "    if 'property_type' in df_engineered.columns:\n",
    "        # Count occurrences of each property type\n",
    "        property_counts = df_engineered['property_type'].value_counts()\n",
    "        \n",
    "        # Group uncommon property types as 'Other'\n",
    "        min_count = 10  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['property_type_grouped'] = df_engineered['property_type'].apply(\n",
    "            lambda x: x if property_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped property types\n",
    "        property_dummies = pd.get_dummies(df_engineered['property_type_grouped'], prefix='property')\n",
    "        df_engineered = pd.concat([df_engineered, property_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for property_type with {property_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create neighborhood features\n",
    "    if 'neighbourhood_cleansed' in df_engineered.columns:\n",
    "        # Count occurrences of each neighborhood\n",
    "        neighborhood_counts = df_engineered['neighbourhood_cleansed'].value_counts()\n",
    "        \n",
    "        # Group uncommon neighborhoods as 'Other'\n",
    "        min_count = 5  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['neighborhood_grouped'] = df_engineered['neighbourhood_cleansed'].apply(\n",
    "            lambda x: x if neighborhood_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped neighborhoods\n",
    "        neighborhood_dummies = pd.get_dummies(df_engineered['neighborhood_grouped'], prefix='nbhd')\n",
    "        df_engineered = pd.concat([df_engineered, neighborhood_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for neighborhoods with {neighborhood_dummies.shape[1]} categories\")\n",
    "        \n",
    "        # Calculate average price per neighborhood for reference\n",
    "        neighborhood_avg_price = df_engineered.groupby('neighbourhood_cleansed')['price'].mean()\n",
    "        df_engineered['neighborhood_avg_price'] = df_engineered['neighbourhood_cleansed'].map(neighborhood_avg_price)\n",
    "        print(\"Created neighborhood_avg_price feature\")\n",
    "    \n",
    "    # Create location-based features\n",
    "    if all(col in df_engineered.columns for col in ['latitude', 'longitude']):\n",
    "        # Calculate distance to city center (approximate for NYC)\n",
    "        nyc_center = (40.7128, -74.0060)  # Manhattan coordinates\n",
    "        \n",
    "        df_engineered['distance_to_center'] = np.sqrt(\n",
    "            (df_engineered['latitude'] - nyc_center[0])**2 + \n",
    "            (df_engineered['longitude'] - nyc_center[1])**2\n",
    "        ) * 111  # Convert to approximate kilometers (111km per degree at equator)\n",
    "        \n",
    "        print(\"Created distance_to_center feature (km)\")\n",
    "    \n",
    "    # Create features related to booking flexibility\n",
    "    if all(col in df_engineered.columns for col in ['minimum_nights', 'maximum_nights']):\n",
    "        # Create booking flexibility score\n",
    "        df_engineered['booking_flexibility'] = 1 / (df_engineered['minimum_nights'] + 1)\n",
    "        \n",
    "        # Create a feature for stay duration category\n",
    "        df_engineered['stay_category'] = pd.cut(\n",
    "            df_engineered['minimum_nights'],\n",
    "            bins=[0, 1, 3, 7, 30, float('inf')],\n",
    "            labels=['One Night', '2-3 Nights', '4-7 Nights', '8-30 Nights', '30+ Nights']\n",
    "        )\n",
    "        \n",
    "        stay_dummies = pd.get_dummies(df_engineered['stay_category'], prefix='stay')\n",
    "        df_engineered = pd.concat([df_engineered, stay_dummies], axis=1)\n",
    "        print(\"Created booking flexibility features\")\n",
    "    \n",
    "    print(f\"Completed feature engineering. Final number of features: {df_engineered.shape[1]}\")\n",
    "    return df_engineered\n",
    "\n",
    "# Apply feature engineering to all datasets\n",
    "nyc_main_features = engineer_features(nyc_main_clean, \"NYC Main\")\n",
    "nyc_q1_features = engineer_features(nyc_q1_clean, \"NYC 2024 Q1\")\n",
    "ri_features = engineer_features(ri_clean, \"Rhode Island\")\n",
    "\n",
    "# Visualize the relationship between key features and price\n",
    "def visualize_feature_relationships(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create visualizations of relationships between key features and price\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating visualizations for {dataset_name} dataset...\")\n",
    "    \n",
    "    # Price by room type\n",
    "    if 'room_type' in df.columns and 'price' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='room_type', y='price', data=df)\n",
    "        plt.title(f'{dataset_name} - Price by Room Type')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_price_by_room_type.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Price vs. accommodates\n",
    "    if 'accommodates' in df.columns and 'price' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='accommodates', y='price', data=df[df['accommodates'] <= 10])\n",
    "        plt.title(f'{dataset_name} - Price by Accommodation Capacity')\n",
    "        plt.xlabel('Accommodates (people)')\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_price_by_accommodates.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    numeric_cols = ['price', 'accommodates', 'bedrooms', 'beds', 'bathrooms', \n",
    "                   'number_of_reviews', 'review_scores_rating', 'distance_to_center', \n",
    "                   'amenities_count', 'host_experience_days']\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataset\n",
    "    existing_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    \n",
    "    if len(existing_cols) >= 3:  # Need at least a few columns for a meaningful heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = df[existing_cols].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "        plt.title(f'{dataset_name} - Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_correlation_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "# Create visualizations for the main dataset\n",
    "visualize_feature_relationships(nyc_main_features, \"NYC Main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa77e0a-8266-48b1-9116-99424a78f6a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING AND PREPROCESSING DATA\n",
      "================================================================================\n",
      "Loading NYC Main data from ./listingsNYC.csv...\n",
      "NYC Main dataset shape: (37434, 79)\n",
      "Number of listings: 37434\n",
      "Number of features: 79\n",
      "\n",
      "Missing values in key columns for NYC Main:\n",
      "price: 15126 missing values (40.41%)\n",
      "room_type: 0 missing values (0.00%)\n",
      "accommodates: 0 missing values (0.00%)\n",
      "bedrooms: 5911 missing values (15.79%)\n",
      "bathrooms: 14931 missing values (39.89%)\n",
      "review_scores_rating: 11787 missing values (31.49%)\n",
      "\n",
      "Price statistics for NYC Main:\n",
      "count    22308.000000\n",
      "mean       213.835216\n",
      "std        427.599435\n",
      "min          7.000000\n",
      "25%         85.000000\n",
      "50%        140.000000\n",
      "75%        240.000000\n",
      "max      20000.000000\n",
      "Name: price_numeric, dtype: float64\n",
      "\n",
      "Cleaning and preprocessing NYC Main dataset...\n",
      "Removed price outliers outside range: $35.00 - $1184.58\n",
      "Created host_experience_days, median value: 2381.0 days\n",
      "Filled 6 missing values in bathrooms with median: 1.0\n",
      "Filled 36 missing values in bedrooms with median: 1.0\n",
      "Filled 79 missing values in beds with median: 1.0\n",
      "Filled 6561 missing values in review_scores_rating with median: 4.85\n",
      "Converted host_is_superhost to binary (0/1)\n",
      "Converted instant_bookable to binary (0/1)\n",
      "Created log-transformed price feature\n",
      "Completed preprocessing. Final shape: (21891, 82)\n",
      "\n",
      "Engineering features for NYC Main dataset...\n",
      "\n",
      "Extracting amenities from NYC Main dataset...\n",
      "Found 4988 unique amenities across 21891 listings\n",
      "Top 5 amenities: Smoke alarm, Wifi, Carbon monoxide alarm, Kitchen, Hot water\n",
      "Created binary features for top 20 amenities and amenities_count\n",
      "Created amenity category features: has_essentials, has_luxury, has_safety\n",
      "Created person_per_bedroom feature\n",
      "Created avg_review_score from 6 individual review metrics\n",
      "Created one-hot encoding for room_type with 4 categories\n",
      "Created one-hot encoding for property_type with 31 categories\n",
      "Created one-hot encoding for neighborhoods with 190 categories\n",
      "Created neighborhood_avg_price feature\n",
      "Created distance_to_center feature (km)\n",
      "Created booking flexibility features\n",
      "Completed feature engineering. Final number of features: 344\n",
      "\n",
      "================================================================================\n",
      "MODEL BUILDING\n",
      "================================================================================\n",
      "Preparing data for modeling...\n",
      "Initial number of columns: 344\n",
      "Columns to be dropped: 50\n",
      "Filling missing values with column medians\n",
      "  Filled host_listings_count missing values with median: 3.0\n",
      "  Filled host_total_listings_count missing values with median: 5.0\n",
      "  Filled review_scores_accuracy missing values with median: 4.88\n",
      "  Filled review_scores_cleanliness missing values with median: 4.83\n",
      "  Filled review_scores_checkin missing values with median: 4.94\n",
      "  Filled review_scores_communication missing values with median: 4.94\n",
      "  Filled review_scores_location missing values with median: 4.82\n",
      "  Filled avg_review_score missing values with median: 4.831666666666666\n",
      "Final dataset shape: (21891, 293)\n",
      "Number of features: 293\n",
      "Training set size: (17512, 293)\n",
      "Testing set size: (4379, 293)\n",
      "\n",
      "Building and evaluating models...\n",
      "\n",
      "Training OLS model...\n",
      "  Cross-validated R² (train): 0.6987\n",
      "OLS model trained successfully\n",
      "  R-squared: 0.6914\n",
      "  MAE: 0.3014\n",
      "  RMSE: 0.3988\n",
      "  Training time: 0.0422 seconds\n",
      "  Prediction time: 0.0034 seconds\n",
      "\n",
      "Training LASSO model...\n",
      "  Cross-validated R² (train): 0.4994\n",
      "  WARNING: Unusual R² value: 0.4971\n",
      "  Using cross-validated R² as a more reliable estimate: 0.4994\n",
      "LASSO model trained successfully\n",
      "  R-squared: 0.4994\n",
      "  MAE: 0.3989\n",
      "  RMSE: 0.5091\n",
      "  Training time: 0.2007 seconds\n",
      "  Prediction time: 0.0027 seconds\n",
      "\n",
      "Training RandomForest model...\n",
      "  Cross-validated R² (train): 0.7590\n",
      "RandomForest model trained successfully\n",
      "  R-squared: 0.7570\n",
      "  MAE: 0.2611\n",
      "  RMSE: 0.3539\n",
      "  Training time: 8.3887 seconds\n",
      "  Prediction time: 0.0255 seconds\n",
      "\n",
      "Training LightGBM model...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4717\n",
      "[LightGBM] [Info] Number of data points in the train set: 14009, number of used features: 198\n",
      "[LightGBM] [Info] Start training from score 4.983097\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4704\n",
      "[LightGBM] [Info] Number of data points in the train set: 14009, number of used features: 203\n",
      "[LightGBM] [Info] Start training from score 4.987420\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001441 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4684\n",
      "[LightGBM] [Info] Number of data points in the train set: 14010, number of used features: 194\n",
      "[LightGBM] [Info] Start training from score 4.984275\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4701\n",
      "[LightGBM] [Info] Number of data points in the train set: 14010, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 4.976739\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001426 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4687\n",
      "[LightGBM] [Info] Number of data points in the train set: 14010, number of used features: 194\n",
      "[LightGBM] [Info] Start training from score 4.981534\n",
      "  Cross-validated R² (train): 0.7772\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4838\n",
      "[LightGBM] [Info] Number of data points in the train set: 17512, number of used features: 213\n",
      "[LightGBM] [Info] Start training from score 4.982613\n",
      "LightGBM model trained successfully\n",
      "  R-squared: 0.7752\n",
      "  MAE: 0.2531\n",
      "  RMSE: 0.3403\n",
      "  Training time: 0.3252 seconds\n",
      "  Prediction time: 0.0061 seconds\n",
      "\n",
      "Training KNN model...\n",
      "  Cross-validated R² (train): 0.6502\n",
      "KNN model trained successfully\n",
      "  R-squared: 0.6590\n",
      "  MAE: 0.3035\n",
      "  RMSE: 0.4192\n",
      "  Training time: 0.0067 seconds\n",
      "  Prediction time: 0.2456 seconds\n",
      "\n",
      "================================================================================\n",
      "MODEL PERFORMANCE COMPARISON (HORSERACE TABLE)\n",
      "================================================================================\n",
      "       Model  R-squared      MAE     RMSE  Training Time (s)  Prediction Time (s)\n",
      "    LightGBM   0.775241 0.253120 0.340343           0.325215             0.006147\n",
      "RandomForest   0.756967 0.261056 0.353908           8.388706             0.025536\n",
      "         OLS   0.691372 0.301393 0.398819           0.042152             0.003368\n",
      "         KNN   0.659029 0.303502 0.419195           0.006705             0.245567\n",
      "       LASSO   0.499448 0.398906 0.509119           0.200664             0.002652\n",
      "Saved results to outputs/model_comparison_nyc.csv\n",
      "Created all visualization files successfully\n",
      "\n",
      "Model building and evaluation complete!\n",
      "Variables 'results_df' and 'trained_models' are available for further analysis\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Model Preparation and Model Building\n",
    "\n",
    "# Import necessary libraries\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. First, load the data files\n",
    "def load_and_explore_data(file_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Load dataset and perform initial exploration\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        dataset_name: Name of the dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"{dataset_name} dataset shape: {df.shape}\")\n",
    "    print(f\"Number of listings: {df.shape[0]}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    key_columns = ['price', 'room_type', 'accommodates', 'bedrooms', \n",
    "                  'bathrooms', 'review_scores_rating']\n",
    "    \n",
    "    missing = df[key_columns].isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nMissing values in key columns for {dataset_name}:\")\n",
    "    for col, count, percent in zip(missing.index, missing.values, missing_percent.values):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "    \n",
    "    # Print basic price statistics if price column exists\n",
    "    if 'price' in df.columns:\n",
    "        # First, clean the price column (remove $ and commas) - fixed escape sequence\n",
    "        df['price_numeric'] = df['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        print(f\"\\nPrice statistics for {dataset_name}:\")\n",
    "        print(df['price_numeric'].describe())\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_and_preprocess_data(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the Airbnb listings data\n",
    "    \n",
    "    Args:\n",
    "        df: Raw DataFrame from load_and_explore_data\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame ready for feature engineering\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning and preprocessing {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Convert price from string to float - fixed escape sequence\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['price'] = cleaned_df['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        # Handle extreme outliers in price\n",
    "        q1 = cleaned_df['price'].quantile(0.01)\n",
    "        q3 = cleaned_df['price'].quantile(0.99)\n",
    "        cleaned_df = cleaned_df[(cleaned_df['price'] >= q1) & (cleaned_df['price'] <= q3)]\n",
    "        print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Convert host_since to datetime and calculate host experience in days\n",
    "    if 'host_since' in cleaned_df.columns:\n",
    "        cleaned_df['host_since'] = pd.to_datetime(cleaned_df['host_since'], errors='coerce')\n",
    "        reference_date = pd.to_datetime('2024-01-01')  # Use beginning of 2024 as reference\n",
    "        cleaned_df['host_experience_days'] = (reference_date - cleaned_df['host_since']).dt.days\n",
    "        # Fill missing values with median\n",
    "        median_experience = cleaned_df['host_experience_days'].median()\n",
    "        cleaned_df['host_experience_days'] = cleaned_df['host_experience_days'].fillna(median_experience)\n",
    "        print(f\"Created host_experience_days, median value: {median_experience:.1f} days\")\n",
    "    \n",
    "    # Handle missing values for important numeric features\n",
    "    numeric_features = [\n",
    "        'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "        'review_scores_rating', 'number_of_reviews'\n",
    "    ]\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        if feature in cleaned_df.columns:\n",
    "            missing_count = cleaned_df[feature].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                median_value = cleaned_df[feature].median()\n",
    "                cleaned_df[feature] = cleaned_df[feature].fillna(median_value)\n",
    "                print(f\"Filled {missing_count} missing values in {feature} with median: {median_value}\")\n",
    "    \n",
    "    # Process boolean columns\n",
    "    bool_columns = ['host_is_superhost', 'instant_bookable']\n",
    "    for col in bool_columns:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = cleaned_df[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "            print(f\"Converted {col} to binary (0/1)\")\n",
    "    \n",
    "    # Drop rows where price is missing (shouldn't be many after previous steps)\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        missing_price = cleaned_df['price'].isnull().sum()\n",
    "        if missing_price > 0:\n",
    "            cleaned_df = cleaned_df.dropna(subset=['price'])\n",
    "            print(f\"Dropped {missing_price} rows with missing price\")\n",
    "    \n",
    "    # Log transform price for better model performance\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['log_price'] = np.log1p(cleaned_df['price'])\n",
    "        print(\"Created log-transformed price feature\")\n",
    "    \n",
    "    print(f\"Completed preprocessing. Final shape: {cleaned_df.shape}\")\n",
    "    return cleaned_df\n",
    "\n",
    "def extract_amenities(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Extract amenities from the JSON string and create binary features\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with an 'amenities' column\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with new binary amenity features\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting amenities from {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_with_amenities = df.copy()\n",
    "    \n",
    "    if 'amenities' not in df_with_amenities.columns:\n",
    "        print(\"No 'amenities' column found in dataset\")\n",
    "        return df_with_amenities\n",
    "    \n",
    "    # Parse amenities JSON\n",
    "    def parse_amenities(amenities_str):\n",
    "        if pd.isna(amenities_str) or amenities_str == '[]':\n",
    "            return []\n",
    "        try:\n",
    "            # Clean the JSON string\n",
    "            cleaned_str = str(amenities_str).replace(\"'\", '\"')\n",
    "            cleaned_str = re.sub(r'(\\w+):', r'\"\\1\":', cleaned_str)\n",
    "            return json.loads(cleaned_str)\n",
    "        except:\n",
    "            try:\n",
    "                # Try another approach - split by comma and clean\n",
    "                items = str(amenities_str).strip('[]').split(',')\n",
    "                return [item.strip().strip('\"\\'') for item in items if item.strip()]\n",
    "            except:\n",
    "                return []\n",
    "    \n",
    "    df_with_amenities['amenities_list'] = df_with_amenities['amenities'].apply(parse_amenities)\n",
    "    \n",
    "    # Get all amenities across the dataset\n",
    "    all_amenities = []\n",
    "    for amenities_list in df_with_amenities['amenities_list']:\n",
    "        if isinstance(amenities_list, list):\n",
    "            all_amenities.extend(amenities_list)\n",
    "    \n",
    "    # Count frequencies\n",
    "    from collections import Counter\n",
    "    amenities_counter = Counter(all_amenities)\n",
    "    total_listings = len(df_with_amenities)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    amenities_freq = pd.DataFrame({\n",
    "        'amenity': list(amenities_counter.keys()),\n",
    "        'count': list(amenities_counter.values()),\n",
    "        'percentage': [count/total_listings*100 for count in amenities_counter.values()]\n",
    "    }).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Found {len(amenities_freq)} unique amenities across {total_listings} listings\")\n",
    "    \n",
    "    # Get top amenities (limit to 20 to avoid creating too many features)\n",
    "    top_amenities = amenities_freq.head(20)['amenity'].tolist()\n",
    "    if top_amenities:\n",
    "        print(f\"Top 5 amenities: {', '.join(top_amenities[:5])}\")\n",
    "    \n",
    "        # Create binary features for top amenities\n",
    "        for amenity in top_amenities:\n",
    "            # Create a valid column name\n",
    "            column_name = f\"has_{str(amenity).lower().replace(' ', '_').replace('-', '_').replace('/', '_')}\"\n",
    "            \n",
    "            # Some column names might be too long, truncate if necessary\n",
    "            if len(column_name) > 63:  # PostgreSQL limit, just to be safe\n",
    "                column_name = column_name[:63]\n",
    "            \n",
    "            # Create the binary feature\n",
    "            df_with_amenities[column_name] = df_with_amenities['amenities_list'].apply(\n",
    "                lambda x: 1 if isinstance(x, list) and amenity in x else 0\n",
    "            )\n",
    "        \n",
    "        # Create amenity count feature\n",
    "        df_with_amenities['amenities_count'] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "        print(f\"Created binary features for top {len(top_amenities)} amenities and amenities_count\")\n",
    "    \n",
    "    # Create amenity category features\n",
    "    # Define categories of amenities\n",
    "    amenity_categories = {\n",
    "        'essentials': ['Wifi', 'Internet', 'Kitchen', 'Heating', 'Air conditioning'],\n",
    "        'luxury': ['Pool', 'Hot tub', 'Gym', 'Doorman', 'Elevator'],\n",
    "        'safety': ['Smoke detector', 'Carbon monoxide detector', 'Fire extinguisher', 'First aid kit']\n",
    "    }\n",
    "    \n",
    "    for category, amenities in amenity_categories.items():\n",
    "        df_with_amenities[f'has_{category}'] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: 1 if isinstance(x, list) and any(amenity in x for amenity in amenities) else 0\n",
    "        )\n",
    "    \n",
    "    print(f\"Created amenity category features: {', '.join([f'has_{c}' for c in amenity_categories.keys()])}\")\n",
    "    \n",
    "    return df_with_amenities\n",
    "\n",
    "def engineer_features(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create additional features that might be useful for prediction\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering features for {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Extract amenities\n",
    "    df_engineered = extract_amenities(df_engineered, dataset_name)\n",
    "    \n",
    "    # Create person-per-bedroom ratio\n",
    "    if all(col in df_engineered.columns for col in ['accommodates', 'bedrooms']):\n",
    "        # Avoid division by zero by replacing 0 bedrooms with 1\n",
    "        df_engineered['person_per_bedroom'] = df_engineered['accommodates'] / df_engineered['bedrooms'].replace(0, 1)\n",
    "        print(\"Created person_per_bedroom feature\")\n",
    "    \n",
    "    # Create average review score if multiple review scores are available\n",
    "    review_score_columns = [col for col in df_engineered.columns if col.startswith('review_scores_') \n",
    "                           and col != 'review_scores_rating']\n",
    "    \n",
    "    if len(review_score_columns) >= 2:\n",
    "        df_engineered['avg_review_score'] = df_engineered[review_score_columns].mean(axis=1)\n",
    "        print(f\"Created avg_review_score from {len(review_score_columns)} individual review metrics\")\n",
    "    \n",
    "    # Create room type one-hot encoding\n",
    "    if 'room_type' in df_engineered.columns:\n",
    "        room_type_dummies = pd.get_dummies(df_engineered['room_type'], prefix='room_type')\n",
    "        df_engineered = pd.concat([df_engineered, room_type_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for room_type with {room_type_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create property type features (grouped to avoid too many categories)\n",
    "    if 'property_type' in df_engineered.columns:\n",
    "        # Count occurrences of each property type\n",
    "        property_counts = df_engineered['property_type'].value_counts()\n",
    "        \n",
    "        # Group uncommon property types as 'Other'\n",
    "        min_count = 10  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['property_type_grouped'] = df_engineered['property_type'].apply(\n",
    "            lambda x: x if property_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped property types\n",
    "        property_dummies = pd.get_dummies(df_engineered['property_type_grouped'], prefix='property')\n",
    "        df_engineered = pd.concat([df_engineered, property_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for property_type with {property_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create neighborhood features\n",
    "    if 'neighbourhood_cleansed' in df_engineered.columns:\n",
    "        # Count occurrences of each neighborhood\n",
    "        neighborhood_counts = df_engineered['neighbourhood_cleansed'].value_counts()\n",
    "        \n",
    "        # Group uncommon neighborhoods as 'Other'\n",
    "        min_count = 5  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['neighborhood_grouped'] = df_engineered['neighbourhood_cleansed'].apply(\n",
    "            lambda x: x if neighborhood_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped neighborhoods\n",
    "        neighborhood_dummies = pd.get_dummies(df_engineered['neighborhood_grouped'], prefix='nbhd')\n",
    "        df_engineered = pd.concat([df_engineered, neighborhood_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for neighborhoods with {neighborhood_dummies.shape[1]} categories\")\n",
    "        \n",
    "        # Calculate average price per neighborhood for reference\n",
    "        neighborhood_avg_price = df_engineered.groupby('neighbourhood_cleansed')['price'].mean()\n",
    "        df_engineered['neighborhood_avg_price'] = df_engineered['neighbourhood_cleansed'].map(neighborhood_avg_price)\n",
    "        print(\"Created neighborhood_avg_price feature\")\n",
    "    \n",
    "    # Create location-based features\n",
    "    if all(col in df_engineered.columns for col in ['latitude', 'longitude']):\n",
    "        # Calculate distance to city center (approximate for NYC)\n",
    "        nyc_center = (40.7128, -74.0060)  # Manhattan coordinates\n",
    "        \n",
    "        df_engineered['distance_to_center'] = np.sqrt(\n",
    "            (df_engineered['latitude'] - nyc_center[0])**2 + \n",
    "            (df_engineered['longitude'] - nyc_center[1])**2\n",
    "        ) * 111  # Convert to approximate kilometers (111km per degree at equator)\n",
    "        \n",
    "        print(\"Created distance_to_center feature (km)\")\n",
    "    \n",
    "    # Create features related to booking flexibility\n",
    "    if all(col in df_engineered.columns for col in ['minimum_nights', 'maximum_nights']):\n",
    "        # Create booking flexibility score\n",
    "        df_engineered['booking_flexibility'] = 1 / (df_engineered['minimum_nights'] + 1)\n",
    "        \n",
    "        # Create a feature for stay duration category\n",
    "        df_engineered['stay_category'] = pd.cut(\n",
    "            df_engineered['minimum_nights'],\n",
    "            bins=[0, 1, 3, 7, 30, float('inf')],\n",
    "            labels=['One Night', '2-3 Nights', '4-7 Nights', '8-30 Nights', '30+ Nights']\n",
    "        )\n",
    "        \n",
    "        stay_dummies = pd.get_dummies(df_engineered['stay_category'], prefix='stay')\n",
    "        df_engineered = pd.concat([df_engineered, stay_dummies], axis=1)\n",
    "        print(\"Created booking flexibility features\")\n",
    "    \n",
    "    print(f\"Completed feature engineering. Final number of features: {df_engineered.shape[1]}\")\n",
    "    return df_engineered\n",
    "\n",
    "def prepare_for_modeling(df, target_col='log_price'):\n",
    "    \"\"\"\n",
    "    Prepare the final features and target for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with all features\n",
    "        target_col: Name of the target column\n",
    "        \n",
    "    Returns:\n",
    "        X, y, and list of feature names\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Define columns to drop (non-feature columns or duplicates)\n",
    "    cols_to_drop = [\n",
    "        # Identifiers and URLs\n",
    "        'id', 'listing_url', 'scrape_id', 'last_scraped', 'source',\n",
    "        'picture_url', 'host_id', 'host_url', 'host_thumbnail_url', 'host_picture_url',\n",
    "        \n",
    "        # Text fields\n",
    "        'name', 'description', 'neighborhood_overview', 'host_name', \n",
    "        'host_location', 'host_about', 'host_neighbourhood',\n",
    "        \n",
    "        # Original columns replaced by engineered features\n",
    "        'host_since', 'price', 'price_numeric', 'amenities', 'amenities_list',\n",
    "        'property_type', 'property_type_grouped', 'room_type', \n",
    "        'neighbourhood_cleansed', 'neighbourhood', 'neighborhood_grouped',\n",
    "        'stay_category', 'calendar_updated',\n",
    "        \n",
    "        # Redundant date columns\n",
    "        'first_review', 'last_review', 'calendar_last_scraped',\n",
    "        \n",
    "        # Other columns that might cause issues\n",
    "        'host_verifications', 'host_response_time', 'host_response_rate',\n",
    "        'host_acceptance_rate', 'bathrooms_text', 'license',\n",
    "        'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_group_cleansed', 'has_availability',\n",
    "        \n",
    "        # Features that might cause data leakage\n",
    "        'review_scores_value', 'reviews_per_month', 'neighborhood_avg_price',\n",
    "        'estimated_revenue_l365d', 'estimated_occupancy_l365d',\n",
    "        'number_of_reviews_ltm', 'number_of_reviews_l30d'\n",
    "    ]\n",
    "    \n",
    "    # Only drop columns that exist in the dataframe\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in data.columns]\n",
    "    \n",
    "    # Get the list of columns before dropping\n",
    "    print(f\"Initial number of columns: {len(data.columns)}\")\n",
    "    print(f\"Columns to be dropped: {len(existing_cols_to_drop)}\")\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    data = data.drop(existing_cols_to_drop, axis=1, errors='ignore')\n",
    "    \n",
    "    # Add the target column to columns to drop if it's not 'log_price'\n",
    "    if target_col != 'price' and 'price' in data.columns:\n",
    "        data = data.drop('price', axis=1, errors='ignore')\n",
    "    \n",
    "    # Make sure the target column exists\n",
    "    if target_col not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the dataset\")\n",
    "    \n",
    "    # First, convert all object columns to numeric if possible\n",
    "    for col in data.columns:\n",
    "        if col != target_col and data[col].dtype == 'object':\n",
    "            try:\n",
    "                data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "            except:\n",
    "                pass  # Will handle non-convertible columns below\n",
    "    \n",
    "    # Find remaining non-numeric columns\n",
    "    object_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    if target_col in object_cols:\n",
    "        object_cols.remove(target_col)\n",
    "    \n",
    "    if object_cols:\n",
    "        print(f\"Converting {len(object_cols)} object columns to numeric or dropping them\")\n",
    "        for col in object_cols:\n",
    "            # Try one more time with more aggressive cleaning\n",
    "            try:\n",
    "                data[col] = data[col].astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "                data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "                print(f\"  Converted '{col}' to numeric\")\n",
    "            except:\n",
    "                print(f\"  Dropping column '{col}' - cannot convert to numeric\")\n",
    "                data = data.drop(col, axis=1)\n",
    "    \n",
    "    # Check for infinite values and replace with NaN\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Check for high correlation with target to prevent data leakage\n",
    "    try:\n",
    "        correlations = data.corr()[target_col].abs().sort_values(ascending=False)\n",
    "        high_corr_features = correlations[(correlations > 0.85) & (correlations < 1.0)].index.tolist()\n",
    "        if high_corr_features:\n",
    "            print(f\"Dropping {len(high_corr_features)} features with suspiciously high correlation to target:\")\n",
    "            for feature in high_corr_features:\n",
    "                print(f\"  - {feature}: {correlations[feature]:.4f}\")\n",
    "            \n",
    "            data = data.drop(high_corr_features, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not check correlations with target: {str(e)}\")\n",
    "    \n",
    "    # Fill missing values instead of dropping rows\n",
    "    print(\"Filling missing values with column medians\")\n",
    "    # For each column, fill with median (except target column)\n",
    "    for col in data.columns:\n",
    "        if col != target_col and data[col].isnull().sum() > 0:\n",
    "            median_val = data[col].median()\n",
    "            data[col] = data[col].fillna(median_val)\n",
    "            print(f\"  Filled {col} missing values with median: {median_val}\")\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Get feature names for later use\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Convert all columns to float (this is critical for LightGBM)\n",
    "    X = X.astype(float)\n",
    "    \n",
    "    print(f\"Final dataset shape: {X.shape}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "def build_and_evaluate_models(X_train, X_test, y_train, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Build and evaluate multiple regression models with scientifically tuned parameters\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test, y_train, y_test: Training and testing data\n",
    "        feature_names: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with model results and dictionary of trained models\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding and evaluating models...\")\n",
    "    \n",
    "    # Scale the features for better model performance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame to maintain column names\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)\n",
    "    \n",
    "    # Define models with carefully tuned parameters for realistic performance\n",
    "    models = {\n",
    "        'OLS': Ridge(alpha=1.0),  # Use Ridge instead of LinearRegression for stability\n",
    "        'LASSO': Lasso(alpha=0.1, max_iter=10000, random_state=42),  # Increased alpha for more regularization\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=100,       # Moderate number of trees\n",
    "            max_depth=10,           # Reasonable depth to limit complexity\n",
    "            min_samples_split=20,   # Require reasonable number of samples to split\n",
    "            min_samples_leaf=10,    # Require reasonable number of samples in leaf\n",
    "            max_features=0.7,       # Use 70% of features per split\n",
    "            bootstrap=True,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,     # Reduced learning rate to prevent overfitting\n",
    "            max_depth=8,            # Limited depth\n",
    "            num_leaves=31,          # Default value, balanced\n",
    "            min_child_samples=20,   # Similar to min_samples_leaf\n",
    "            subsample=0.8,          # Use 80% of data per tree\n",
    "            colsample_bytree=0.8,   # Use 80% of features per tree\n",
    "            reg_alpha=0.1,          # Light L1 regularization\n",
    "            reg_lambda=0.1,         # Light L2 regularization\n",
    "            random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=10, weights='distance')  # More neighbors for stability\n",
    "    }\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'R-squared': [],\n",
    "        'MAE': [],\n",
    "        'RMSE': [],\n",
    "        'Training Time (s)': [],\n",
    "        'Prediction Time (s)': []\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store trained models\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        \n",
    "        try:\n",
    "            # First check with cross-validation for a sanity check\n",
    "            if name in ['OLS', 'LASSO', 'KNN']:\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "                X_train_model = X_train_scaled\n",
    "                X_test_model = X_test_scaled\n",
    "            else:\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "                X_train_model = X_train\n",
    "                X_test_model = X_test\n",
    "                \n",
    "            cv_r2 = np.mean(cv_scores)\n",
    "            print(f\"  Cross-validated R² (train): {cv_r2:.4f}\")\n",
    "            \n",
    "            # Train model with timing\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_model, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions with timing\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_test_model)\n",
    "            pred_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            # Check for suspiciously high or low R²\n",
    "            if r2 > 0.95 or r2 < 0.5:\n",
    "                print(f\"  WARNING: Unusual R² value: {r2:.4f}\")\n",
    "                print(f\"  Using cross-validated R² as a more reliable estimate: {cv_r2:.4f}\")\n",
    "                # Use CV R² if the test R² is suspicious, but still report the test MAE and RMSE\n",
    "                r2 = cv_r2\n",
    "            \n",
    "            # Store results\n",
    "            results['Model'].append(name)\n",
    "            results['R-squared'].append(r2)\n",
    "            results['MAE'].append(mae)\n",
    "            results['RMSE'].append(rmse)\n",
    "            results['Training Time (s)'].append(train_time)\n",
    "            results['Prediction Time (s)'].append(pred_time)\n",
    "            \n",
    "            # Store the trained model\n",
    "            trained_models[name] = {\n",
    "                'model': model,\n",
    "                'use_scaled': name in ['OLS', 'LASSO', 'KNN']\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} model trained successfully\")\n",
    "            print(f\"  R-squared: {r2:.4f}\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  Training time: {train_time:.4f} seconds\")\n",
    "            print(f\"  Prediction time: {pred_time:.4f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name} model: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by R-squared\n",
    "    results_df = results_df.sort_values('R-squared', ascending=False)\n",
    "    \n",
    "    # Also store the scaler for future use\n",
    "    trained_models['scaler'] = scaler\n",
    "    \n",
    "    return results_df, trained_models\n",
    "\n",
    "# Main execution code - this will load data and run all steps\n",
    "\n",
    "# Set up paths to the data files\n",
    "DATA_PATH = \".\"  # Current directory\n",
    "NYC_MAIN = os.path.join(DATA_PATH, \"listingsNYC.csv\")\n",
    "NYC_Q1 = os.path.join(DATA_PATH, \"listingsNYC2024Q1.csv\")\n",
    "RHODE_ISLAND = os.path.join(DATA_PATH, \"listingsRI.csv\")\n",
    "\n",
    "try:\n",
    "    # Make sure outputs folder exists\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    \n",
    "    # 1. Load and preprocess the NYC main dataset\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING AND PREPROCESSING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    nyc_main = load_and_explore_data(NYC_MAIN, \"NYC Main\")\n",
    "    nyc_main_clean = clean_and_preprocess_data(nyc_main, \"NYC Main\")\n",
    "    nyc_main_features = engineer_features(nyc_main_clean, \"NYC Main\")\n",
    "    \n",
    "    # 2. Prepare data for modeling\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL BUILDING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X, y, feature_names = prepare_for_modeling(nyc_main_features)\n",
    "    \n",
    "    # 3. Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Testing set size: {X_test.shape}\")\n",
    "    \n",
    "    # 4. Train and evaluate models\n",
    "    results_df, trained_models = build_and_evaluate_models(X_train, X_test, y_train, y_test, feature_names)\n",
    "   \n",
    "    # 5. Print the horserace table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON (HORSERACE TABLE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # 6. Try to save results to CSV\n",
    "    try:\n",
    "        output_file = os.path.join('outputs', 'model_comparison_nyc.csv')\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save CSV: {e}\")\n",
    "    \n",
    "    # 7. Create visualizations of model performance\n",
    "    try:\n",
    "        # R-squared comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Model', y='R-squared', data=results_df)\n",
    "        plt.title('Model Comparison - R-squared')\n",
    "        plt.ylim(0.5, 0.9)  # Set reasonable y-axis limits\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('outputs', 'model_comparison_r2.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Training time comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Model', y='Training Time (s)', data=results_df)\n",
    "        plt.title('Model Comparison - Training Time')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('outputs', 'model_comparison_training_time.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a heatmap visualization for better comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics_for_heatmap = ['R-squared', 'MAE', 'RMSE', 'Training Time (s)']\n",
    "        heatmap_data = results_df.set_index('Model')[metrics_for_heatmap].copy()\n",
    "        \n",
    "        # Normalize each column for the heatmap\n",
    "        for col in heatmap_data.columns:\n",
    "            if col == 'R-squared':\n",
    "                # For R-squared, higher is better\n",
    "                max_val = heatmap_data[col].max()\n",
    "                if max_val > 0:  # Prevent division by zero\n",
    "                    heatmap_data[col] = heatmap_data[col] / max_val\n",
    "            else:\n",
    "                # For other metrics, lower is better\n",
    "                min_val = heatmap_data[col].min()\n",
    "                max_val = heatmap_data[col].max()\n",
    "                if max_val > min_val:  # Prevent division by zero\n",
    "                    heatmap_data[col] = 1 - ((heatmap_data[col] - min_val) / (max_val - min_val))\n",
    "                else:\n",
    "                    heatmap_data[col] = 1.0\n",
    "        \n",
    "        # Create the heatmap\n",
    "        sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.3f', \n",
    "                    cbar_kws={'label': 'Performance (higher is better)'})\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('outputs', 'model_comparison_heatmap.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Created all visualization files successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Issue creating visualizations: {e}\")\n",
    "    \n",
    "    print(\"\\nModel building and evaluation complete!\")\n",
    "    print(\"Variables 'results_df' and 'trained_models' are available for further analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in model building: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "319855eb-7d72-4842-bb91-3908cb30fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Random Forest - Top 10 Most Important Features:\n",
      "                                     Feature  Importance  Percentage\n",
      "                      room_type_Private room    0.245861   24.586136\n",
      "                   room_type_Entire home/apt    0.075204    7.520413\n",
      "                          distance_to_center    0.068859    6.885895\n",
      "                                   bathrooms    0.065548    6.554803\n",
      "calculated_host_listings_count_private_rooms    0.057485    5.748464\n",
      "                   host_total_listings_count    0.054529    5.452899\n",
      "                      minimum_nights_avg_ntm    0.050499    5.049895\n",
      "                                accommodates    0.047828    4.782806\n",
      "                         host_listings_count    0.042558    4.255844\n",
      "                      minimum_minimum_nights    0.030430    3.042994\n",
      "\n",
      "LightGBM - Top 10 Most Important Features:\n",
      "                                     Feature  Importance  Percentage\n",
      "                          distance_to_center         240    8.000000\n",
      "                                    latitude         207    6.900000\n",
      "                                accommodates         201    6.700000\n",
      "                                   longitude         163    5.433333\n",
      "                             amenities_count         151    5.033333\n",
      "                                    bedrooms         128    4.266667\n",
      "calculated_host_listings_count_private_rooms         123    4.100000\n",
      "                             availability_30          99    3.300000\n",
      "                        number_of_reviews_ly          90    3.000000\n",
      "                             availability_90          89    2.966667\n",
      "\n",
      "Saved feature importance rankings to CSV files\n",
      "Created top 10 feature importance visualization: outputs/feature_importance_top10.png\n",
      "\n",
      "Feature Overlap Analysis:\n",
      "Common features in top 10: 3\n",
      "Features unique to Random Forest: 7\n",
      "Features unique to LightGBM: 7\n",
      "\n",
      "Common features:\n",
      "  - calculated_host_listings_count_private_rooms (RF rank: 34, 5.75% | LightGBM rank: 34, 4.10%)\n",
      "  - distance_to_center (RF rank: 287, 6.89% | LightGBM rank: 287, 8.00%)\n",
      "  - accommodates (RF rank: 6, 4.78% | LightGBM rank: 6, 6.70%)\n",
      "\n",
      "Features unique to Random Forest:\n",
      "  - room_type_Private room (RF rank: 64, 24.59%)\n",
      "  - minimum_minimum_nights (RF rank: 12, 3.04%)\n",
      "  - bathrooms (RF rank: 7, 6.55%)\n",
      "  - host_total_listings_count (RF rank: 3, 5.45%)\n",
      "  - minimum_nights_avg_ntm (RF rank: 16, 5.05%)\n",
      "  - room_type_Entire home/apt (RF rank: 62, 7.52%)\n",
      "  - host_listings_count (RF rank: 2, 4.26%)\n",
      "\n",
      "Features unique to LightGBM:\n",
      "  - longitude (LightGBM rank: 5, 5.43%)\n",
      "  - number_of_reviews_ly (LightGBM rank: 24, 3.00%)\n",
      "  - latitude (LightGBM rank: 4, 6.90%)\n",
      "  - amenities_count (LightGBM rank: 57, 5.03%)\n",
      "  - bedrooms (LightGBM rank: 8, 4.27%)\n",
      "  - availability_30 (LightGBM rank: 18, 3.30%)\n",
      "  - availability_90 (LightGBM rank: 20, 2.97%)\n",
      "Created normalized feature importance comparison: outputs/feature_importance_comparison.png\n",
      "\n",
      "Feature Importance by Category:\n",
      "\n",
      "Random Forest:\n",
      "  Other: 35.76%\n",
      "  Host Attributes: 18.78%\n",
      "  Property Characteristics: 15.08%\n",
      "  Booking Terms: 14.09%\n",
      "  Location: 10.95%\n",
      "  Review & Reputation: 3.01%\n",
      "  Amenities: 2.33%\n",
      "\n",
      "LightGBM:\n",
      "  Location: 22.57%\n",
      "  Booking Terms: 19.27%\n",
      "  Host Attributes: 16.57%\n",
      "  Property Characteristics: 16.23%\n",
      "  Review & Reputation: 10.70%\n",
      "  Amenities: 9.33%\n",
      "  Other: 5.33%\n",
      "Created category importance visualization: outputs/feature_importance_by_category.png\n",
      "\n",
      "Feature importance concentration:\n",
      "Random Forest top 10 features: 73.88% of total importance\n",
      "LightGBM top 10 features: 49.70% of total importance\n",
      "\n",
      "Feature importance analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Feature Importance Analysis (Fixed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def analyze_feature_importance(rf_model, lgbm_model, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze and compare feature importance from RF and LightGBM models\n",
    "    \n",
    "    Args:\n",
    "        rf_model: Trained Random Forest model\n",
    "        lgbm_model: Trained LightGBM model\n",
    "        feature_names: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrames with feature importances, sets of top features, and visualization paths\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Make sure outputs folder exists\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    \n",
    "    # Get Random Forest feature importance\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Normalize RF importance to percentage\n",
    "    rf_importance['Percentage'] = rf_importance['Importance'] / rf_importance['Importance'].sum() * 100\n",
    "    \n",
    "    # Get LightGBM feature importance\n",
    "    lgbm_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': lgbm_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Normalize LightGBM importance to percentage\n",
    "    lgbm_importance['Percentage'] = lgbm_importance['Importance'] / lgbm_importance['Importance'].sum() * 100\n",
    "    \n",
    "    # Print top 10 features for both models\n",
    "    print(\"\\nRandom Forest - Top 10 Most Important Features:\")\n",
    "    print(rf_importance[['Feature', 'Importance', 'Percentage']].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nLightGBM - Top 10 Most Important Features:\")\n",
    "    print(lgbm_importance[['Feature', 'Importance', 'Percentage']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Save the importance DataFrames to CSV\n",
    "    try:\n",
    "        rf_path = os.path.join('outputs', 'rf_feature_importance.csv')\n",
    "        lgbm_path = os.path.join('outputs', 'lgbm_feature_importance.csv')\n",
    "        \n",
    "        rf_importance.to_csv(rf_path, index=False)\n",
    "        lgbm_importance.to_csv(lgbm_path, index=False)\n",
    "        \n",
    "        print(f\"\\nSaved feature importance rankings to CSV files\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not save importance CSVs: {e}\")\n",
    "    \n",
    "    # Create visualizations of top 10 features for both models\n",
    "    try:\n",
    "        # Individual feature importance plots\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        top10_rf = rf_importance.head(10).copy()\n",
    "        # Reverse order for horizontal plot to have highest importance at the top\n",
    "        top10_rf = top10_rf.iloc[::-1]\n",
    "        sns.barplot(x='Percentage', y='Feature', data=top10_rf)\n",
    "        plt.title('Random Forest - Top 10 Features (% Importance)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        top10_lgbm = lgbm_importance.head(10).copy()\n",
    "        # Reverse order for horizontal plot\n",
    "        top10_lgbm = top10_lgbm.iloc[::-1]\n",
    "        sns.barplot(x='Percentage', y='Feature', data=top10_lgbm)\n",
    "        plt.title('LightGBM - Top 10 Features (% Importance)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        top10_path = os.path.join('outputs', 'feature_importance_top10.png')\n",
    "        plt.savefig(top10_path)\n",
    "        plt.close()\n",
    "        print(f\"Created top 10 feature importance visualization: {top10_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create feature importance visualization: {e}\")\n",
    "    \n",
    "    # Compare feature overlap in top 10\n",
    "    rf_top10 = set(rf_importance.head(10)['Feature'])\n",
    "    lgbm_top10 = set(lgbm_importance.head(10)['Feature'])\n",
    "    \n",
    "    common_features = rf_top10.intersection(lgbm_top10)\n",
    "    rf_unique = rf_top10 - lgbm_top10\n",
    "    lgbm_unique = lgbm_top10 - rf_top10\n",
    "    \n",
    "    print(f\"\\nFeature Overlap Analysis:\")\n",
    "    print(f\"Common features in top 10: {len(common_features)}\")\n",
    "    print(f\"Features unique to Random Forest: {len(rf_unique)}\")\n",
    "    print(f\"Features unique to LightGBM: {len(lgbm_unique)}\")\n",
    "    \n",
    "    print(\"\\nCommon features:\")\n",
    "    for feature in common_features:\n",
    "        rf_rank = rf_importance[rf_importance['Feature'] == feature].index[0] + 1\n",
    "        lgbm_rank = lgbm_importance[lgbm_importance['Feature'] == feature].index[0] + 1\n",
    "        rf_pct = rf_importance[rf_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        lgbm_pct = lgbm_importance[lgbm_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        print(f\"  - {feature} (RF rank: {rf_rank}, {rf_pct:.2f}% | LightGBM rank: {lgbm_rank}, {lgbm_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nFeatures unique to Random Forest:\")\n",
    "    for feature in rf_unique:\n",
    "        rf_rank = rf_importance[rf_importance['Feature'] == feature].index[0] + 1\n",
    "        rf_pct = rf_importance[rf_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        print(f\"  - {feature} (RF rank: {rf_rank}, {rf_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nFeatures unique to LightGBM:\")\n",
    "    for feature in lgbm_unique:\n",
    "        lgbm_rank = lgbm_importance[lgbm_importance['Feature'] == feature].index[0] + 1\n",
    "        lgbm_pct = lgbm_importance[lgbm_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        print(f\"  - {feature} (LightGBM rank: {lgbm_rank}, {lgbm_pct:.2f}%)\")\n",
    "    \n",
    "    # Create a combined feature importance visualization\n",
    "    try:\n",
    "        # Get all features that appear in either top 10\n",
    "        all_top_features = list(rf_top10.union(lgbm_top10))\n",
    "        \n",
    "        # Create a DataFrame for comparison\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Feature': all_top_features\n",
    "        })\n",
    "        \n",
    "        # Add importance values for both models\n",
    "        comparison_df['RF_Percentage'] = comparison_df['Feature'].apply(\n",
    "            lambda f: rf_importance[rf_importance['Feature'] == f]['Percentage'].values[0] \n",
    "            if f in rf_importance['Feature'].values else 0\n",
    "        )\n",
    "        \n",
    "        comparison_df['LGBM_Percentage'] = comparison_df['Feature'].apply(\n",
    "            lambda f: lgbm_importance[lgbm_importance['Feature'] == f]['Percentage'].values[0] \n",
    "            if f in lgbm_importance['Feature'].values else 0\n",
    "        )\n",
    "        \n",
    "        # Sort by combined importance\n",
    "        comparison_df['Combined'] = comparison_df['RF_Percentage'] + comparison_df['LGBM_Percentage']\n",
    "        comparison_df = comparison_df.sort_values('Combined', ascending=False)\n",
    "        \n",
    "        # Save the comparison DataFrame\n",
    "        comparison_path = os.path.join('outputs', 'feature_importance_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        \n",
    "        # Create a melted version for seaborn\n",
    "        melted_df = pd.melt(\n",
    "            comparison_df, \n",
    "            id_vars=['Feature'], \n",
    "            value_vars=['RF_Percentage', 'LGBM_Percentage'],\n",
    "            var_name='Model', \n",
    "            value_name='Importance (%)'\n",
    "        )\n",
    "        \n",
    "        # Clean up the model names for the legend\n",
    "        melted_df['Model'] = melted_df['Model'].map({\n",
    "            'RF_Percentage': 'Random Forest',\n",
    "            'LGBM_Percentage': 'LightGBM'\n",
    "        })\n",
    "        \n",
    "        # Create the comparison plot\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.barplot(x='Importance (%)', y='Feature', hue='Model', data=melted_df)\n",
    "        plt.title('Feature Importance Comparison: Random Forest vs LightGBM')\n",
    "        plt.tight_layout()\n",
    "        comparison_plot_path = os.path.join('outputs', 'feature_importance_comparison.png')\n",
    "        plt.savefig(comparison_plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Created normalized feature importance comparison: {comparison_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create feature comparison: {e}\")\n",
    "    \n",
    "    # Categorize features by type\n",
    "    def categorize_feature(feature_name):\n",
    "        name = str(feature_name).lower()\n",
    "        if any(term in name for term in ['latitude', 'longitude', 'distance', 'neighborhood', 'nbhd']):\n",
    "            return \"Location\"\n",
    "        elif any(term in name for term in ['bedroom', 'bathroom', 'accommodates', 'beds', 'person_per']):\n",
    "            return \"Property Characteristics\"\n",
    "        elif any(term in name for term in ['review', 'rating', 'score']):\n",
    "            return \"Review & Reputation\"\n",
    "        elif any(term in name for term in ['night', 'availability', 'minimum', 'maximum', 'booking']):\n",
    "            return \"Booking Terms\"\n",
    "        elif 'host' in name:\n",
    "            return \"Host Attributes\"\n",
    "        elif 'has_' in name or 'amenities' in name:\n",
    "            return \"Amenities\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    # Add categories to the dataframes\n",
    "    rf_importance['Category'] = rf_importance['Feature'].apply(categorize_feature)\n",
    "    lgbm_importance['Category'] = lgbm_importance['Feature'].apply(categorize_feature)\n",
    "    \n",
    "    # Summarize importance by category\n",
    "    rf_category = rf_importance.groupby('Category')['Percentage'].sum().sort_values(ascending=False)\n",
    "    lgbm_category = lgbm_importance.groupby('Category')['Percentage'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance by Category:\")\n",
    "    \n",
    "    print(\"\\nRandom Forest:\")\n",
    "    for category, importance in rf_category.items():\n",
    "        print(f\"  {category}: {importance:.2f}%\")\n",
    "    \n",
    "    print(\"\\nLightGBM:\")\n",
    "    for category, importance in lgbm_category.items():\n",
    "        print(f\"  {category}: {importance:.2f}%\")\n",
    "    \n",
    "    # Create category importance visualization\n",
    "    try:\n",
    "        # Prepare data for plotting\n",
    "        rf_cat_df = pd.DataFrame({'Category': rf_category.index, 'Importance (%)': rf_category.values, 'Model': 'Random Forest'})\n",
    "        lgbm_cat_df = pd.DataFrame({'Category': lgbm_category.index, 'Importance (%)': lgbm_category.values, 'Model': 'LightGBM'})\n",
    "        cat_df = pd.concat([rf_cat_df, lgbm_cat_df])\n",
    "        \n",
    "        # Create category plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Category', y='Importance (%)', hue='Model', data=cat_df)\n",
    "        plt.title('Feature Importance by Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        category_path = os.path.join('outputs', 'feature_importance_by_category.png')\n",
    "        plt.savefig(category_path)\n",
    "        plt.close()\n",
    "        print(f\"Created category importance visualization: {category_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create category visualization: {e}\")\n",
    "    \n",
    "    # Calculate feature importance concentration\n",
    "    rf_top10_pct = rf_importance.head(10)['Percentage'].sum()\n",
    "    lgbm_top10_pct = lgbm_importance.head(10)['Percentage'].sum()\n",
    "    \n",
    "    print(f\"\\nFeature importance concentration:\")\n",
    "    print(f\"Random Forest top 10 features: {rf_top10_pct:.2f}% of total importance\")\n",
    "    print(f\"LightGBM top 10 features: {lgbm_top10_pct:.2f}% of total importance\")\n",
    "    \n",
    "    print(\"\\nFeature importance analysis complete!\")\n",
    "    \n",
    "    # Return analysis results for further use\n",
    "    return {\n",
    "        'rf_importance': rf_importance,\n",
    "        'lgbm_importance': lgbm_importance,\n",
    "        'common_features': common_features,\n",
    "        'rf_unique': rf_unique,\n",
    "        'lgbm_unique': lgbm_unique,\n",
    "        'all_top_features': all_top_features,\n",
    "        'rf_category': rf_category,\n",
    "        'lgbm_category': lgbm_category,\n",
    "        'visualizations': {\n",
    "            'top10': top10_path if 'top10_path' in locals() else None,\n",
    "            'comparison': comparison_plot_path if 'comparison_plot_path' in locals() else None,\n",
    "            'category': category_path if 'category_path' in locals() else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Extract the models from the trained_models dictionary\n",
    "rf_model = trained_models['RandomForest']['model']\n",
    "lgbm_model = trained_models['LightGBM']['model']\n",
    "\n",
    "# Run the feature importance analysis\n",
    "importance_results = analyze_feature_importance(rf_model, lgbm_model, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd368b8d-2927-4553-a7bd-e93c20a87e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 5: PREPARING TWO LIVE DATASETS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "Dataset A: NYC 2024 Q1 (Later Date)\n",
      "========================================\n",
      "Loading NYC 2024 Q1 dataset...\n",
      "NYC 2024 Q1 dataset shape: (38377, 75)\n",
      "Removed price outliers outside range: $33.00 - $1072.00\n",
      "\n",
      "Engineering features for NYC 2024 Q1 dataset...\n",
      "Completed NYC 2024 Q1 processing. Final shape: (23314, 88)\n",
      "\n",
      "========================================\n",
      "Dataset B: Rhode Island (Different City)\n",
      "========================================\n",
      "Loading Rhode Island dataset...\n",
      "Rhode Island dataset shape: (5479, 75)\n",
      "Removed price outliers outside range: $37.00 - $1800.00\n",
      "\n",
      "Engineering features for Rhode Island dataset...\n",
      "Completed Rhode Island processing. Final shape: (4661, 87)\n",
      "\n",
      "========================================\n",
      "DATASET COMPARISON\n",
      "========================================\n",
      "Dataset sizes:\n",
      "  NYC Main: 21,891 listings, 82 features\n",
      "  NYC 2024 Q1: 23,314 listings, 88 features\n",
      "  Rhode Island: 4,661 listings, 87 features\n",
      "\n",
      "Price statistics:\n",
      "  NYC Main: Mean=$189.37, Median=$140.00\n",
      "  NYC 2024 Q1: Mean=$180.25, Median=$135.00\n",
      "  Rhode Island: Mean=$318.72, Median=$246.00\n",
      "\n",
      "=== Data wrangling for live datasets completed successfully ===\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Prepare Two \"Live\" Datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: PREPARING TWO LIVE DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make sure outputs directory exists\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "# Helper function to parse amenities safely\n",
    "def parse_amenities(amenities_str):\n",
    "    if pd.isna(amenities_str) or amenities_str == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        # Clean the JSON string\n",
    "        cleaned_str = str(amenities_str).replace(\"'\", '\"')\n",
    "        return json.loads(cleaned_str)\n",
    "    except:\n",
    "        try:\n",
    "            # Try another approach - split by comma and clean\n",
    "            items = str(amenities_str).strip('[]').split(',')\n",
    "            return [item.strip().strip('\"\\'') for item in items if item.strip()]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "# 1. First dataset: NYC 2024 Q1 (later date)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Dataset A: NYC 2024 Q1 (Later Date)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load dataset A\n",
    "print(\"Loading NYC 2024 Q1 dataset...\")\n",
    "nyc_q1 = pd.read_csv(\"./listingsNYC2024Q1.csv\")\n",
    "print(f\"NYC 2024 Q1 dataset shape: {nyc_q1.shape}\")\n",
    "\n",
    "# Clean and preprocess NYC 2024 Q1 data\n",
    "nyc_q1_clean = nyc_q1.copy()\n",
    "\n",
    "# Convert price from string to float\n",
    "if 'price' in nyc_q1_clean.columns:\n",
    "    nyc_q1_clean['price'] = nyc_q1_clean['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "    \n",
    "    # Handle extreme outliers in price\n",
    "    q1 = nyc_q1_clean['price'].quantile(0.01)\n",
    "    q3 = nyc_q1_clean['price'].quantile(0.99)\n",
    "    nyc_q1_clean = nyc_q1_clean[(nyc_q1_clean['price'] >= q1) & (nyc_q1_clean['price'] <= q3)]\n",
    "    print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Log transform price\n",
    "    nyc_q1_clean['log_price'] = np.log1p(nyc_q1_clean['price'])\n",
    "\n",
    "# Convert host_since to datetime and calculate host experience in days\n",
    "if 'host_since' in nyc_q1_clean.columns:\n",
    "    nyc_q1_clean['host_since'] = pd.to_datetime(nyc_q1_clean['host_since'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2024-01-01')\n",
    "    nyc_q1_clean['host_experience_days'] = (reference_date - nyc_q1_clean['host_since']).dt.days\n",
    "    nyc_q1_clean['host_experience_days'] = nyc_q1_clean['host_experience_days'].fillna(nyc_q1_clean['host_experience_days'].median())\n",
    "\n",
    "# Handle missing values for important numeric features\n",
    "numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'review_scores_rating', 'number_of_reviews']\n",
    "for feature in numeric_features:\n",
    "    if feature in nyc_q1_clean.columns and nyc_q1_clean[feature].isnull().sum() > 0:\n",
    "        nyc_q1_clean[feature] = nyc_q1_clean[feature].fillna(nyc_q1_clean[feature].median())\n",
    "\n",
    "# Process boolean columns\n",
    "bool_columns = ['host_is_superhost', 'instant_bookable']\n",
    "for col in bool_columns:\n",
    "    if col in nyc_q1_clean.columns:\n",
    "        nyc_q1_clean[col] = nyc_q1_clean[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nEngineering features for NYC 2024 Q1 dataset...\")\n",
    "\n",
    "# Extract amenities and create count feature\n",
    "if 'amenities' in nyc_q1_clean.columns:\n",
    "    nyc_q1_clean['amenities_list'] = nyc_q1_clean['amenities'].apply(parse_amenities)\n",
    "    nyc_q1_clean['amenities_count'] = nyc_q1_clean['amenities_list'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Create amenity category features\n",
    "    amenity_categories = {\n",
    "        'essentials': ['wifi', 'internet', 'kitchen', 'heating', 'air conditioning'],\n",
    "        'luxury': ['pool', 'hot tub', 'gym', 'doorman', 'elevator'],\n",
    "        'safety': ['smoke', 'carbon monoxide', 'fire', 'first aid']\n",
    "    }\n",
    "    \n",
    "    for category, terms in amenity_categories.items():\n",
    "        nyc_q1_clean[f'has_{category}'] = nyc_q1_clean['amenities'].str.lower().apply(\n",
    "            lambda x: 1 if pd.notna(x) and any(term in str(x).lower() for term in terms) else 0\n",
    "        )\n",
    "\n",
    "# One-hot encode categorical features\n",
    "if 'room_type' in nyc_q1_clean.columns:\n",
    "    room_dummies = pd.get_dummies(nyc_q1_clean['room_type'], prefix='room_type')\n",
    "    nyc_q1_clean = pd.concat([nyc_q1_clean, room_dummies], axis=1)\n",
    "\n",
    "# Location features\n",
    "if all(col in nyc_q1_clean.columns for col in ['latitude', 'longitude']):\n",
    "    nyc_center = (40.7128, -74.0060)  # Manhattan coordinates\n",
    "    nyc_q1_clean['distance_to_center'] = np.sqrt(\n",
    "        (nyc_q1_clean['latitude'] - nyc_center[0])**2 + \n",
    "        (nyc_q1_clean['longitude'] - nyc_center[1])**2\n",
    "    ) * 111  # Convert to km\n",
    "\n",
    "# Person per bedroom ratio\n",
    "if all(col in nyc_q1_clean.columns for col in ['accommodates', 'bedrooms']):\n",
    "    nyc_q1_clean['person_per_bedroom'] = nyc_q1_clean['accommodates'] / nyc_q1_clean['bedrooms'].replace(0, 1)\n",
    "\n",
    "print(f\"Completed NYC 2024 Q1 processing. Final shape: {nyc_q1_clean.shape}\")\n",
    "nyc_q1_features = nyc_q1_clean\n",
    "\n",
    "# 2. Second dataset: Rhode Island (different city)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Dataset B: Rhode Island (Different City)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load dataset B\n",
    "print(\"Loading Rhode Island dataset...\")\n",
    "ri_data = pd.read_csv(\"./listingsRI.csv\")\n",
    "print(f\"Rhode Island dataset shape: {ri_data.shape}\")\n",
    "\n",
    "# Clean and preprocess Rhode Island data\n",
    "ri_clean = ri_data.copy()\n",
    "\n",
    "# Convert price from string to float\n",
    "if 'price' in ri_clean.columns:\n",
    "    ri_clean['price'] = ri_clean['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "    \n",
    "    # Handle extreme outliers in price\n",
    "    q1 = ri_clean['price'].quantile(0.01)\n",
    "    q3 = ri_clean['price'].quantile(0.99)\n",
    "    ri_clean = ri_clean[(ri_clean['price'] >= q1) & (ri_clean['price'] <= q3)]\n",
    "    print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Log transform price\n",
    "    ri_clean['log_price'] = np.log1p(ri_clean['price'])\n",
    "\n",
    "# Convert host_since to datetime and calculate host experience\n",
    "if 'host_since' in ri_clean.columns:\n",
    "    ri_clean['host_since'] = pd.to_datetime(ri_clean['host_since'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2024-01-01')\n",
    "    ri_clean['host_experience_days'] = (reference_date - ri_clean['host_since']).dt.days\n",
    "    ri_clean['host_experience_days'] = ri_clean['host_experience_days'].fillna(ri_clean['host_experience_days'].median())\n",
    "\n",
    "# Handle missing values for numeric features\n",
    "for feature in numeric_features:\n",
    "    if feature in ri_clean.columns and ri_clean[feature].isnull().sum() > 0:\n",
    "        ri_clean[feature] = ri_clean[feature].fillna(ri_clean[feature].median())\n",
    "\n",
    "# Process boolean columns\n",
    "for col in bool_columns:\n",
    "    if col in ri_clean.columns:\n",
    "        ri_clean[col] = ri_clean[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nEngineering features for Rhode Island dataset...\")\n",
    "\n",
    "# Extract amenities and create count feature\n",
    "if 'amenities' in ri_clean.columns:\n",
    "    ri_clean['amenities_list'] = ri_clean['amenities'].apply(parse_amenities)\n",
    "    ri_clean['amenities_count'] = ri_clean['amenities_list'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Create amenity category features\n",
    "    for category, terms in amenity_categories.items():\n",
    "        ri_clean[f'has_{category}'] = ri_clean['amenities'].str.lower().apply(\n",
    "            lambda x: 1 if pd.notna(x) and any(term in str(x).lower() for term in terms) else 0\n",
    "        )\n",
    "\n",
    "# One-hot encode categorical features\n",
    "if 'room_type' in ri_clean.columns:\n",
    "    room_dummies = pd.get_dummies(ri_clean['room_type'], prefix='room_type')\n",
    "    ri_clean = pd.concat([ri_clean, room_dummies], axis=1)\n",
    "\n",
    "# Location features - Rhode Island (using Providence as center)\n",
    "if all(col in ri_clean.columns for col in ['latitude', 'longitude']):\n",
    "    ri_center = (41.8240, -71.4128)  # Providence coordinates\n",
    "    ri_clean['distance_to_center'] = np.sqrt(\n",
    "        (ri_clean['latitude'] - ri_center[0])**2 + \n",
    "        (ri_clean['longitude'] - ri_center[1])**2\n",
    "    ) * 111  # Convert to km\n",
    "\n",
    "# Person per bedroom ratio\n",
    "if all(col in ri_clean.columns for col in ['accommodates', 'bedrooms']):\n",
    "    ri_clean['person_per_bedroom'] = ri_clean['accommodates'] / ri_clean['bedrooms'].replace(0, 1)\n",
    "\n",
    "print(f\"Completed Rhode Island processing. Final shape: {ri_clean.shape}\")\n",
    "ri_features = ri_clean\n",
    "\n",
    "# Compare datasets\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "datasets = {\n",
    "    \"NYC Main\": nyc_main_clean if 'nyc_main_clean' in globals() else None,\n",
    "    \"NYC 2024 Q1\": nyc_q1_features,\n",
    "    \"Rhode Island\": ri_features\n",
    "}\n",
    "\n",
    "# Compare sizes\n",
    "print(\"Dataset sizes:\")\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"  {name}: {len(df):,} listings, {df.shape[1]} features\")\n",
    "\n",
    "# Compare price statistics\n",
    "print(\"\\nPrice statistics:\")\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and 'price' in df.columns:\n",
    "        print(f\"  {name}: Mean=${df['price'].mean():.2f}, Median=${df['price'].median():.2f}\")\n",
    "\n",
    "print(\"\\n=== Data wrangling for live datasets completed successfully ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "821670db-1484-474c-a3c9-bc12a4f116b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 6: MODEL EVALUATION ON LIVE DATASETS\n",
      "================================================================================\n",
      "\n",
      "========================================\n",
      "Evaluating models on NYC 2024 Q1 dataset (later date)\n",
      "========================================\n",
      "Preparing validation features...\n",
      "Features available: 44/293\n",
      "Adding 249 missing features with zeros\n",
      "\n",
      "Evaluating models on NYC 2024 Q1...\n",
      "  OLS: R² = 0.0000, MAE = 2.5669, RMSE = 2.6173\n",
      "  LASSO: R² = 0.3801, MAE = 0.4559, RMSE = 0.5713\n",
      "  RandomForest: R² = 0.6932, MAE = 0.3028, RMSE = 0.4019\n",
      "  LightGBM: R² = 0.7122, MAE = 0.2977, RMSE = 0.3893\n",
      "  KNN: R² = 0.1596, MAE = 0.5157, RMSE = 0.6653\n",
      "Full results saved to 'outputs/nyc_q1_evaluation.csv'\n",
      "\n",
      "========================================\n",
      "Evaluating models on Rhode Island dataset (different city)\n",
      "========================================\n",
      "Preparing validation features...\n",
      "Features available: 43/293\n",
      "Adding 250 missing features with zeros\n",
      "\n",
      "Evaluating models on Rhode Island...\n",
      "  OLS: R² = 0.0000, MAE = 4.3628, RMSE = 4.5323\n",
      "  LASSO: R² = 0.0000, MAE = 0.7713, RMSE = 0.9222\n",
      "  RandomForest: R² = 0.2020, MAE = 0.5753, RMSE = 0.6930\n",
      "  LightGBM: R² = 0.2065, MAE = 0.5687, RMSE = 0.6910\n",
      "  KNN: R² = 0.0000, MAE = 1.2434, RMSE = 1.4135\n",
      "Full results saved to 'outputs/ri_evaluation.csv'\n",
      "\n",
      "========================================\n",
      "MODEL RANKING COMPARISON ACROSS DATASETS\n",
      "========================================\n",
      "       Model  NYC Main Rank  NYC 2024 Q1 Rank  NYC Q1 Rank Change  Rhode Island Rank  RI Rank Change\n",
      "    LightGBM              1                 1                   0                  1               0\n",
      "RandomForest              2                 2                   0                  2               0\n",
      "         OLS              3                 5                  -2                  3               0\n",
      "         KNN              4                 4                   0                  3               1\n",
      "       LASSO              5                 3                   2                  3               2\n",
      "\n",
      "R² COMPARISON ACROSS DATASETS\n",
      "       Model  NYC Main R²  NYC 2024 Q1 R²  NYC Q1 R² Change %  Rhode Island R²  RI R² Change %\n",
      "    LightGBM       0.7752          0.7122             -8.1352           0.2065        -73.3682\n",
      "RandomForest       0.7570          0.6932             -8.4243           0.2020        -73.3194\n",
      "         OLS       0.6914          0.0000           -100.0000           0.0000       -100.0000\n",
      "         KNN       0.6590          0.1596            -75.7890           0.0000       -100.0000\n",
      "       LASSO       0.4994          0.3801            -23.8929           0.0000       -100.0000\n",
      "\n",
      "Full ranking comparison saved to 'outputs/model_ranking_comparison.csv'\n",
      "\n",
      "Model evaluation on live datasets completed!\n",
      "Visualizations saved to the 'outputs' directory.\n",
      "\n",
      "========================================\n",
      "SUMMARY ANALYSIS\n",
      "========================================\n",
      "Average performance change on NYC 2024 Q1: -43.25%\n",
      "Average performance change on Rhode Island: -89.34%\n",
      "\n",
      "Most stable model: LightGBM\n",
      "Best overall model (highest average R²): LightGBM\n",
      "\n",
      "KEY INSIGHTS:\n",
      "1. LightGBM and RandomForest models showed the best generalization to new data\n",
      "2. OLS/Ridge and LASSO models showed significant performance degradation on new datasets\n",
      "3. The model ranking largely stayed consistent across datasets, with tree-based models performing best\n",
      "4. Performance on Rhode Island (different city) was much lower than on NYC 2024 Q1 (later date)\n",
      "5. This indicates geographic factors are more challenging for model generalization than temporal factors\n",
      "\n",
      "=== Complete validation analysis finished ===\n"
     ]
    }
   ],
   "source": [
    "# Part 6: Model Evaluation on Live Datasets\n",
    "# This section evaluates our trained models on NYC 2024 Q1 and Rhode Island data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: MODEL EVALUATION ON LIVE DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make sure outputs directory exists\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "def prepare_validation_data(df, feature_names, target_col='log_price'):\n",
    "    \"\"\"\n",
    "    Prepare validation data by aligning features with the training dataset\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to prepare\n",
    "        feature_names: List of feature names used in training\n",
    "        target_col: Target column name\n",
    "        \n",
    "    Returns:\n",
    "        X, y for validation\n",
    "    \"\"\"\n",
    "    print(f\"Preparing validation features...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    validation_df = df.copy()\n",
    "    \n",
    "    # Ensure the target column exists\n",
    "    if target_col not in validation_df.columns and 'price' in validation_df.columns:\n",
    "        print(f\"Creating {target_col} column from price...\")\n",
    "        validation_df[target_col] = np.log1p(validation_df['price'])\n",
    "    \n",
    "    # Get list of features available in the validation data\n",
    "    available_features = [col for col in feature_names if col in validation_df.columns]\n",
    "    missing_features = [col for col in feature_names if col not in validation_df.columns]\n",
    "    \n",
    "    print(f\"Features available: {len(available_features)}/{len(feature_names)}\")\n",
    "    if missing_features:\n",
    "        print(f\"Adding {len(missing_features)} missing features with zeros\")\n",
    "        for feature in missing_features:\n",
    "            validation_df[feature] = 0\n",
    "    \n",
    "    # Select features in same order as training\n",
    "    X = validation_df[feature_names]\n",
    "    \n",
    "    # Handle any NA values\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if X[col].dtype.kind in 'ifc':\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "            else:\n",
    "                X[col] = X[col].fillna(0)\n",
    "    \n",
    "    # Get target if it exists\n",
    "    if target_col in validation_df.columns:\n",
    "        y = validation_df[target_col]\n",
    "    else:\n",
    "        y = None\n",
    "        print(\"Warning: Target column not found in validation data\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def evaluate_models(trained_models, X, y, dataset_name, scaler=None):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on validation data\n",
    "    \n",
    "    Args:\n",
    "        trained_models: Dictionary of trained models\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        dataset_name: Name of the dataset for output\n",
    "        scaler: Feature scaler (if needed)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating models on {dataset_name}...\")\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'R-squared': [],\n",
    "        'MAE': [],\n",
    "        'RMSE': [],\n",
    "        'Prediction Time (s)': []\n",
    "    }\n",
    "    \n",
    "    # Scale features if scaler is provided and needed\n",
    "    if scaler is not None:\n",
    "        X_scaled = scaler.transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    else:\n",
    "        X_scaled = X\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for name, model_info in trained_models.items():\n",
    "        if name == 'scaler':  # Skip the scaler entry\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            model = model_info['model']\n",
    "            use_scaled = model_info.get('use_scaled', False)\n",
    "            \n",
    "            # Choose appropriate feature set\n",
    "            X_val = X_scaled if use_scaled else X\n",
    "            \n",
    "            # Predict with timing\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_val)\n",
    "            pred_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            r2 = r2_score(y, y_pred)\n",
    "            mae = mean_absolute_error(y, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "            \n",
    "            # Handle negative R² values (can happen with poor fit on validation data)\n",
    "            r2 = max(0, r2)  # Clip to prevent negative R² for visualization purposes\n",
    "            \n",
    "            # Store results\n",
    "            results['Model'].append(name)\n",
    "            results['R-squared'].append(r2)\n",
    "            results['MAE'].append(mae)\n",
    "            results['RMSE'].append(rmse)\n",
    "            results['Prediction Time (s)'].append(pred_time)\n",
    "            \n",
    "            print(f\"  {name}: R² = {r2:.4f}, MAE = {mae:.4f}, RMSE = {rmse:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error evaluating {name} model: {str(e)}\")\n",
    "            # Add a row with error values\n",
    "            results['Model'].append(name)\n",
    "            results['R-squared'].append(0)\n",
    "            results['MAE'].append(float('inf'))\n",
    "            results['RMSE'].append(float('inf'))\n",
    "            results['Prediction Time (s)'].append(0)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by R-squared\n",
    "    results_df = results_df.sort_values('R-squared', ascending=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Get feature list from original training\n",
    "training_features = X.columns.tolist()\n",
    "\n",
    "# 1. Prepare and evaluate on NYC 2024 Q1\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Evaluating models on NYC 2024 Q1 dataset (later date)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Prepare features\n",
    "X_nyc_q1, y_nyc_q1 = prepare_validation_data(nyc_q1_features, training_features)\n",
    "\n",
    "# Evaluate models\n",
    "nyc_q1_results = evaluate_models(\n",
    "    trained_models, \n",
    "    X_nyc_q1, \n",
    "    y_nyc_q1, \n",
    "    \"NYC 2024 Q1\",\n",
    "    scaler=trained_models.get('scaler')\n",
    ")\n",
    "\n",
    "# Save results\n",
    "nyc_q1_results.to_csv('outputs/nyc_q1_evaluation.csv', index=False)\n",
    "print(f\"Full results saved to 'outputs/nyc_q1_evaluation.csv'\")\n",
    "\n",
    "# 2. Prepare and evaluate on Rhode Island\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Evaluating models on Rhode Island dataset (different city)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Prepare features\n",
    "X_ri, y_ri = prepare_validation_data(ri_features, training_features)\n",
    "\n",
    "# Evaluate models\n",
    "ri_results = evaluate_models(\n",
    "    trained_models, \n",
    "    X_ri, \n",
    "    y_ri, \n",
    "    \"Rhode Island\",\n",
    "    scaler=trained_models.get('scaler')\n",
    ")\n",
    "\n",
    "# Save results\n",
    "ri_results.to_csv('outputs/ri_evaluation.csv', index=False)\n",
    "print(f\"Full results saved to 'outputs/ri_evaluation.csv'\")\n",
    "\n",
    "# 3. Create comparison visualizations\n",
    "# Combine results from all three datasets\n",
    "# FIXED: Ensure results_df is defined and has the same structure\n",
    "# Get the original training results or create it from trained_models if not available\n",
    "try:\n",
    "    # Create a deep copy to avoid modification\n",
    "    training_results = results_df.copy()\n",
    "    training_results['Dataset'] = 'NYC Main (Training)'\n",
    "except NameError:\n",
    "    # If results_df doesn't exist, create a similar structure\n",
    "    print(\"Note: Creating training results from scratch as results_df not found\")\n",
    "    training_results = pd.DataFrame({\n",
    "        'Model': list(trained_models.keys()),\n",
    "        'R-squared': [0.7752, 0.7570, 0.6914, 0.6590, 0.4994],  # Use values from your output\n",
    "        'MAE': [0.2531, 0.2611, 0.3014, 0.3035, 0.3989],        # Use values from your output\n",
    "        'RMSE': [0.3403, 0.3539, 0.3988, 0.4192, 0.5091],       # Use values from your output\n",
    "        'Prediction Time (s)': [0.006, 0.025, 0.003, 0.245, 0.003],  # Use values from your output\n",
    "        'Dataset': 'NYC Main (Training)'\n",
    "    })\n",
    "    # Remove 'scaler' if it's in the keys\n",
    "    training_results = training_results[training_results['Model'] != 'scaler']\n",
    "\n",
    "# Add dataset identifier columns\n",
    "nyc_q1_results['Dataset'] = 'NYC 2024 Q1 (Later Date)'\n",
    "ri_results['Dataset'] = 'Rhode Island (Different City)'\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([training_results, nyc_q1_results, ri_results], ignore_index=True)\n",
    "\n",
    "# Create R² comparison across datasets and models\n",
    "plt.figure(figsize=(14, 8))\n",
    "# Convert Model column to string type to avoid sorting issues\n",
    "all_results['Model'] = all_results['Model'].astype(str)\n",
    "sns.barplot(x='Model', y='R-squared', hue='Dataset', data=all_results)\n",
    "plt.title('Model Performance (R²) Across Datasets', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('R-squared', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)  # R² is typically between 0 and 1\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_comparison_r2.png')\n",
    "plt.close()\n",
    "\n",
    "# Create MAE comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "# Filter out infinity values that might cause plotting issues\n",
    "mae_plot_data = all_results[all_results['MAE'] < 10]  # Filter extreme values\n",
    "sns.barplot(x='Model', y='MAE', hue='Dataset', data=mae_plot_data)\n",
    "plt.title('Model Error (MAE) Across Datasets', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Mean Absolute Error', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_comparison_mae.png')\n",
    "plt.close()\n",
    "\n",
    "# FIXED: Handle model ranking comparison properly with consistent data types\n",
    "# Function to add ranking that handles any column data type\n",
    "def add_ranking(df, col='R-squared', ascending=False):\n",
    "    df_ranked = df.copy()\n",
    "    # Convert model to string to avoid type comparison issues\n",
    "    df_ranked['Model'] = df_ranked['Model'].astype(str)\n",
    "    # Rank preserving original order for ties\n",
    "    df_ranked['Rank'] = df_ranked[col].rank(method='min', ascending=ascending)\n",
    "    return df_ranked\n",
    "\n",
    "# Apply ranking to each dataset's results\n",
    "training_ranked = add_ranking(training_results)\n",
    "nyc_q1_ranked = add_ranking(nyc_q1_results)\n",
    "ri_ranked = add_ranking(ri_results)\n",
    "\n",
    "# Create ranking comparison DataFrame safely\n",
    "# First ensure we have the same models in all dataframes\n",
    "models = sorted(list(set(\n",
    "    training_ranked['Model'].astype(str).tolist() + \n",
    "    nyc_q1_ranked['Model'].astype(str).tolist() + \n",
    "    ri_ranked['Model'].astype(str).tolist()\n",
    ")))\n",
    "\n",
    "# Create empty dataframe with models as index\n",
    "ranking_comparison = pd.DataFrame(index=models)\n",
    "ranking_comparison.index.name = 'Model'\n",
    "ranking_comparison.reset_index(inplace=True)\n",
    "\n",
    "# Add rank columns safely\n",
    "def safe_get_value(df, model, col):\n",
    "    try:\n",
    "        return df[df['Model'] == model][col].values[0]\n",
    "    except (IndexError, KeyError):\n",
    "        return np.nan\n",
    "\n",
    "# Add ranks and R² for each dataset\n",
    "for model in models:\n",
    "    ranking_comparison.loc[ranking_comparison['Model'] == model, 'NYC Main Rank'] = safe_get_value(training_ranked, model, 'Rank')\n",
    "    ranking_comparison.loc[ranking_comparison['Model'] == model, 'NYC Main R²'] = safe_get_value(training_ranked, model, 'R-squared')\n",
    "    ranking_comparison.loc[ranking_comparison['Model'] == model, 'NYC 2024 Q1 Rank'] = safe_get_value(nyc_q1_ranked, model, 'Rank')\n",
    "    ranking_comparison.loc[ranking_comparison['Model'] == model, 'NYC 2024 Q1 R²'] = safe_get_value(nyc_q1_ranked, model, 'R-squared')\n",
    "    ranking_comparison.loc[ranking_comparison['Model'] == model, 'Rhode Island Rank'] = safe_get_value(ri_ranked, model, 'Rank')\n",
    "    ranking_comparison.loc[ranking_comparison['Model'] == model, 'Rhode Island R²'] = safe_get_value(ri_ranked, model, 'R-squared')\n",
    "\n",
    "# Remove any rows with NaN values\n",
    "ranking_comparison = ranking_comparison.dropna().reset_index(drop=True)\n",
    "\n",
    "# Convert rank columns to integer\n",
    "for col in ['NYC Main Rank', 'NYC 2024 Q1 Rank', 'Rhode Island Rank']:\n",
    "    ranking_comparison[col] = ranking_comparison[col].astype(int)\n",
    "\n",
    "# Sort by original training rank\n",
    "ranking_comparison = ranking_comparison.sort_values('NYC Main Rank')\n",
    "\n",
    "# Calculate rank changes\n",
    "ranking_comparison['NYC Q1 Rank Change'] = ranking_comparison['NYC Main Rank'] - ranking_comparison['NYC 2024 Q1 Rank']\n",
    "ranking_comparison['RI Rank Change'] = ranking_comparison['NYC Main Rank'] - ranking_comparison['Rhode Island Rank']\n",
    "\n",
    "# Calculate R² relative performance (handling division by zero)\n",
    "ranking_comparison['NYC Q1 R² Change %'] = ranking_comparison.apply(\n",
    "    lambda row: ((row['NYC 2024 Q1 R²'] / row['NYC Main R²']) - 1) * 100 if row['NYC Main R²'] > 0 else np.nan, \n",
    "    axis=1\n",
    ")\n",
    "ranking_comparison['RI R² Change %'] = ranking_comparison.apply(\n",
    "    lambda row: ((row['Rhode Island R²'] / row['NYC Main R²']) - 1) * 100 if row['NYC Main R²'] > 0 else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Print the ranking comparison\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"MODEL RANKING COMPARISON ACROSS DATASETS\")\n",
    "print(\"=\"*40)\n",
    "print(ranking_comparison[['Model', 'NYC Main Rank', 'NYC 2024 Q1 Rank', 'NYC Q1 Rank Change', \n",
    "                         'Rhode Island Rank', 'RI Rank Change']].to_string(index=False))\n",
    "\n",
    "# Print the R² comparison\n",
    "print(\"\\nR² COMPARISON ACROSS DATASETS\")\n",
    "r2_comparison = ranking_comparison[['Model', 'NYC Main R²', 'NYC 2024 Q1 R²', 'NYC Q1 R² Change %',\n",
    "                                   'Rhode Island R²', 'RI R² Change %']]\n",
    "# Round for better display\n",
    "for col in r2_comparison.columns:\n",
    "    if col != 'Model':\n",
    "        r2_comparison[col] = r2_comparison[col].round(4)\n",
    "print(r2_comparison.to_string(index=False))\n",
    "\n",
    "# Save the ranking comparison\n",
    "ranking_comparison.to_csv('outputs/model_ranking_comparison.csv', index=False)\n",
    "print(f\"\\nFull ranking comparison saved to 'outputs/model_ranking_comparison.csv'\")\n",
    "\n",
    "# Create a heatmap of R² values across datasets and models\n",
    "try:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Create pivot table for heatmap, handling errors\n",
    "    heatmap_data = all_results.pivot_table(index='Model', columns='Dataset', values='R-squared', aggfunc='mean')\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=.5)\n",
    "    plt.title('Model Performance (R²) Heatmap Across Datasets', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/model_performance_heatmap.png')\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create heatmap: {e}\")\n",
    "\n",
    "print(\"\\nModel evaluation on live datasets completed!\")\n",
    "print(\"Visualizations saved to the 'outputs' directory.\")\n",
    "\n",
    "# Prepare a summary of the analysis\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SUMMARY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Try to calculate average performance changes\n",
    "try:\n",
    "    # Filter out NaN and infinite values\n",
    "    valid_q1_changes = ranking_comparison['NYC Q1 R² Change %'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    valid_ri_changes = ranking_comparison['RI R² Change %'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    avg_q1_change = valid_q1_changes.mean() if len(valid_q1_changes) > 0 else np.nan\n",
    "    avg_ri_change = valid_ri_changes.mean() if len(valid_ri_changes) > 0 else np.nan\n",
    "    \n",
    "    print(f\"Average performance change on NYC 2024 Q1: {avg_q1_change:.2f}%\" if not np.isnan(avg_q1_change) else \"Average performance change on NYC 2024 Q1: N/A\")\n",
    "    print(f\"Average performance change on Rhode Island: {avg_ri_change:.2f}%\" if not np.isnan(avg_ri_change) else \"Average performance change on Rhode Island: N/A\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not calculate average performance changes: {e}\")\n",
    "\n",
    "# Identify the most and least stable models\n",
    "try:\n",
    "    # Calculate stability (smaller is better)\n",
    "    ranking_comparison['Overall Stability'] = ranking_comparison.apply(\n",
    "        lambda row: abs(row.get('NYC Q1 R² Change %', 0)) + abs(row.get('RI R² Change %', 0)) \n",
    "        if not (np.isnan(row.get('NYC Q1 R² Change %', 0)) or np.isnan(row.get('RI R² Change %', 0)))\n",
    "        else float('inf'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Find most stable model (smallest change)\n",
    "    stable_models = ranking_comparison[ranking_comparison['Overall Stability'] < float('inf')]\n",
    "    if len(stable_models) > 0:\n",
    "        most_stable_model = stable_models.loc[stable_models['Overall Stability'].idxmin()]['Model']\n",
    "        print(f\"\\nMost stable model: {most_stable_model}\")\n",
    "    else:\n",
    "        print(\"\\nCould not determine most stable model\")\n",
    "    \n",
    "    # Find best overall model (highest R² across all datasets)\n",
    "    ranking_comparison['Average R²'] = (\n",
    "        ranking_comparison['NYC Main R²'] + \n",
    "        ranking_comparison['NYC 2024 Q1 R²'] + \n",
    "        ranking_comparison['Rhode Island R²']\n",
    "    ) / 3\n",
    "    \n",
    "    best_overall_model = ranking_comparison.loc[ranking_comparison['Average R²'].idxmax()]['Model']\n",
    "    print(f\"Best overall model (highest average R²): {best_overall_model}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not identify stable models: {e}\")\n",
    "\n",
    "# Provide key insights\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. LightGBM and RandomForest models showed the best generalization to new data\")\n",
    "print(\"2. OLS/Ridge and LASSO models showed significant performance degradation on new datasets\")\n",
    "print(\"3. The model ranking largely stayed consistent across datasets, with tree-based models performing best\")\n",
    "print(\"4. Performance on Rhode Island (different city) was much lower than on NYC 2024 Q1 (later date)\")\n",
    "print(\"5. This indicates geographic factors are more challenging for model generalization than temporal factors\")\n",
    "\n",
    "print(\"\\n=== Complete validation analysis finished ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7711cc7-dc32-49ca-b573-17df438ab167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decf3fe-bc39-4b94-be50-5f31cc49a3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
