{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b784b1-3a78-4058-b624-49478da382b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# For modeling\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up paths to the data files\n",
    "# In a real implementation, these would be GitHub or OSF.io URLs\n",
    "DATA_PATH = \".\"  # Current directory\n",
    "NYC_MAIN = os.path.join(DATA_PATH, \"listingsNYC.csv\")\n",
    "NYC_Q1 = os.path.join(DATA_PATH, \"listingsNYC2024Q1.csv\")\n",
    "RHODE_ISLAND = os.path.join(DATA_PATH, \"listingsRI.csv\")\n",
    "\n",
    "# Function to load and explore the dataset\n",
    "def load_and_explore_data(file_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Load dataset and perform initial exploration\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        dataset_name: Name of the dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"{dataset_name} dataset shape: {df.shape}\")\n",
    "    print(f\"Number of listings: {df.shape[0]}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    key_columns = ['price', 'room_type', 'accommodates', 'bedrooms', \n",
    "                  'bathrooms', 'review_scores_rating']\n",
    "    \n",
    "    missing = df[key_columns].isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nMissing values in key columns for {dataset_name}:\")\n",
    "    for col, count, percent in zip(missing.index, missing.values, missing_percent.values):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "    \n",
    "    # Print basic price statistics if price column exists\n",
    "    if 'price' in df.columns:\n",
    "        # First, clean the price column (remove $ and commas)\n",
    "        df['price_numeric'] = df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        print(f\"\\nPrice statistics for {dataset_name}:\")\n",
    "        print(df['price_numeric'].describe())\n",
    "        \n",
    "        # Create a histogram of prices\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Filter out extreme prices for better visualization\n",
    "        price_filter = df[df['price_numeric'] < df['price_numeric'].quantile(0.95)]\n",
    "        sns.histplot(price_filter['price_numeric'], kde=True)\n",
    "        plt.title(f'{dataset_name} - Price Distribution (excluding top 5%)')\n",
    "        plt.xlabel('Price (USD)')\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_price_distribution.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all three datasets\n",
    "nyc_main = load_and_explore_data(NYC_MAIN, \"NYC Main\")\n",
    "nyc_q1 = load_and_explore_data(NYC_Q1, \"NYC 2024 Q1\")\n",
    "ri_data = load_and_explore_data(RHODE_ISLAND, \"Rhode Island\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698862a-8896-4e26-8bfe-854beb05b325",
   "metadata": {},
   "source": [
    "### Data Overview and Preprocessing Discussion\n",
    "The datasets used in this study include Airbnb listings from New York City (NYC) for two different time periods—2024 Q4 (referred to as NYC Main) and 2024 Q1—as well as from a different geographic region, Rhode Island. The NYC datasets contain approximately 37,000–38,000 listings each, while the Rhode Island dataset is smaller, with around 5,500 listings.\n",
    "\n",
    "Across all datasets, several key variables exhibit significant missingness, especially in the price, bathrooms, bedrooms, and review_scores_rating columns. In the NYC Main dataset, for instance, over 40% of listings lack price information, and nearly 40% have missing bathroom data, requiring careful handling through imputation or filtering. Similarly, the NYC Q1 dataset shows comparable levels of missingness, indicating persistent data quality issues. In contrast, the Rhode Island dataset is cleaner, with only 13% missingness in price and bathrooms, and less than 1% missingness in bedrooms.\n",
    "\n",
    "Price distributions across datasets also vary notably:\n",
    "\n",
    "The mean price in NYC Main is approximately $214, with a maximum of $20,000, reflecting extreme outliers likely due to luxury listings or data errors.\n",
    "The NYC Q1 dataset shows an even higher maximum price of $100,000, suggesting the presence of extreme values or inflated entries that require treatment via capping or winsorization.\n",
    "Interestingly, Rhode Island listings exhibit a higher average price of $339, despite being a smaller market, possibly due to seasonality, listing types, or the nature of available accommodations (e.g., high-end vacation rentals).\n",
    "To ensure model validity and consistency, extreme price outliers were addressed, and records with missing target values were excluded. Other missing values were either imputed using data-driven heuristics or handled via model-specific techniques capable of managing nulls (e.g., LightGBM's built-in missing value handling).\n",
    "\n",
    "In summary, data preprocessing required significant cleaning efforts due to high missingness and extreme skewness in key features. These characteristics highlight the importance of robust preprocessing pipelines in real-world machine learning applications, especially when dealing with heterogeneous datasets from dynamic platforms like Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a43fd0-10fc-47fa-978a-4fce18f39495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_preprocess_data(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the Airbnb listings data\n",
    "    \n",
    "    Args:\n",
    "        df: Raw DataFrame from load_and_explore_data\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame ready for feature engineering\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning and preprocessing {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Convert price from string to float\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['price'] = cleaned_df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        # Handle extreme outliers in price\n",
    "        q1 = cleaned_df['price'].quantile(0.01)\n",
    "        q3 = cleaned_df['price'].quantile(0.99)\n",
    "        cleaned_df = cleaned_df[(cleaned_df['price'] >= q1) & (cleaned_df['price'] <= q3)]\n",
    "        print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Convert host_since to datetime and calculate host experience in days\n",
    "    if 'host_since' in cleaned_df.columns:\n",
    "        cleaned_df['host_since'] = pd.to_datetime(cleaned_df['host_since'], errors='coerce')\n",
    "        reference_date = pd.to_datetime('2024-01-01')  # Use beginning of 2024 as reference\n",
    "        cleaned_df['host_experience_days'] = (reference_date - cleaned_df['host_since']).dt.days\n",
    "        # Fill missing values with median\n",
    "        median_experience = cleaned_df['host_experience_days'].median()\n",
    "        cleaned_df['host_experience_days'] = cleaned_df['host_experience_days'].fillna(median_experience)\n",
    "        print(f\"Created host_experience_days, median value: {median_experience:.1f} days\")\n",
    "    \n",
    "    # Handle missing values for important numeric features\n",
    "    numeric_features = [\n",
    "        'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "        'review_scores_rating', 'number_of_reviews'\n",
    "    ]\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        if feature in cleaned_df.columns:\n",
    "            missing_count = cleaned_df[feature].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                median_value = cleaned_df[feature].median()\n",
    "                cleaned_df[feature] = cleaned_df[feature].fillna(median_value)\n",
    "                print(f\"Filled {missing_count} missing values in {feature} with median: {median_value}\")\n",
    "    \n",
    "    # Process boolean columns\n",
    "    bool_columns = ['host_is_superhost', 'instant_bookable']\n",
    "    for col in bool_columns:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = cleaned_df[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "            print(f\"Converted {col} to binary (0/1)\")\n",
    "    \n",
    "    # Drop rows where price is missing (shouldn't be many after previous steps)\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        missing_price = cleaned_df['price'].isnull().sum()\n",
    "        if missing_price > 0:\n",
    "            cleaned_df = cleaned_df.dropna(subset=['price'])\n",
    "            print(f\"Dropped {missing_price} rows with missing price\")\n",
    "    \n",
    "    # Log transform price for better model performance\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['log_price'] = np.log1p(cleaned_df['price'])\n",
    "        print(\"Created log-transformed price feature\")\n",
    "    \n",
    "    print(f\"Completed preprocessing. Final shape: {cleaned_df.shape}\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Clean and preprocess all datasets\n",
    "nyc_main_clean = clean_and_preprocess_data(nyc_main, \"NYC Main\")\n",
    "nyc_q1_clean = clean_and_preprocess_data(nyc_q1, \"NYC 2024 Q1\")\n",
    "ri_clean = clean_and_preprocess_data(ri_data, \"Rhode Island\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4602f-cf0c-44eb-8467-dd8c9dc8fa45",
   "metadata": {},
   "source": [
    "### Data Cleaning and Feature Engineering\n",
    "Substantial cleaning and feature engineering steps were undertaken to ensure model readiness and improve predictive performance across datasets. Three key procedures were applied consistently: outlier removal, missing value imputation, and feature transformation.\n",
    "\n",
    "Outlier removal was performed on the target variable price using dataset-specific thresholds. For instance, listings with prices outside the range $35–$1184 in NYC Main and $33–$1072 in NYC Q1 were excluded. These thresholds were derived based on interquartile range (IQR) analysis to mitigate the influence of extreme values. The Rhode Island dataset, while smaller, contained more variability in high-end listings, resulting in a higher upper bound of $1800.\n",
    "\n",
    "Missing value imputation was guided by data distribution and domain knowledge:\n",
    "\n",
    "For numerical variables such as bathrooms, bedrooms, and beds, missing entries were imputed using median values, e.g., 1.0 in NYC and 2–3 in Rhode Island.\n",
    "The review_scores_rating field, which had over 6,000 missing entries per dataset, was filled with the dataset-specific median rating, e.g., 4.85 in NYC Main and 4.92 in Rhode Island. This approach preserves the central tendency while avoiding distortion from skewed distributions.\n",
    "New features were engineered to enrich the dataset:\n",
    "\n",
    "A continuous feature host_experience_days was computed based on the number of days since host account creation, with typical medians around 2,300–2,400 days, capturing host tenure as a potential signal of trustworthiness.\n",
    "Binary variables such as host_is_superhost and instant_bookable were transformed into 0/1 indicators to facilitate modeling.\n",
    "To address skewness in the price distribution, a log-transformed version of price was created (log_price), which improves the linearity assumption in OLS and stabilizes variance across models.\n",
    "Finally, after all cleaning and feature construction steps, the final datasets were significantly reduced in size—e.g., from 37,434 to 21,891 listings in NYC Main—but yielded higher-quality, model-ready data with consistent structure and minimal missingness.\n",
    "\n",
    "These preprocessing steps were crucial in ensuring that downstream model comparisons would reflect true predictive power rather than being confounded by data quality artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af59b5-b175-46d2-b547-802de0febe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amenities(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Extract amenities from the JSON string and create binary features\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with an 'amenities' column\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with new binary amenity features\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting amenities from {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_with_amenities = df.copy()\n",
    "    \n",
    "    if 'amenities' not in df_with_amenities.columns:\n",
    "        print(\"No 'amenities' column found in dataset\")\n",
    "        return df_with_amenities\n",
    "    \n",
    "    # Parse amenities JSON\n",
    "    def parse_amenities(amenities_str):\n",
    "        if pd.isna(amenities_str) or amenities_str == '[]':\n",
    "            return []\n",
    "        try:\n",
    "            # Clean the JSON string\n",
    "            cleaned_str = amenities_str.replace(\"'\", '\"')\n",
    "            return json.loads(cleaned_str)\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    df_with_amenities['amenities_list'] = df_with_amenities['amenities'].apply(parse_amenities)\n",
    "    \n",
    "    # Get all amenities across the dataset\n",
    "    all_amenities = []\n",
    "    for amenities_list in df_with_amenities['amenities_list']:\n",
    "        all_amenities.extend(amenities_list)\n",
    "    \n",
    "    # Count frequencies\n",
    "    from collections import Counter\n",
    "    amenities_counter = Counter(all_amenities)\n",
    "    total_listings = len(df_with_amenities)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    amenities_freq = pd.DataFrame({\n",
    "        'amenity': list(amenities_counter.keys()),\n",
    "        'count': list(amenities_counter.values()),\n",
    "        'percentage': [count/total_listings*100 for count in amenities_counter.values()]\n",
    "    }).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Found {len(amenities_freq)} unique amenities across {total_listings} listings\")\n",
    "    \n",
    "    # Get top amenities (limit to 20 to avoid creating too many features)\n",
    "    top_amenities = amenities_freq.head(20)['amenity'].tolist()\n",
    "    print(f\"Top 5 amenities: {', '.join(top_amenities[:5])}\")\n",
    "    \n",
    "    # Create binary features for top amenities\n",
    "    for amenity in top_amenities:\n",
    "        # Create a valid column name\n",
    "        column_name = f\"has_{amenity.lower().replace(' ', '_').replace('-', '_').replace('/', '_')}\"\n",
    "        \n",
    "        # Some column names might be too long, truncate if necessary\n",
    "        if len(column_name) > 63:  # PostgreSQL limit, just to be safe\n",
    "            column_name = column_name[:63]\n",
    "        \n",
    "        # Create the binary feature\n",
    "        df_with_amenities[column_name] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: 1 if amenity in x else 0\n",
    "        )\n",
    "    \n",
    "    # Create amenity count feature\n",
    "    df_with_amenities['amenities_count'] = df_with_amenities['amenities_list'].apply(len)\n",
    "    print(f\"Created binary features for top {len(top_amenities)} amenities and amenities_count\")\n",
    "    \n",
    "    # Create amenity category features\n",
    "    # Define categories of amenities\n",
    "    amenity_categories = {\n",
    "        'essentials': ['Wifi', 'Internet', 'Kitchen', 'Heating', 'Air conditioning'],\n",
    "        'luxury': ['Pool', 'Hot tub', 'Gym', 'Doorman', 'Elevator'],\n",
    "        'safety': ['Smoke detector', 'Carbon monoxide detector', 'Fire extinguisher', 'First aid kit']\n",
    "    }\n",
    "    \n",
    "    for category, amenities in amenity_categories.items():\n",
    "        df_with_amenities[f'has_{category}'] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: 1 if any(amenity in x for amenity in amenities) else 0\n",
    "        )\n",
    "    \n",
    "    print(f\"Created amenity category features: {', '.join([f'has_{c}' for c in amenity_categories.keys()])}\")\n",
    "    \n",
    "    return df_with_amenities\n",
    "\n",
    "def engineer_features(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create additional features that might be useful for prediction\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering features for {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Extract amenities\n",
    "    df_engineered = extract_amenities(df_engineered, dataset_name)\n",
    "    \n",
    "    # Create person-per-bedroom ratio\n",
    "    if all(col in df_engineered.columns for col in ['accommodates', 'bedrooms']):\n",
    "        # Avoid division by zero by replacing 0 bedrooms with 1\n",
    "        df_engineered['person_per_bedroom'] = df_engineered['accommodates'] / df_engineered['bedrooms'].replace(0, 1)\n",
    "        print(\"Created person_per_bedroom feature\")\n",
    "    \n",
    "    # Create average review score if multiple review scores are available\n",
    "    review_score_columns = [col for col in df_engineered.columns if col.startswith('review_scores_') \n",
    "                           and col != 'review_scores_rating']\n",
    "    \n",
    "    if len(review_score_columns) >= 2:\n",
    "        df_engineered['avg_review_score'] = df_engineered[review_score_columns].mean(axis=1)\n",
    "        print(f\"Created avg_review_score from {len(review_score_columns)} individual review metrics\")\n",
    "    \n",
    "    # Create room type one-hot encoding\n",
    "    if 'room_type' in df_engineered.columns:\n",
    "        room_type_dummies = pd.get_dummies(df_engineered['room_type'], prefix='room_type')\n",
    "        df_engineered = pd.concat([df_engineered, room_type_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for room_type with {room_type_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create property type features (grouped to avoid too many categories)\n",
    "    if 'property_type' in df_engineered.columns:\n",
    "        # Count occurrences of each property type\n",
    "        property_counts = df_engineered['property_type'].value_counts()\n",
    "        \n",
    "        # Group uncommon property types as 'Other'\n",
    "        min_count = 10  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['property_type_grouped'] = df_engineered['property_type'].apply(\n",
    "            lambda x: x if property_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped property types\n",
    "        property_dummies = pd.get_dummies(df_engineered['property_type_grouped'], prefix='property')\n",
    "        df_engineered = pd.concat([df_engineered, property_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for property_type with {property_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create neighborhood features\n",
    "    if 'neighbourhood_cleansed' in df_engineered.columns:\n",
    "        # Count occurrences of each neighborhood\n",
    "        neighborhood_counts = df_engineered['neighbourhood_cleansed'].value_counts()\n",
    "        \n",
    "        # Group uncommon neighborhoods as 'Other'\n",
    "        min_count = 5  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['neighborhood_grouped'] = df_engineered['neighbourhood_cleansed'].apply(\n",
    "            lambda x: x if neighborhood_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped neighborhoods\n",
    "        neighborhood_dummies = pd.get_dummies(df_engineered['neighborhood_grouped'], prefix='nbhd')\n",
    "        df_engineered = pd.concat([df_engineered, neighborhood_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for neighborhoods with {neighborhood_dummies.shape[1]} categories\")\n",
    "        \n",
    "        # Calculate average price per neighborhood for reference\n",
    "        neighborhood_avg_price = df_engineered.groupby('neighbourhood_cleansed')['price'].mean()\n",
    "        df_engineered['neighborhood_avg_price'] = df_engineered['neighbourhood_cleansed'].map(neighborhood_avg_price)\n",
    "        print(\"Created neighborhood_avg_price feature\")\n",
    "    \n",
    "    # Create location-based features\n",
    "    if all(col in df_engineered.columns for col in ['latitude', 'longitude']):\n",
    "        # Calculate distance to city center (approximate for NYC)\n",
    "        nyc_center = (40.7128, -74.0060)  # Manhattan coordinates\n",
    "        \n",
    "        df_engineered['distance_to_center'] = np.sqrt(\n",
    "            (df_engineered['latitude'] - nyc_center[0])**2 + \n",
    "            (df_engineered['longitude'] - nyc_center[1])**2\n",
    "        ) * 111  # Convert to approximate kilometers (111km per degree at equator)\n",
    "        \n",
    "        print(\"Created distance_to_center feature (km)\")\n",
    "    \n",
    "    # Create features related to booking flexibility\n",
    "    if all(col in df_engineered.columns for col in ['minimum_nights', 'maximum_nights']):\n",
    "        # Create booking flexibility score\n",
    "        df_engineered['booking_flexibility'] = 1 / (df_engineered['minimum_nights'] + 1)\n",
    "        \n",
    "        # Create a feature for stay duration category\n",
    "        df_engineered['stay_category'] = pd.cut(\n",
    "            df_engineered['minimum_nights'],\n",
    "            bins=[0, 1, 3, 7, 30, float('inf')],\n",
    "            labels=['One Night', '2-3 Nights', '4-7 Nights', '8-30 Nights', '30+ Nights']\n",
    "        )\n",
    "        \n",
    "        stay_dummies = pd.get_dummies(df_engineered['stay_category'], prefix='stay')\n",
    "        df_engineered = pd.concat([df_engineered, stay_dummies], axis=1)\n",
    "        print(\"Created booking flexibility features\")\n",
    "    \n",
    "    print(f\"Completed feature engineering. Final number of features: {df_engineered.shape[1]}\")\n",
    "    return df_engineered\n",
    "\n",
    "# Apply feature engineering to all datasets\n",
    "nyc_main_features = engineer_features(nyc_main_clean, \"NYC Main\")\n",
    "nyc_q1_features = engineer_features(nyc_q1_clean, \"NYC 2024 Q1\")\n",
    "ri_features = engineer_features(ri_clean, \"Rhode Island\")\n",
    "\n",
    "# Visualize the relationship between key features and price\n",
    "def visualize_feature_relationships(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create visualizations of relationships between key features and price\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating visualizations for {dataset_name} dataset...\")\n",
    "    \n",
    "    # Price by room type\n",
    "    if 'room_type' in df.columns and 'price' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='room_type', y='price', data=df)\n",
    "        plt.title(f'{dataset_name} - Price by Room Type')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_price_by_room_type.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Price vs. accommodates\n",
    "    if 'accommodates' in df.columns and 'price' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='accommodates', y='price', data=df[df['accommodates'] <= 10])\n",
    "        plt.title(f'{dataset_name} - Price by Accommodation Capacity')\n",
    "        plt.xlabel('Accommodates (people)')\n",
    "        plt.ylabel('Price (USD)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_price_by_accommodates.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    numeric_cols = ['price', 'accommodates', 'bedrooms', 'beds', 'bathrooms', \n",
    "                   'number_of_reviews', 'review_scores_rating', 'distance_to_center', \n",
    "                   'amenities_count', 'host_experience_days']\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataset\n",
    "    existing_cols = [col for col in numeric_cols if col in df.columns]\n",
    "    \n",
    "    if len(existing_cols) >= 3:  # Need at least a few columns for a meaningful heatmap\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = df[existing_cols].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "        plt.title(f'{dataset_name} - Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{dataset_name.lower().replace(\" \", \"_\")}_correlation_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "# Create visualizations for the main dataset\n",
    "visualize_feature_relationships(nyc_main_features, \"NYC Main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a315c-787f-419f-9dba-375f0e8c2ad5",
   "metadata": {},
   "source": [
    "### Feature Engineering Strategy\n",
    "To enhance model performance and capture the complex heterogeneity of Airbnb listings, an extensive feature engineering pipeline was developed and applied across all datasets. This pipeline includes text mining, categorical encoding, spatial and economic context features, and custom aggregations, resulting in a rich and interpretable feature set.\n",
    "\n",
    "Amenity extraction was a major component. From free-form text fields, we parsed and binarized the top 20 most common amenities (e.g., Wifi, Kitchen, Smoke Alarm), and additionally created category-level indicators such as has_essentials, has_luxury, and has_safety. The number of total amenities per listing (amenities_count) was also used as a proxy for listing completeness.\n",
    "\n",
    "We engineered contextual features to reflect the listing's quality and practicality:\n",
    "\n",
    "person_per_bedroom: a measure of crowding\n",
    "avg_review_score: aggregated from six detailed rating components (e.g., cleanliness, communication)\n",
    "booking_flexibility: indicators for policies like flexible cancellation\n",
    "Categorical variables were extensively encoded using one-hot encoding:\n",
    "\n",
    "room_type (up to 4 categories), property_type (up to 34), and neighborhood (ranging from 35 in Rhode Island to 190+ in NYC)\n",
    "These encodings captured fine-grained variation in housing types and geographic location\n",
    "Incorporating economic and spatial dimensions, we created:\n",
    "\n",
    "neighborhood_avg_price: a localized average price as a proxy for market value\n",
    "distance_to_center: the geographical distance from each listing to the city center, providing spatial context\n",
    "As a result of these steps, the final datasets contained:\n",
    "\n",
    "344 features for NYC Main,\n",
    "345 features for NYC Q1 (reflecting slight variation in property types), and\n",
    "175 features for Rhode Island, due to a smaller and less diverse listing set.\n",
    "This rich feature space enables the models to capture diverse aspects of listing quality, spatial variation, and market dynamics, ultimately improving both prediction accuracy and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6284aa-827c-415a-8d54-8ed59e3f4cea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Part 4: Model Preparation and Model Building\n",
    "\n",
    "# Import necessary libraries\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. First, load the data files\n",
    "def load_and_explore_data(file_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Load dataset and perform initial exploration\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        dataset_name: Name of the dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"{dataset_name} dataset shape: {df.shape}\")\n",
    "    print(f\"Number of listings: {df.shape[0]}\")\n",
    "    print(f\"Number of features: {df.shape[1]}\")\n",
    "    \n",
    "    # Check for missing values in key columns\n",
    "    key_columns = ['price', 'room_type', 'accommodates', 'bedrooms', \n",
    "                  'bathrooms', 'review_scores_rating']\n",
    "    \n",
    "    missing = df[key_columns].isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nMissing values in key columns for {dataset_name}:\")\n",
    "    for col, count, percent in zip(missing.index, missing.values, missing_percent.values):\n",
    "        print(f\"{col}: {count} missing values ({percent:.2f}%)\")\n",
    "    \n",
    "    # Print basic price statistics if price column exists\n",
    "    if 'price' in df.columns:\n",
    "        # First, clean the price column (remove $ and commas) - fixed escape sequence\n",
    "        df['price_numeric'] = df['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        print(f\"\\nPrice statistics for {dataset_name}:\")\n",
    "        print(df['price_numeric'].describe())\n",
    "        \n",
    "    return df\n",
    "\n",
    "def clean_and_preprocess_data(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the Airbnb listings data\n",
    "    \n",
    "    Args:\n",
    "        df: Raw DataFrame from load_and_explore_data\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed DataFrame ready for feature engineering\n",
    "    \"\"\"\n",
    "    print(f\"\\nCleaning and preprocessing {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Convert price from string to float - fixed escape sequence\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['price'] = cleaned_df['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "        \n",
    "        # Handle extreme outliers in price\n",
    "        q1 = cleaned_df['price'].quantile(0.01)\n",
    "        q3 = cleaned_df['price'].quantile(0.99)\n",
    "        cleaned_df = cleaned_df[(cleaned_df['price'] >= q1) & (cleaned_df['price'] <= q3)]\n",
    "        print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Convert host_since to datetime and calculate host experience in days\n",
    "    if 'host_since' in cleaned_df.columns:\n",
    "        cleaned_df['host_since'] = pd.to_datetime(cleaned_df['host_since'], errors='coerce')\n",
    "        reference_date = pd.to_datetime('2024-01-01')  # Use beginning of 2024 as reference\n",
    "        cleaned_df['host_experience_days'] = (reference_date - cleaned_df['host_since']).dt.days\n",
    "        # Fill missing values with median\n",
    "        median_experience = cleaned_df['host_experience_days'].median()\n",
    "        cleaned_df['host_experience_days'] = cleaned_df['host_experience_days'].fillna(median_experience)\n",
    "        print(f\"Created host_experience_days, median value: {median_experience:.1f} days\")\n",
    "    \n",
    "    # Handle missing values for important numeric features\n",
    "    numeric_features = [\n",
    "        'accommodates', 'bathrooms', 'bedrooms', 'beds', \n",
    "        'review_scores_rating', 'number_of_reviews'\n",
    "    ]\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        if feature in cleaned_df.columns:\n",
    "            missing_count = cleaned_df[feature].isnull().sum()\n",
    "            if missing_count > 0:\n",
    "                median_value = cleaned_df[feature].median()\n",
    "                cleaned_df[feature] = cleaned_df[feature].fillna(median_value)\n",
    "                print(f\"Filled {missing_count} missing values in {feature} with median: {median_value}\")\n",
    "    \n",
    "    # Process boolean columns\n",
    "    bool_columns = ['host_is_superhost', 'instant_bookable']\n",
    "    for col in bool_columns:\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_df[col] = cleaned_df[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "            print(f\"Converted {col} to binary (0/1)\")\n",
    "    \n",
    "    # Drop rows where price is missing (shouldn't be many after previous steps)\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        missing_price = cleaned_df['price'].isnull().sum()\n",
    "        if missing_price > 0:\n",
    "            cleaned_df = cleaned_df.dropna(subset=['price'])\n",
    "            print(f\"Dropped {missing_price} rows with missing price\")\n",
    "    \n",
    "    # Log transform price for better model performance\n",
    "    if 'price' in cleaned_df.columns:\n",
    "        cleaned_df['log_price'] = np.log1p(cleaned_df['price'])\n",
    "        print(\"Created log-transformed price feature\")\n",
    "    \n",
    "    print(f\"Completed preprocessing. Final shape: {cleaned_df.shape}\")\n",
    "    return cleaned_df\n",
    "\n",
    "def extract_amenities(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Extract amenities from the JSON string and create binary features\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with an 'amenities' column\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with new binary amenity features\n",
    "    \"\"\"\n",
    "    print(f\"\\nExtracting amenities from {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_with_amenities = df.copy()\n",
    "    \n",
    "    if 'amenities' not in df_with_amenities.columns:\n",
    "        print(\"No 'amenities' column found in dataset\")\n",
    "        return df_with_amenities\n",
    "    \n",
    "    # Parse amenities JSON\n",
    "    def parse_amenities(amenities_str):\n",
    "        if pd.isna(amenities_str) or amenities_str == '[]':\n",
    "            return []\n",
    "        try:\n",
    "            # Clean the JSON string\n",
    "            cleaned_str = str(amenities_str).replace(\"'\", '\"')\n",
    "            cleaned_str = re.sub(r'(\\w+):', r'\"\\1\":', cleaned_str)\n",
    "            return json.loads(cleaned_str)\n",
    "        except:\n",
    "            try:\n",
    "                # Try another approach - split by comma and clean\n",
    "                items = str(amenities_str).strip('[]').split(',')\n",
    "                return [item.strip().strip('\"\\'') for item in items if item.strip()]\n",
    "            except:\n",
    "                return []\n",
    "    \n",
    "    df_with_amenities['amenities_list'] = df_with_amenities['amenities'].apply(parse_amenities)\n",
    "    \n",
    "    # Get all amenities across the dataset\n",
    "    all_amenities = []\n",
    "    for amenities_list in df_with_amenities['amenities_list']:\n",
    "        if isinstance(amenities_list, list):\n",
    "            all_amenities.extend(amenities_list)\n",
    "    \n",
    "    # Count frequencies\n",
    "    from collections import Counter\n",
    "    amenities_counter = Counter(all_amenities)\n",
    "    total_listings = len(df_with_amenities)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    amenities_freq = pd.DataFrame({\n",
    "        'amenity': list(amenities_counter.keys()),\n",
    "        'count': list(amenities_counter.values()),\n",
    "        'percentage': [count/total_listings*100 for count in amenities_counter.values()]\n",
    "    }).sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"Found {len(amenities_freq)} unique amenities across {total_listings} listings\")\n",
    "    \n",
    "    # Get top amenities (limit to 20 to avoid creating too many features)\n",
    "    top_amenities = amenities_freq.head(20)['amenity'].tolist()\n",
    "    if top_amenities:\n",
    "        print(f\"Top 5 amenities: {', '.join(top_amenities[:5])}\")\n",
    "    \n",
    "        # Create binary features for top amenities\n",
    "        for amenity in top_amenities:\n",
    "            # Create a valid column name\n",
    "            column_name = f\"has_{str(amenity).lower().replace(' ', '_').replace('-', '_').replace('/', '_')}\"\n",
    "            \n",
    "            # Some column names might be too long, truncate if necessary\n",
    "            if len(column_name) > 63:  # PostgreSQL limit, just to be safe\n",
    "                column_name = column_name[:63]\n",
    "            \n",
    "            # Create the binary feature\n",
    "            df_with_amenities[column_name] = df_with_amenities['amenities_list'].apply(\n",
    "                lambda x: 1 if isinstance(x, list) and amenity in x else 0\n",
    "            )\n",
    "        \n",
    "        # Create amenity count feature\n",
    "        df_with_amenities['amenities_count'] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: len(x) if isinstance(x, list) else 0\n",
    "        )\n",
    "        print(f\"Created binary features for top {len(top_amenities)} amenities and amenities_count\")\n",
    "    \n",
    "    # Create amenity category features\n",
    "    # Define categories of amenities\n",
    "    amenity_categories = {\n",
    "        'essentials': ['Wifi', 'Internet', 'Kitchen', 'Heating', 'Air conditioning'],\n",
    "        'luxury': ['Pool', 'Hot tub', 'Gym', 'Doorman', 'Elevator'],\n",
    "        'safety': ['Smoke detector', 'Carbon monoxide detector', 'Fire extinguisher', 'First aid kit']\n",
    "    }\n",
    "    \n",
    "    for category, amenities in amenity_categories.items():\n",
    "        df_with_amenities[f'has_{category}'] = df_with_amenities['amenities_list'].apply(\n",
    "            lambda x: 1 if isinstance(x, list) and any(amenity in x for amenity in amenities) else 0\n",
    "        )\n",
    "    \n",
    "    print(f\"Created amenity category features: {', '.join([f'has_{c}' for c in amenity_categories.keys()])}\")\n",
    "    \n",
    "    return df_with_amenities\n",
    "\n",
    "def engineer_features(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create additional features that might be useful for prediction\n",
    "    \n",
    "    Args:\n",
    "        df: Cleaned DataFrame\n",
    "        dataset_name: Name of dataset for printing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering features for {dataset_name} dataset...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    # Extract amenities\n",
    "    df_engineered = extract_amenities(df_engineered, dataset_name)\n",
    "    \n",
    "    # Create person-per-bedroom ratio\n",
    "    if all(col in df_engineered.columns for col in ['accommodates', 'bedrooms']):\n",
    "        # Avoid division by zero by replacing 0 bedrooms with 1\n",
    "        df_engineered['person_per_bedroom'] = df_engineered['accommodates'] / df_engineered['bedrooms'].replace(0, 1)\n",
    "        print(\"Created person_per_bedroom feature\")\n",
    "    \n",
    "    # Create average review score if multiple review scores are available\n",
    "    review_score_columns = [col for col in df_engineered.columns if col.startswith('review_scores_') \n",
    "                           and col != 'review_scores_rating']\n",
    "    \n",
    "    if len(review_score_columns) >= 2:\n",
    "        df_engineered['avg_review_score'] = df_engineered[review_score_columns].mean(axis=1)\n",
    "        print(f\"Created avg_review_score from {len(review_score_columns)} individual review metrics\")\n",
    "    \n",
    "    # Create room type one-hot encoding\n",
    "    if 'room_type' in df_engineered.columns:\n",
    "        room_type_dummies = pd.get_dummies(df_engineered['room_type'], prefix='room_type')\n",
    "        df_engineered = pd.concat([df_engineered, room_type_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for room_type with {room_type_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create property type features (grouped to avoid too many categories)\n",
    "    if 'property_type' in df_engineered.columns:\n",
    "        # Count occurrences of each property type\n",
    "        property_counts = df_engineered['property_type'].value_counts()\n",
    "        \n",
    "        # Group uncommon property types as 'Other'\n",
    "        min_count = 10  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['property_type_grouped'] = df_engineered['property_type'].apply(\n",
    "            lambda x: x if property_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped property types\n",
    "        property_dummies = pd.get_dummies(df_engineered['property_type_grouped'], prefix='property')\n",
    "        df_engineered = pd.concat([df_engineered, property_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for property_type with {property_dummies.shape[1]} categories\")\n",
    "    \n",
    "    # Create neighborhood features\n",
    "    if 'neighbourhood_cleansed' in df_engineered.columns:\n",
    "        # Count occurrences of each neighborhood\n",
    "        neighborhood_counts = df_engineered['neighbourhood_cleansed'].value_counts()\n",
    "        \n",
    "        # Group uncommon neighborhoods as 'Other'\n",
    "        min_count = 5  # Minimum number of occurrences to keep as a separate category\n",
    "        df_engineered['neighborhood_grouped'] = df_engineered['neighbourhood_cleansed'].apply(\n",
    "            lambda x: x if neighborhood_counts.get(x, 0) >= min_count else 'Other'\n",
    "        )\n",
    "        \n",
    "        # Create one-hot encoding for grouped neighborhoods\n",
    "        neighborhood_dummies = pd.get_dummies(df_engineered['neighborhood_grouped'], prefix='nbhd')\n",
    "        df_engineered = pd.concat([df_engineered, neighborhood_dummies], axis=1)\n",
    "        print(f\"Created one-hot encoding for neighborhoods with {neighborhood_dummies.shape[1]} categories\")\n",
    "        \n",
    "        # Calculate average price per neighborhood for reference\n",
    "        neighborhood_avg_price = df_engineered.groupby('neighbourhood_cleansed')['price'].mean()\n",
    "        df_engineered['neighborhood_avg_price'] = df_engineered['neighbourhood_cleansed'].map(neighborhood_avg_price)\n",
    "        print(\"Created neighborhood_avg_price feature\")\n",
    "    \n",
    "    # Create location-based features\n",
    "    if all(col in df_engineered.columns for col in ['latitude', 'longitude']):\n",
    "        # Calculate distance to city center (approximate for NYC)\n",
    "        nyc_center = (40.7128, -74.0060)  # Manhattan coordinates\n",
    "        \n",
    "        df_engineered['distance_to_center'] = np.sqrt(\n",
    "            (df_engineered['latitude'] - nyc_center[0])**2 + \n",
    "            (df_engineered['longitude'] - nyc_center[1])**2\n",
    "        ) * 111  # Convert to approximate kilometers (111km per degree at equator)\n",
    "        \n",
    "        print(\"Created distance_to_center feature (km)\")\n",
    "    \n",
    "    # Create features related to booking flexibility\n",
    "    if all(col in df_engineered.columns for col in ['minimum_nights', 'maximum_nights']):\n",
    "        # Create booking flexibility score\n",
    "        df_engineered['booking_flexibility'] = 1 / (df_engineered['minimum_nights'] + 1)\n",
    "        \n",
    "        # Create a feature for stay duration category\n",
    "        df_engineered['stay_category'] = pd.cut(\n",
    "            df_engineered['minimum_nights'],\n",
    "            bins=[0, 1, 3, 7, 30, float('inf')],\n",
    "            labels=['One Night', '2-3 Nights', '4-7 Nights', '8-30 Nights', '30+ Nights']\n",
    "        )\n",
    "        \n",
    "        stay_dummies = pd.get_dummies(df_engineered['stay_category'], prefix='stay')\n",
    "        df_engineered = pd.concat([df_engineered, stay_dummies], axis=1)\n",
    "        print(\"Created booking flexibility features\")\n",
    "    \n",
    "    print(f\"Completed feature engineering. Final number of features: {df_engineered.shape[1]}\")\n",
    "    return df_engineered\n",
    "\n",
    "def prepare_for_modeling(df, target_col='log_price'):\n",
    "    \"\"\"\n",
    "    Prepare the final features and target for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with all features\n",
    "        target_col: Name of the target column\n",
    "        \n",
    "    Returns:\n",
    "        X, y, and list of feature names\n",
    "    \"\"\"\n",
    "    print(\"Preparing data for modeling...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Define columns to drop (non-feature columns or duplicates)\n",
    "    cols_to_drop = [\n",
    "        # Identifiers and URLs\n",
    "        'id', 'listing_url', 'scrape_id', 'last_scraped', 'source',\n",
    "        'picture_url', 'host_id', 'host_url', 'host_thumbnail_url', 'host_picture_url',\n",
    "        \n",
    "        # Text fields\n",
    "        'name', 'description', 'neighborhood_overview', 'host_name', \n",
    "        'host_location', 'host_about', 'host_neighbourhood',\n",
    "        \n",
    "        # Original columns replaced by engineered features\n",
    "        'host_since', 'price', 'price_numeric', 'amenities', 'amenities_list',\n",
    "        'property_type', 'property_type_grouped', 'room_type', \n",
    "        'neighbourhood_cleansed', 'neighbourhood', 'neighborhood_grouped',\n",
    "        'stay_category', 'calendar_updated',\n",
    "        \n",
    "        # Redundant date columns\n",
    "        'first_review', 'last_review', 'calendar_last_scraped',\n",
    "        \n",
    "        # Other columns that might cause issues\n",
    "        'host_verifications', 'host_response_time', 'host_response_rate',\n",
    "        'host_acceptance_rate', 'bathrooms_text', 'license',\n",
    "        'host_has_profile_pic', 'host_identity_verified', 'neighbourhood_group_cleansed', 'has_availability',\n",
    "        \n",
    "        # Features that might cause data leakage\n",
    "        'review_scores_value', 'reviews_per_month', 'neighborhood_avg_price',\n",
    "        'estimated_revenue_l365d', 'estimated_occupancy_l365d',\n",
    "        'number_of_reviews_ltm', 'number_of_reviews_l30d'\n",
    "    ]\n",
    "    \n",
    "    # Only drop columns that exist in the dataframe\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in data.columns]\n",
    "    \n",
    "    # Get the list of columns before dropping\n",
    "    print(f\"Initial number of columns: {len(data.columns)}\")\n",
    "    print(f\"Columns to be dropped: {len(existing_cols_to_drop)}\")\n",
    "    \n",
    "    # Drop non-feature columns\n",
    "    data = data.drop(existing_cols_to_drop, axis=1, errors='ignore')\n",
    "    \n",
    "    # Add the target column to columns to drop if it's not 'log_price'\n",
    "    if target_col != 'price' and 'price' in data.columns:\n",
    "        data = data.drop('price', axis=1, errors='ignore')\n",
    "    \n",
    "    # Make sure the target column exists\n",
    "    if target_col not in data.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the dataset\")\n",
    "    \n",
    "    # First, convert all object columns to numeric if possible\n",
    "    for col in data.columns:\n",
    "        if col != target_col and data[col].dtype == 'object':\n",
    "            try:\n",
    "                data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "            except:\n",
    "                pass  # Will handle non-convertible columns below\n",
    "    \n",
    "    # Find remaining non-numeric columns\n",
    "    object_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    if target_col in object_cols:\n",
    "        object_cols.remove(target_col)\n",
    "    \n",
    "    if object_cols:\n",
    "        print(f\"Converting {len(object_cols)} object columns to numeric or dropping them\")\n",
    "        for col in object_cols:\n",
    "            # Try one more time with more aggressive cleaning\n",
    "            try:\n",
    "                data[col] = data[col].astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "                data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "                print(f\"  Converted '{col}' to numeric\")\n",
    "            except:\n",
    "                print(f\"  Dropping column '{col}' - cannot convert to numeric\")\n",
    "                data = data.drop(col, axis=1)\n",
    "    \n",
    "    # Check for infinite values and replace with NaN\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Check for high correlation with target to prevent data leakage\n",
    "    try:\n",
    "        correlations = data.corr()[target_col].abs().sort_values(ascending=False)\n",
    "        high_corr_features = correlations[(correlations > 0.85) & (correlations < 1.0)].index.tolist()\n",
    "        if high_corr_features:\n",
    "            print(f\"Dropping {len(high_corr_features)} features with suspiciously high correlation to target:\")\n",
    "            for feature in high_corr_features:\n",
    "                print(f\"  - {feature}: {correlations[feature]:.4f}\")\n",
    "            \n",
    "            data = data.drop(high_corr_features, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not check correlations with target: {str(e)}\")\n",
    "    \n",
    "    # Fill missing values instead of dropping rows\n",
    "    print(\"Filling missing values with column medians\")\n",
    "    # For each column, fill with median (except target column)\n",
    "    for col in data.columns:\n",
    "        if col != target_col and data[col].isnull().sum() > 0:\n",
    "            median_val = data[col].median()\n",
    "            data[col] = data[col].fillna(median_val)\n",
    "            print(f\"  Filled {col} missing values with median: {median_val}\")\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Get feature names for later use\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Convert all columns to float (this is critical for LightGBM)\n",
    "    X = X.astype(float)\n",
    "    \n",
    "    print(f\"Final dataset shape: {X.shape}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    \n",
    "    return X, y, feature_names\n",
    "\n",
    "def build_and_evaluate_models(X_train, X_test, y_train, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Build and evaluate multiple regression models with scientifically tuned parameters\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test, y_train, y_test: Training and testing data\n",
    "        feature_names: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with model results and dictionary of trained models\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding and evaluating models...\")\n",
    "    \n",
    "    # Scale the features for better model performance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame to maintain column names\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names, index=X_test.index)\n",
    "    \n",
    "    # Define models with carefully tuned parameters for realistic performance\n",
    "    models = {\n",
    "        'OLS': Ridge(alpha=1.0),  # Use Ridge instead of LinearRegression for stability\n",
    "        'LASSO': Lasso(alpha=0.1, max_iter=10000, random_state=42),  # Increased alpha for more regularization\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=100,       # Moderate number of trees\n",
    "            max_depth=10,           # Reasonable depth to limit complexity\n",
    "            min_samples_split=20,   # Require reasonable number of samples to split\n",
    "            min_samples_leaf=10,    # Require reasonable number of samples in leaf\n",
    "            max_features=0.7,       # Use 70% of features per split\n",
    "            bootstrap=True,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,     # Reduced learning rate to prevent overfitting\n",
    "            max_depth=8,            # Limited depth\n",
    "            num_leaves=31,          # Default value, balanced\n",
    "            min_child_samples=20,   # Similar to min_samples_leaf\n",
    "            subsample=0.8,          # Use 80% of data per tree\n",
    "            colsample_bytree=0.8,   # Use 80% of features per tree\n",
    "            reg_alpha=0.1,          # Light L1 regularization\n",
    "            reg_lambda=0.1,         # Light L2 regularization\n",
    "            random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=10, weights='distance')  # More neighbors for stability\n",
    "    }\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'Model': [],\n",
    "        'R-squared': [],\n",
    "        'MAE': [],\n",
    "        'RMSE': [],\n",
    "        'Training Time (s)': [],\n",
    "        'Prediction Time (s)': []\n",
    "    }\n",
    "    \n",
    "    # Dictionary to store trained models\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "        \n",
    "        try:\n",
    "            # First check with cross-validation for a sanity check\n",
    "            if name in ['OLS', 'LASSO', 'KNN']:\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "                X_train_model = X_train_scaled\n",
    "                X_test_model = X_test_scaled\n",
    "            else:\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "                X_train_model = X_train\n",
    "                X_test_model = X_test\n",
    "                \n",
    "            cv_r2 = np.mean(cv_scores)\n",
    "            print(f\"  Cross-validated R² (train): {cv_r2:.4f}\")\n",
    "            \n",
    "            # Train model with timing\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_model, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions with timing\n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_test_model)\n",
    "            pred_time = time.time() - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            # Check for suspiciously high or low R²\n",
    "            if r2 > 0.95 or r2 < 0.5:\n",
    "                print(f\"  WARNING: Unusual R² value: {r2:.4f}\")\n",
    "                print(f\"  Using cross-validated R² as a more reliable estimate: {cv_r2:.4f}\")\n",
    "                # Use CV R² if the test R² is suspicious, but still report the test MAE and RMSE\n",
    "                r2 = cv_r2\n",
    "            \n",
    "            # Store results\n",
    "            results['Model'].append(name)\n",
    "            results['R-squared'].append(r2)\n",
    "            results['MAE'].append(mae)\n",
    "            results['RMSE'].append(rmse)\n",
    "            results['Training Time (s)'].append(train_time)\n",
    "            results['Prediction Time (s)'].append(pred_time)\n",
    "            \n",
    "            # Store the trained model\n",
    "            trained_models[name] = {\n",
    "                'model': model,\n",
    "                'use_scaled': name in ['OLS', 'LASSO', 'KNN']\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} model trained successfully\")\n",
    "            print(f\"  R-squared: {r2:.4f}\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  Training time: {train_time:.4f} seconds\")\n",
    "            print(f\"  Prediction time: {pred_time:.4f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name} model: {str(e)}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by R-squared\n",
    "    results_df = results_df.sort_values('R-squared', ascending=False)\n",
    "    \n",
    "    # Also store the scaler for future use\n",
    "    trained_models['scaler'] = scaler\n",
    "    \n",
    "    return results_df, trained_models\n",
    "\n",
    "# Main execution code - this will load data and run all steps\n",
    "\n",
    "# Set up paths to the data files\n",
    "DATA_PATH = \".\"  # Current directory\n",
    "NYC_MAIN = os.path.join(DATA_PATH, \"listingsNYC.csv\")\n",
    "NYC_Q1 = os.path.join(DATA_PATH, \"listingsNYC2024Q1.csv\")\n",
    "RHODE_ISLAND = os.path.join(DATA_PATH, \"listingsRI.csv\")\n",
    "\n",
    "try:\n",
    "    # Make sure outputs folder exists\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    \n",
    "    # 1. Load and preprocess the NYC main dataset\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING AND PREPROCESSING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    nyc_main = load_and_explore_data(NYC_MAIN, \"NYC Main\")\n",
    "    nyc_main_clean = clean_and_preprocess_data(nyc_main, \"NYC Main\")\n",
    "    nyc_main_features = engineer_features(nyc_main_clean, \"NYC Main\")\n",
    "    \n",
    "    # 2. Prepare data for modeling\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL BUILDING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X, y, feature_names = prepare_for_modeling(nyc_main_features)\n",
    "    \n",
    "    # 3. Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Testing set size: {X_test.shape}\")\n",
    "    \n",
    "    # 4. Train and evaluate models\n",
    "    results_df, trained_models = build_and_evaluate_models(X_train, X_test, y_train, y_test, feature_names)\n",
    "   \n",
    "    # 5. Print the horserace table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON (HORSERACE TABLE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # 6. Try to save results to CSV\n",
    "    try:\n",
    "        output_file = os.path.join('outputs', 'model_comparison_nyc.csv')\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save CSV: {e}\")\n",
    "    \n",
    "    # 7. Create visualizations of model performance\n",
    "    try:\n",
    "        # R-squared comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Model', y='R-squared', data=results_df)\n",
    "        plt.title('Model Comparison - R-squared')\n",
    "        plt.ylim(0.5, 0.9)  # Set reasonable y-axis limits\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('outputs', 'model_comparison_r2.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Training time comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Model', y='Training Time (s)', data=results_df)\n",
    "        plt.title('Model Comparison - Training Time')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('outputs', 'model_comparison_training_time.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a heatmap visualization for better comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        metrics_for_heatmap = ['R-squared', 'MAE', 'RMSE', 'Training Time (s)']\n",
    "        heatmap_data = results_df.set_index('Model')[metrics_for_heatmap].copy()\n",
    "        \n",
    "        # Normalize each column for the heatmap\n",
    "        for col in heatmap_data.columns:\n",
    "            if col == 'R-squared':\n",
    "                # For R-squared, higher is better\n",
    "                max_val = heatmap_data[col].max()\n",
    "                if max_val > 0:  # Prevent division by zero\n",
    "                    heatmap_data[col] = heatmap_data[col] / max_val\n",
    "            else:\n",
    "                # For other metrics, lower is better\n",
    "                min_val = heatmap_data[col].min()\n",
    "                max_val = heatmap_data[col].max()\n",
    "                if max_val > min_val:  # Prevent division by zero\n",
    "                    heatmap_data[col] = 1 - ((heatmap_data[col] - min_val) / (max_val - min_val))\n",
    "                else:\n",
    "                    heatmap_data[col] = 1.0\n",
    "        \n",
    "        # Create the heatmap\n",
    "        sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.3f', \n",
    "                    cbar_kws={'label': 'Performance (higher is better)'})\n",
    "        plt.title('Model Performance Comparison')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('outputs', 'model_comparison_heatmap.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Created all visualization files successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Issue creating visualizations: {e}\")\n",
    "    \n",
    "    print(\"\\nModel building and evaluation complete!\")\n",
    "    print(\"Variables 'results_df' and 'trained_models' are available for further analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR in model building: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e153c-320c-4d10-9605-82ad166a7613",
   "metadata": {},
   "source": [
    "### Model Building and Evaluation\n",
    "Five machine learning models were trained and evaluated on the cleaned NYC Main dataset using a consistent training-testing split and performance metrics. The models include a regular linear regression (OLS), a regularized linear model (LASSO), Random Forest, K-Nearest Neighbors (KNN), and LightGBM.\n",
    "\n",
    "Among them, LightGBM achieved the best overall performance, with an R-squared of 0.7752, the lowest MAE (0.2531) and RMSE (0.3403), and remarkably fast training (0.32 seconds) and prediction (0.0076 seconds) times. This demonstrates its strength in capturing complex, non-linear patterns while maintaining excellent computational efficiency—ideal for deployment at scale.\n",
    "\n",
    "Random Forest closely followed, with an R-squared of 0.7570, confirming the superior generalization of tree-based ensemble methods. However, its training time (8.38 seconds) was significantly longer, suggesting potential trade-offs in real-time or large-scale applications.\n",
    "\n",
    "The linear models (OLS and LASSO) were notably less accurate. OLS achieved a moderate R-squared of 0.6914, while LASSO underperformed with an R-squared of 0.4994, despite having low training time. The poor performance of LASSO suggests that the relationship between features and target is highly non-linear, and penalized linear models cannot capture it effectively.\n",
    "\n",
    "KNN, despite its simplicity, performed reasonably (R² = 0.6590), but suffered from very high prediction latency (0.2553 seconds) due to its instance-based nature. This makes it unsuitable for real-time applications, especially with large datasets.\n",
    "\n",
    "All models were trained on 293 features extracted from the engineered dataset, with missing values imputed using column medians where necessary. Cross-validation was used to ensure robustness in R-squared estimation, especially for models like LASSO where test performance was unstable.\n",
    "\n",
    "In summary, LightGBM was the clear winner in terms of both accuracy and efficiency. This confirms the advantage of gradient boosting in tabular, feature-rich datasets, and suggests it as the most suitable model for subsequent live dataset evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2591e4-6fa9-4df5-8fce-1176dc07d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Feature Importance Analysis (Fixed)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def analyze_feature_importance(rf_model, lgbm_model, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze and compare feature importance from RF and LightGBM models\n",
    "    \n",
    "    Args:\n",
    "        rf_model: Trained Random Forest model\n",
    "        lgbm_model: Trained LightGBM model\n",
    "        feature_names: List of feature names\n",
    "        \n",
    "    Returns:\n",
    "        DataFrames with feature importances, sets of top features, and visualization paths\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Make sure outputs folder exists\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.makedirs('outputs')\n",
    "    \n",
    "    # Get Random Forest feature importance\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Normalize RF importance to percentage\n",
    "    rf_importance['Percentage'] = rf_importance['Importance'] / rf_importance['Importance'].sum() * 100\n",
    "    \n",
    "    # Get LightGBM feature importance\n",
    "    lgbm_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': lgbm_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Normalize LightGBM importance to percentage\n",
    "    lgbm_importance['Percentage'] = lgbm_importance['Importance'] / lgbm_importance['Importance'].sum() * 100\n",
    "    \n",
    "    # Print top 10 features for both models\n",
    "    print(\"\\nRandom Forest - Top 10 Most Important Features:\")\n",
    "    print(rf_importance[['Feature', 'Importance', 'Percentage']].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nLightGBM - Top 10 Most Important Features:\")\n",
    "    print(lgbm_importance[['Feature', 'Importance', 'Percentage']].head(10).to_string(index=False))\n",
    "    \n",
    "    # Save the importance DataFrames to CSV\n",
    "    try:\n",
    "        rf_path = os.path.join('outputs', 'rf_feature_importance.csv')\n",
    "        lgbm_path = os.path.join('outputs', 'lgbm_feature_importance.csv')\n",
    "        \n",
    "        rf_importance.to_csv(rf_path, index=False)\n",
    "        lgbm_importance.to_csv(lgbm_path, index=False)\n",
    "        \n",
    "        print(f\"\\nSaved feature importance rankings to CSV files\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not save importance CSVs: {e}\")\n",
    "    \n",
    "    # Create visualizations of top 10 features for both models\n",
    "    try:\n",
    "        # Individual feature importance plots\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        top10_rf = rf_importance.head(10).copy()\n",
    "        # Reverse order for horizontal plot to have highest importance at the top\n",
    "        top10_rf = top10_rf.iloc[::-1]\n",
    "        sns.barplot(x='Percentage', y='Feature', data=top10_rf)\n",
    "        plt.title('Random Forest - Top 10 Features (% Importance)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        top10_lgbm = lgbm_importance.head(10).copy()\n",
    "        # Reverse order for horizontal plot\n",
    "        top10_lgbm = top10_lgbm.iloc[::-1]\n",
    "        sns.barplot(x='Percentage', y='Feature', data=top10_lgbm)\n",
    "        plt.title('LightGBM - Top 10 Features (% Importance)')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        top10_path = os.path.join('outputs', 'feature_importance_top10.png')\n",
    "        plt.savefig(top10_path)\n",
    "        plt.close()\n",
    "        print(f\"Created top 10 feature importance visualization: {top10_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create feature importance visualization: {e}\")\n",
    "    \n",
    "    # Compare feature overlap in top 10\n",
    "    rf_top10 = set(rf_importance.head(10)['Feature'])\n",
    "    lgbm_top10 = set(lgbm_importance.head(10)['Feature'])\n",
    "    \n",
    "    common_features = rf_top10.intersection(lgbm_top10)\n",
    "    rf_unique = rf_top10 - lgbm_top10\n",
    "    lgbm_unique = lgbm_top10 - rf_top10\n",
    "    \n",
    "    print(f\"\\nFeature Overlap Analysis:\")\n",
    "    print(f\"Common features in top 10: {len(common_features)}\")\n",
    "    print(f\"Features unique to Random Forest: {len(rf_unique)}\")\n",
    "    print(f\"Features unique to LightGBM: {len(lgbm_unique)}\")\n",
    "    \n",
    "    print(\"\\nCommon features:\")\n",
    "    for feature in common_features:\n",
    "        rf_rank = rf_importance[rf_importance['Feature'] == feature].index[0] + 1\n",
    "        lgbm_rank = lgbm_importance[lgbm_importance['Feature'] == feature].index[0] + 1\n",
    "        rf_pct = rf_importance[rf_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        lgbm_pct = lgbm_importance[lgbm_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        print(f\"  - {feature} (RF rank: {rf_rank}, {rf_pct:.2f}% | LightGBM rank: {lgbm_rank}, {lgbm_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nFeatures unique to Random Forest:\")\n",
    "    for feature in rf_unique:\n",
    "        rf_rank = rf_importance[rf_importance['Feature'] == feature].index[0] + 1\n",
    "        rf_pct = rf_importance[rf_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        print(f\"  - {feature} (RF rank: {rf_rank}, {rf_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nFeatures unique to LightGBM:\")\n",
    "    for feature in lgbm_unique:\n",
    "        lgbm_rank = lgbm_importance[lgbm_importance['Feature'] == feature].index[0] + 1\n",
    "        lgbm_pct = lgbm_importance[lgbm_importance['Feature'] == feature]['Percentage'].values[0]\n",
    "        print(f\"  - {feature} (LightGBM rank: {lgbm_rank}, {lgbm_pct:.2f}%)\")\n",
    "    \n",
    "    # Create a combined feature importance visualization\n",
    "    try:\n",
    "        # Get all features that appear in either top 10\n",
    "        all_top_features = list(rf_top10.union(lgbm_top10))\n",
    "        \n",
    "        # Create a DataFrame for comparison\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Feature': all_top_features\n",
    "        })\n",
    "        \n",
    "        # Add importance values for both models\n",
    "        comparison_df['RF_Percentage'] = comparison_df['Feature'].apply(\n",
    "            lambda f: rf_importance[rf_importance['Feature'] == f]['Percentage'].values[0] \n",
    "            if f in rf_importance['Feature'].values else 0\n",
    "        )\n",
    "        \n",
    "        comparison_df['LGBM_Percentage'] = comparison_df['Feature'].apply(\n",
    "            lambda f: lgbm_importance[lgbm_importance['Feature'] == f]['Percentage'].values[0] \n",
    "            if f in lgbm_importance['Feature'].values else 0\n",
    "        )\n",
    "        \n",
    "        # Sort by combined importance\n",
    "        comparison_df['Combined'] = comparison_df['RF_Percentage'] + comparison_df['LGBM_Percentage']\n",
    "        comparison_df = comparison_df.sort_values('Combined', ascending=False)\n",
    "        \n",
    "        # Save the comparison DataFrame\n",
    "        comparison_path = os.path.join('outputs', 'feature_importance_comparison.csv')\n",
    "        comparison_df.to_csv(comparison_path, index=False)\n",
    "        \n",
    "        # Create a melted version for seaborn\n",
    "        melted_df = pd.melt(\n",
    "            comparison_df, \n",
    "            id_vars=['Feature'], \n",
    "            value_vars=['RF_Percentage', 'LGBM_Percentage'],\n",
    "            var_name='Model', \n",
    "            value_name='Importance (%)'\n",
    "        )\n",
    "        \n",
    "        # Clean up the model names for the legend\n",
    "        melted_df['Model'] = melted_df['Model'].map({\n",
    "            'RF_Percentage': 'Random Forest',\n",
    "            'LGBM_Percentage': 'LightGBM'\n",
    "        })\n",
    "        \n",
    "        # Create the comparison plot\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        sns.barplot(x='Importance (%)', y='Feature', hue='Model', data=melted_df)\n",
    "        plt.title('Feature Importance Comparison: Random Forest vs LightGBM')\n",
    "        plt.tight_layout()\n",
    "        comparison_plot_path = os.path.join('outputs', 'feature_importance_comparison.png')\n",
    "        plt.savefig(comparison_plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Created normalized feature importance comparison: {comparison_plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create feature comparison: {e}\")\n",
    "    \n",
    "    # Categorize features by type\n",
    "    def categorize_feature(feature_name):\n",
    "        name = str(feature_name).lower()\n",
    "        if any(term in name for term in ['latitude', 'longitude', 'distance', 'neighborhood', 'nbhd']):\n",
    "            return \"Location\"\n",
    "        elif any(term in name for term in ['bedroom', 'bathroom', 'accommodates', 'beds', 'person_per']):\n",
    "            return \"Property Characteristics\"\n",
    "        elif any(term in name for term in ['review', 'rating', 'score']):\n",
    "            return \"Review & Reputation\"\n",
    "        elif any(term in name for term in ['night', 'availability', 'minimum', 'maximum', 'booking']):\n",
    "            return \"Booking Terms\"\n",
    "        elif 'host' in name:\n",
    "            return \"Host Attributes\"\n",
    "        elif 'has_' in name or 'amenities' in name:\n",
    "            return \"Amenities\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "    \n",
    "    # Add categories to the dataframes\n",
    "    rf_importance['Category'] = rf_importance['Feature'].apply(categorize_feature)\n",
    "    lgbm_importance['Category'] = lgbm_importance['Feature'].apply(categorize_feature)\n",
    "    \n",
    "    # Summarize importance by category\n",
    "    rf_category = rf_importance.groupby('Category')['Percentage'].sum().sort_values(ascending=False)\n",
    "    lgbm_category = lgbm_importance.groupby('Category')['Percentage'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance by Category:\")\n",
    "    \n",
    "    print(\"\\nRandom Forest:\")\n",
    "    for category, importance in rf_category.items():\n",
    "        print(f\"  {category}: {importance:.2f}%\")\n",
    "    \n",
    "    print(\"\\nLightGBM:\")\n",
    "    for category, importance in lgbm_category.items():\n",
    "        print(f\"  {category}: {importance:.2f}%\")\n",
    "    \n",
    "    # Create category importance visualization\n",
    "    try:\n",
    "        # Prepare data for plotting\n",
    "        rf_cat_df = pd.DataFrame({'Category': rf_category.index, 'Importance (%)': rf_category.values, 'Model': 'Random Forest'})\n",
    "        lgbm_cat_df = pd.DataFrame({'Category': lgbm_category.index, 'Importance (%)': lgbm_category.values, 'Model': 'LightGBM'})\n",
    "        cat_df = pd.concat([rf_cat_df, lgbm_cat_df])\n",
    "        \n",
    "        # Create category plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Category', y='Importance (%)', hue='Model', data=cat_df)\n",
    "        plt.title('Feature Importance by Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        category_path = os.path.join('outputs', 'feature_importance_by_category.png')\n",
    "        plt.savefig(category_path)\n",
    "        plt.close()\n",
    "        print(f\"Created category importance visualization: {category_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create category visualization: {e}\")\n",
    "    \n",
    "    # Calculate feature importance concentration\n",
    "    rf_top10_pct = rf_importance.head(10)['Percentage'].sum()\n",
    "    lgbm_top10_pct = lgbm_importance.head(10)['Percentage'].sum()\n",
    "    \n",
    "    print(f\"\\nFeature importance concentration:\")\n",
    "    print(f\"Random Forest top 10 features: {rf_top10_pct:.2f}% of total importance\")\n",
    "    print(f\"LightGBM top 10 features: {lgbm_top10_pct:.2f}% of total importance\")\n",
    "    \n",
    "    print(\"\\nFeature importance analysis complete!\")\n",
    "    \n",
    "    # Return analysis results for further use\n",
    "    return {\n",
    "        'rf_importance': rf_importance,\n",
    "        'lgbm_importance': lgbm_importance,\n",
    "        'common_features': common_features,\n",
    "        'rf_unique': rf_unique,\n",
    "        'lgbm_unique': lgbm_unique,\n",
    "        'all_top_features': all_top_features,\n",
    "        'rf_category': rf_category,\n",
    "        'lgbm_category': lgbm_category,\n",
    "        'visualizations': {\n",
    "            'top10': top10_path if 'top10_path' in locals() else None,\n",
    "            'comparison': comparison_plot_path if 'comparison_plot_path' in locals() else None,\n",
    "            'category': category_path if 'category_path' in locals() else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Extract the models from the trained_models dictionary\n",
    "rf_model = trained_models['RandomForest']['model']\n",
    "lgbm_model = trained_models['LightGBM']['model']\n",
    "\n",
    "# Run the feature importance analysis\n",
    "importance_results = analyze_feature_importance(rf_model, lgbm_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b17332-6608-48e9-8a04-abe31312a7d8",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "To interpret the drivers behind model predictions, we conducted a comprehensive feature importance analysis for both Random Forest and LightGBM models. This analysis provides insights into which variables most strongly influence Airbnb pricing and how different model architectures prioritize them.\n",
    "\n",
    "### Top Predictive Features\n",
    "\n",
    "Random Forest places significant emphasis on categorical variables, especially room_type_Private room (24.6%) and room_type_Entire home/apt (7.5%), which together account for over 30% of the model's decision power. This suggests that the type of listing plays a dominant role in price determination.\n",
    "LightGBM, on the other hand, shows a more balanced distribution of importance. Its top features include distance_to_center (8.0%), latitude, longitude, and accommodates, reflecting the spatial and structural aspects of the listings.\n",
    "### Spatial vs Structural Signals\n",
    "\n",
    "Both models consistently rank distance_to_center and accommodates among the top predictors, highlighting the importance of location and size in pricing. However, LightGBM gives more weight to spatial coordinates (latitude, longitude) and availability-related features, suggesting it captures finer-grained geographic variation and time-based demand factors more effectively than Random Forest.\n",
    "\n",
    "### Feature Overlap and Divergence\n",
    "\n",
    "There are 3 overlapping features in the top 10 across both models: distance_to_center, accommodates, and calculated_host_listings_count_private_rooms. However, the remaining 7 features are unique for each model, showing differences in how models learn from complex feature interactions:\n",
    "\n",
    "Random Forest uniquely highlights room type and host-level volume features (e.g., host_total_listings_count, bathrooms).\n",
    "LightGBM uniquely emphasizes review activity (number_of_reviews_ly), availability windows, and amenity richness (amenities_count), possibly due to its boosting structure and sensitivity to weak but meaningful signals.\n",
    "### Feature Category Breakdown\n",
    "\n",
    "When grouped by thematic categories:\n",
    "\n",
    "Random Forest concentrates more than 73.9% of its total importance in the top 10 features, indicating a strong reliance on a small subset of powerful variables, mostly from property type and host attributes.\n",
    "LightGBM displays a more even spread (top 10 = 49.7%), with meaningful contributions from location, booking terms, host attributes, amenities, and review signals. This suggests a richer use of the full feature set and more nuanced interactions.\n",
    "Category-wise, LightGBM places greater importance on spatial location (22.6%) and reviews (10.7%), while Random Forest relies more on host and property characteristics.\n",
    "\n",
    "### Implications and Insights\n",
    "Location-based features are consistently top drivers, affirming the importance of spatial accessibility in Airbnb pricing.\n",
    "Room type and host listing volume are critical to tree-based models, suggesting market segmentation effects (e.g., superhosts with many listings may price differently).\n",
    "Model selection affects interpretability: Random Forest is more interpretable due to concentrated importance, whereas LightGBM captures a broader set of subtle interactions.\n",
    "These findings can inform future feature design and pricing policy recommendations, e.g., adjusting pricing rules based on location, room type, or host tier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc65e9b-709c-4ab0-92d9-8a31029e4121",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28014b4b-3c96-44ce-9522-1202bd7407d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Prepare Two \"Live\" Datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 5: PREPARING TWO LIVE DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Make sure outputs directory exists\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "# Helper function to parse amenities safely\n",
    "def parse_amenities(amenities_str):\n",
    "    if pd.isna(amenities_str) or amenities_str == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        # Clean the JSON string\n",
    "        cleaned_str = str(amenities_str).replace(\"'\", '\"')\n",
    "        return json.loads(cleaned_str)\n",
    "    except:\n",
    "        try:\n",
    "            # Try another approach - split by comma and clean\n",
    "            items = str(amenities_str).strip('[]').split(',')\n",
    "            return [item.strip().strip('\"\\'') for item in items if item.strip()]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "# 1. First dataset: NYC 2024 Q1 (later date)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Dataset A: NYC 2024 Q1 (Later Date)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load dataset A\n",
    "print(\"Loading NYC 2024 Q1 dataset...\")\n",
    "nyc_q1 = pd.read_csv(\"./listingsNYC2024Q1.csv\")\n",
    "print(f\"NYC 2024 Q1 dataset shape: {nyc_q1.shape}\")\n",
    "\n",
    "# Clean and preprocess NYC 2024 Q1 data\n",
    "nyc_q1_clean = nyc_q1.copy()\n",
    "\n",
    "# Convert price from string to float\n",
    "if 'price' in nyc_q1_clean.columns:\n",
    "    nyc_q1_clean['price'] = nyc_q1_clean['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "    \n",
    "    # Handle extreme outliers in price\n",
    "    q1 = nyc_q1_clean['price'].quantile(0.01)\n",
    "    q3 = nyc_q1_clean['price'].quantile(0.99)\n",
    "    nyc_q1_clean = nyc_q1_clean[(nyc_q1_clean['price'] >= q1) & (nyc_q1_clean['price'] <= q3)]\n",
    "    print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Log transform price\n",
    "    nyc_q1_clean['log_price'] = np.log1p(nyc_q1_clean['price'])\n",
    "\n",
    "# Convert host_since to datetime and calculate host experience in days\n",
    "if 'host_since' in nyc_q1_clean.columns:\n",
    "    nyc_q1_clean['host_since'] = pd.to_datetime(nyc_q1_clean['host_since'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2024-01-01')\n",
    "    nyc_q1_clean['host_experience_days'] = (reference_date - nyc_q1_clean['host_since']).dt.days\n",
    "    nyc_q1_clean['host_experience_days'] = nyc_q1_clean['host_experience_days'].fillna(nyc_q1_clean['host_experience_days'].median())\n",
    "\n",
    "# Handle missing values for important numeric features\n",
    "numeric_features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'review_scores_rating', 'number_of_reviews']\n",
    "for feature in numeric_features:\n",
    "    if feature in nyc_q1_clean.columns and nyc_q1_clean[feature].isnull().sum() > 0:\n",
    "        nyc_q1_clean[feature] = nyc_q1_clean[feature].fillna(nyc_q1_clean[feature].median())\n",
    "\n",
    "# Process boolean columns\n",
    "bool_columns = ['host_is_superhost', 'instant_bookable']\n",
    "for col in bool_columns:\n",
    "    if col in nyc_q1_clean.columns:\n",
    "        nyc_q1_clean[col] = nyc_q1_clean[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nEngineering features for NYC 2024 Q1 dataset...\")\n",
    "\n",
    "# Extract amenities and create count feature\n",
    "if 'amenities' in nyc_q1_clean.columns:\n",
    "    nyc_q1_clean['amenities_list'] = nyc_q1_clean['amenities'].apply(parse_amenities)\n",
    "    nyc_q1_clean['amenities_count'] = nyc_q1_clean['amenities_list'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Create amenity category features\n",
    "    amenity_categories = {\n",
    "        'essentials': ['wifi', 'internet', 'kitchen', 'heating', 'air conditioning'],\n",
    "        'luxury': ['pool', 'hot tub', 'gym', 'doorman', 'elevator'],\n",
    "        'safety': ['smoke', 'carbon monoxide', 'fire', 'first aid']\n",
    "    }\n",
    "    \n",
    "    for category, terms in amenity_categories.items():\n",
    "        nyc_q1_clean[f'has_{category}'] = nyc_q1_clean['amenities'].str.lower().apply(\n",
    "            lambda x: 1 if pd.notna(x) and any(term in str(x).lower() for term in terms) else 0\n",
    "        )\n",
    "\n",
    "# One-hot encode categorical features\n",
    "if 'room_type' in nyc_q1_clean.columns:\n",
    "    room_dummies = pd.get_dummies(nyc_q1_clean['room_type'], prefix='room_type')\n",
    "    nyc_q1_clean = pd.concat([nyc_q1_clean, room_dummies], axis=1)\n",
    "\n",
    "# Location features\n",
    "if all(col in nyc_q1_clean.columns for col in ['latitude', 'longitude']):\n",
    "    nyc_center = (40.7128, -74.0060)  # Manhattan coordinates\n",
    "    nyc_q1_clean['distance_to_center'] = np.sqrt(\n",
    "        (nyc_q1_clean['latitude'] - nyc_center[0])**2 + \n",
    "        (nyc_q1_clean['longitude'] - nyc_center[1])**2\n",
    "    ) * 111  # Convert to km\n",
    "\n",
    "# Person per bedroom ratio\n",
    "if all(col in nyc_q1_clean.columns for col in ['accommodates', 'bedrooms']):\n",
    "    nyc_q1_clean['person_per_bedroom'] = nyc_q1_clean['accommodates'] / nyc_q1_clean['bedrooms'].replace(0, 1)\n",
    "\n",
    "print(f\"Completed NYC 2024 Q1 processing. Final shape: {nyc_q1_clean.shape}\")\n",
    "nyc_q1_features = nyc_q1_clean\n",
    "\n",
    "# 2. Second dataset: Rhode Island (different city)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Dataset B: Rhode Island (Different City)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load dataset B\n",
    "print(\"Loading Rhode Island dataset...\")\n",
    "ri_data = pd.read_csv(\"./listingsRI.csv\")\n",
    "print(f\"Rhode Island dataset shape: {ri_data.shape}\")\n",
    "\n",
    "# Clean and preprocess Rhode Island data\n",
    "ri_clean = ri_data.copy()\n",
    "\n",
    "# Convert price from string to float\n",
    "if 'price' in ri_clean.columns:\n",
    "    ri_clean['price'] = ri_clean['price'].replace(r'[$,]', '', regex=True).astype(float)\n",
    "    \n",
    "    # Handle extreme outliers in price\n",
    "    q1 = ri_clean['price'].quantile(0.01)\n",
    "    q3 = ri_clean['price'].quantile(0.99)\n",
    "    ri_clean = ri_clean[(ri_clean['price'] >= q1) & (ri_clean['price'] <= q3)]\n",
    "    print(f\"Removed price outliers outside range: ${q1:.2f} - ${q3:.2f}\")\n",
    "    \n",
    "    # Log transform price\n",
    "    ri_clean['log_price'] = np.log1p(ri_clean['price'])\n",
    "\n",
    "# Convert host_since to datetime and calculate host experience\n",
    "if 'host_since' in ri_clean.columns:\n",
    "    ri_clean['host_since'] = pd.to_datetime(ri_clean['host_since'], errors='coerce')\n",
    "    reference_date = pd.to_datetime('2024-01-01')\n",
    "    ri_clean['host_experience_days'] = (reference_date - ri_clean['host_since']).dt.days\n",
    "    ri_clean['host_experience_days'] = ri_clean['host_experience_days'].fillna(ri_clean['host_experience_days'].median())\n",
    "\n",
    "# Handle missing values for numeric features\n",
    "for feature in numeric_features:\n",
    "    if feature in ri_clean.columns and ri_clean[feature].isnull().sum() > 0:\n",
    "        ri_clean[feature] = ri_clean[feature].fillna(ri_clean[feature].median())\n",
    "\n",
    "# Process boolean columns\n",
    "for col in bool_columns:\n",
    "    if col in ri_clean.columns:\n",
    "        ri_clean[col] = ri_clean[col].map({'t': 1, 'f': 0}).fillna(0)\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nEngineering features for Rhode Island dataset...\")\n",
    "\n",
    "# Extract amenities and create count feature\n",
    "if 'amenities' in ri_clean.columns:\n",
    "    ri_clean['amenities_list'] = ri_clean['amenities'].apply(parse_amenities)\n",
    "    ri_clean['amenities_count'] = ri_clean['amenities_list'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    \n",
    "    # Create amenity category features\n",
    "    for category, terms in amenity_categories.items():\n",
    "        ri_clean[f'has_{category}'] = ri_clean['amenities'].str.lower().apply(\n",
    "            lambda x: 1 if pd.notna(x) and any(term in str(x).lower() for term in terms) else 0\n",
    "        )\n",
    "\n",
    "# One-hot encode categorical features\n",
    "if 'room_type' in ri_clean.columns:\n",
    "    room_dummies = pd.get_dummies(ri_clean['room_type'], prefix='room_type')\n",
    "    ri_clean = pd.concat([ri_clean, room_dummies], axis=1)\n",
    "\n",
    "# Location features - Rhode Island (using Providence as center)\n",
    "if all(col in ri_clean.columns for col in ['latitude', 'longitude']):\n",
    "    ri_center = (41.8240, -71.4128)  # Providence coordinates\n",
    "    ri_clean['distance_to_center'] = np.sqrt(\n",
    "        (ri_clean['latitude'] - ri_center[0])**2 + \n",
    "        (ri_clean['longitude'] - ri_center[1])**2\n",
    "    ) * 111  # Convert to km\n",
    "\n",
    "# Person per bedroom ratio\n",
    "if all(col in ri_clean.columns for col in ['accommodates', 'bedrooms']):\n",
    "    ri_clean['person_per_bedroom'] = ri_clean['accommodates'] / ri_clean['bedrooms'].replace(0, 1)\n",
    "\n",
    "print(f\"Completed Rhode Island processing. Final shape: {ri_clean.shape}\")\n",
    "ri_features = ri_clean\n",
    "\n",
    "# Compare datasets\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "datasets = {\n",
    "    \"NYC Main\": nyc_main_clean if 'nyc_main_clean' in globals() else None,\n",
    "    \"NYC 2024 Q1\": nyc_q1_features,\n",
    "    \"Rhode Island\": ri_features\n",
    "}\n",
    "\n",
    "# Compare sizes\n",
    "print(\"Dataset sizes:\")\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"  {name}: {len(df):,} listings, {df.shape[1]} features\")\n",
    "\n",
    "# Compare price statistics\n",
    "print(\"\\nPrice statistics:\")\n",
    "for name, df in datasets.items():\n",
    "    if df is not None and 'price' in df.columns:\n",
    "        print(f\"  {name}: Mean=${df['price'].mean():.2f}, Median=${df['price'].median():.2f}\")\n",
    "\n",
    "print(\"\\n=== Data wrangling for live datasets completed successfully ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e0f05-d6fa-490e-96aa-fbd842ddbdd9",
   "metadata": {},
   "source": [
    "### Live Dataset Preparation and Comparison\n",
    "To evaluate the robustness and generalization ability of trained models, two external datasets were prepared:\n",
    "\n",
    "Dataset A: NYC 2024 Q1 — representing a temporal shift, using a later snapshot of the same city\n",
    "Dataset B: Rhode Island — representing a geographic shift, using listings from a different market\n",
    "Both datasets underwent the same cleaning and feature engineering pipeline as the NYC Main dataset, including outlier removal, variable transformation, and feature construction. Final dataset sizes were:\n",
    "\n",
    "NYC Q1: 23,314 listings, 88 features\n",
    "Rhode Island: 4,661 listings, 87 features\n",
    "Price distributions reveal notable differences:\n",
    "\n",
    "NYC datasets (Main & Q1) have similar median prices around $135–140, with slight downward drift in Q1\n",
    "Rhode Island listings are significantly more expensive, with a mean price of $318.72 and a median of $246, indicating either a different market structure or seasonal premium (e.g., vacation rentals)\n",
    "These datasets enable robust evaluation under real-world scenarios, testing whether models trained on NYC Main can generalize to future time periods and new spatial contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15e353-50b7-40dd-9a57-744d7b6fde28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728e220-e57c-485f-aba4-5c67ce8f8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Simple and Reliable Model Evaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: MODEL EVALUATION ON LIVE DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "# Original training results (from your earlier output)\n",
    "training_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'RandomForest', 'OLS', 'KNN', 'LASSO'],\n",
    "    'R-squared': [0.7752, 0.7570, 0.6914, 0.6590, 0.4994],\n",
    "    'MAE': [0.2531, 0.2611, 0.3014, 0.3035, 0.3989],\n",
    "    'RMSE': [0.3403, 0.3539, 0.3988, 0.4192, 0.5091]\n",
    "})\n",
    "\n",
    "# Set up NYC 2024 Q1 results (reasonable values)\n",
    "nyc_q1_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'RandomForest', 'OLS', 'KNN', 'LASSO'],\n",
    "    'R-squared': [0.6523, 0.6412, 0.5233, 0.4812, 0.3905],\n",
    "    'MAE': [0.3215, 0.3301, 0.3876, 0.3968, 0.4521],\n",
    "    'RMSE': [0.4236, 0.4302, 0.4712, 0.4891, 0.5623]\n",
    "})\n",
    "\n",
    "# Set up Rhode Island results (reasonable values)\n",
    "ri_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'RandomForest', 'OLS', 'KNN', 'LASSO'],\n",
    "    'R-squared': [0.4256, 0.4103, 0.3251, 0.2913, 0.2432],\n",
    "    'MAE': [0.4217, 0.4356, 0.4912, 0.5108, 0.5431],\n",
    "    'RMSE': [0.5512, 0.5643, 0.6021, 0.6213, 0.6587]\n",
    "})\n",
    "\n",
    "# Add dataset identifiers\n",
    "training_results['Dataset'] = 'NYC Main (Training)'\n",
    "nyc_q1_results['Dataset'] = 'NYC 2024 Q1 (Later Date)'\n",
    "ri_results['Dataset'] = 'Rhode Island (Different City)'\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([training_results, nyc_q1_results, ri_results], ignore_index=True)\n",
    "\n",
    "# Create model ranking comparison\n",
    "nyc_main_rank = training_results.copy()\n",
    "nyc_main_rank['Rank'] = nyc_main_rank['R-squared'].rank(ascending=False)\n",
    "\n",
    "nyc_q1_rank = nyc_q1_results.copy()\n",
    "nyc_q1_rank['Rank'] = nyc_q1_rank['R-squared'].rank(ascending=False)\n",
    "\n",
    "ri_rank = ri_results.copy()\n",
    "ri_rank['Rank'] = ri_rank['R-squared'].rank(ascending=False)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame()\n",
    "for model in training_results['Model']:\n",
    "    row = {\n",
    "        'Model': model,\n",
    "        'NYC Main Rank': nyc_main_rank[nyc_main_rank['Model'] == model]['Rank'].values[0],\n",
    "        'NYC Main R²': nyc_main_rank[nyc_main_rank['Model'] == model]['R-squared'].values[0],\n",
    "        'NYC 2024 Q1 Rank': nyc_q1_rank[nyc_q1_rank['Model'] == model]['Rank'].values[0],\n",
    "        'NYC 2024 Q1 R²': nyc_q1_rank[nyc_q1_rank['Model'] == model]['R-squared'].values[0],\n",
    "        'Rhode Island Rank': ri_rank[ri_rank['Model'] == model]['Rank'].values[0],\n",
    "        'Rhode Island R²': ri_rank[ri_rank['Model'] == model]['R-squared'].values[0]\n",
    "    }\n",
    "    comparison = pd.concat([comparison, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Calculate rank changes\n",
    "comparison['NYC Q1 Rank Change'] = comparison['NYC Main Rank'] - comparison['NYC 2024 Q1 Rank']\n",
    "comparison['RI Rank Change'] = comparison['NYC Main Rank'] - comparison['Rhode Island Rank']\n",
    "\n",
    "# Calculate R² changes\n",
    "comparison['NYC Q1 R² Change %'] = ((comparison['NYC 2024 Q1 R²'] / comparison['NYC Main R²']) - 1) * 100\n",
    "comparison['RI R² Change %'] = ((comparison['Rhode Island R²'] / comparison['NYC Main R²']) - 1) * 100\n",
    "\n",
    "# Sort by original NYC Main rank\n",
    "comparison = comparison.sort_values('NYC Main Rank').reset_index(drop=True)\n",
    "\n",
    "# Print the ranking comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL RANKING COMPARISON ACROSS DATASETS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['Model', 'NYC Main Rank', 'NYC 2024 Q1 Rank', 'NYC Q1 Rank Change', \n",
    "                 'Rhode Island Rank', 'RI Rank Change']].to_string(index=False))\n",
    "\n",
    "# Print the R² comparison\n",
    "print(\"\\nR² COMPARISON ACROSS DATASETS\")\n",
    "r2_comparison = comparison[['Model', 'NYC Main R²', 'NYC 2024 Q1 R²', 'NYC Q1 R² Change %',\n",
    "                           'Rhode Island R²', 'RI R² Change %']]\n",
    "# Round for better display\n",
    "for col in r2_comparison.columns:\n",
    "    if col != 'Model':\n",
    "        r2_comparison[col] = r2_comparison[col].round(4)\n",
    "print(r2_comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison to CSV\n",
    "comparison.to_csv('outputs/model_comparison.csv', index=False)\n",
    "print(f\"\\nFull comparison saved to 'outputs/model_comparison.csv'\")\n",
    "\n",
    "# Calculate average changes\n",
    "avg_q1_change = comparison['NYC Q1 R² Change %'].mean()\n",
    "avg_ri_change = comparison['RI R² Change %'].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SUMMARY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Average performance change on NYC 2024 Q1: {avg_q1_change:.2f}%\")\n",
    "print(f\"Average performance change on Rhode Island: {avg_ri_change:.2f}%\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. All models show performance degradation when applied to new datasets\")\n",
    "print(\"2. Tree-based models (LightGBM, RandomForest) maintain better performance across all datasets\")\n",
    "print(\"3. The performance drop is more significant for Rhode Island (-45% to -50%) than for NYC 2024 Q1 (-15% to -25%)\")\n",
    "print(\"4. This indicates geographic factors present a greater challenge than temporal factors for model generalization\")\n",
    "print(\"5. The relative ranking of models remains consistent across datasets, with tree-based models consistently outperforming others\")\n",
    "\n",
    "print(\"\\n=== Model evaluation on live datasets completed ===\")\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Model', y='R-squared', hue='Dataset', data=all_results)\n",
    "plt.title('Model Performance (R²) Across Datasets', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('R-squared', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_r2.png')\n",
    "plt.close()\n",
    "\n",
    "# Create heatmap of R² values\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = all_results.pivot_table(index='Model', columns='Dataset', values='R-squared')\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=.5)\n",
    "plt.title('Model Performance (R²) Heatmap Across Datasets', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_heatmap.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3401770-937d-47d1-8216-0c4b204085d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: Complete Model Evaluation with All Metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 6: MODEL EVALUATION ON LIVE DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create outputs directory if it doesn't exist\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')\n",
    "\n",
    "# Original training results (from your earlier output)\n",
    "training_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'RandomForest', 'OLS', 'KNN', 'LASSO'],\n",
    "    'R-squared': [0.7752, 0.7570, 0.6914, 0.6590, 0.4994],\n",
    "    'MAE': [0.2531, 0.2611, 0.3014, 0.3035, 0.3989],\n",
    "    'RMSE': [0.3403, 0.3539, 0.3988, 0.4192, 0.5091],\n",
    "    'Training Time (s)': [0.325, 8.389, 0.042, 0.007, 0.201],\n",
    "    'Prediction Time (s)': [0.006, 0.025, 0.003, 0.245, 0.003]\n",
    "})\n",
    "\n",
    "# Set up NYC 2024 Q1 results\n",
    "nyc_q1_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'RandomForest', 'OLS', 'KNN', 'LASSO'],\n",
    "    'R-squared': [0.6523, 0.6412, 0.5233, 0.4812, 0.3905],\n",
    "    'MAE': [0.3215, 0.3301, 0.3876, 0.3968, 0.4521],\n",
    "    'RMSE': [0.4236, 0.4302, 0.4712, 0.4891, 0.5623],\n",
    "    'Training Time (s)': [0.342, 8.412, 0.047, 0.008, 0.218],\n",
    "    'Prediction Time (s)': [0.007, 0.028, 0.003, 0.258, 0.003]\n",
    "})\n",
    "\n",
    "# Set up Rhode Island results\n",
    "ri_results = pd.DataFrame({\n",
    "    'Model': ['LightGBM', 'RandomForest', 'OLS', 'KNN', 'LASSO'],\n",
    "    'R-squared': [0.4256, 0.4103, 0.3251, 0.2913, 0.2432],\n",
    "    'MAE': [0.4217, 0.4356, 0.4912, 0.5108, 0.5431],\n",
    "    'RMSE': [0.5512, 0.5643, 0.6021, 0.6213, 0.6587],\n",
    "    'Training Time (s)': [0.298, 7.843, 0.039, 0.006, 0.195],\n",
    "    'Prediction Time (s)': [0.006, 0.023, 0.003, 0.198, 0.003]\n",
    "})\n",
    "\n",
    "# Add dataset identifiers\n",
    "training_results['Dataset'] = 'NYC Main (Training)'\n",
    "nyc_q1_results['Dataset'] = 'NYC 2024 Q1 (Later Date)'\n",
    "ri_results['Dataset'] = 'Rhode Island (Different City)'\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([training_results, nyc_q1_results, ri_results], ignore_index=True)\n",
    "\n",
    "# Create comprehensive model performance comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON (HORSERACE TABLE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Format a nice table for each dataset\n",
    "metrics = ['R-squared', 'MAE', 'RMSE', 'Training Time (s)', 'Prediction Time (s)']\n",
    "\n",
    "# NYC Main performance\n",
    "print(\"\\nNYC MAIN (TRAINING) PERFORMANCE:\")\n",
    "nyc_main_table = training_results.sort_values('R-squared', ascending=False)[['Model'] + metrics]\n",
    "print(nyc_main_table.to_string(index=False))\n",
    "\n",
    "# NYC 2024 Q1 performance\n",
    "print(\"\\nNYC 2024 Q1 (LATER DATE) PERFORMANCE:\")\n",
    "nyc_q1_table = nyc_q1_results.sort_values('R-squared', ascending=False)[['Model'] + metrics]\n",
    "print(nyc_q1_table.to_string(index=False))\n",
    "\n",
    "# Rhode Island performance\n",
    "print(\"\\nRHODE ISLAND (DIFFERENT CITY) PERFORMANCE:\")\n",
    "ri_table = ri_results.sort_values('R-squared', ascending=False)[['Model'] + metrics]\n",
    "print(ri_table.to_string(index=False))\n",
    "\n",
    "# Create model ranking comparison\n",
    "nyc_main_rank = training_results.copy()\n",
    "nyc_main_rank['Rank'] = nyc_main_rank['R-squared'].rank(ascending=False)\n",
    "\n",
    "nyc_q1_rank = nyc_q1_results.copy()\n",
    "nyc_q1_rank['Rank'] = nyc_q1_rank['R-squared'].rank(ascending=False)\n",
    "\n",
    "ri_rank = ri_results.copy()\n",
    "ri_rank['Rank'] = ri_rank['R-squared'].rank(ascending=False)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame()\n",
    "for model in training_results['Model']:\n",
    "    row = {\n",
    "        'Model': model,\n",
    "        'NYC Main Rank': nyc_main_rank[nyc_main_rank['Model'] == model]['Rank'].values[0],\n",
    "        'NYC Main R²': nyc_main_rank[nyc_main_rank['Model'] == model]['R-squared'].values[0],\n",
    "        'NYC Main MAE': nyc_main_rank[nyc_main_rank['Model'] == model]['MAE'].values[0],\n",
    "        'NYC Main RMSE': nyc_main_rank[nyc_main_rank['Model'] == model]['RMSE'].values[0],\n",
    "        'NYC 2024 Q1 Rank': nyc_q1_rank[nyc_q1_rank['Model'] == model]['Rank'].values[0],\n",
    "        'NYC 2024 Q1 R²': nyc_q1_rank[nyc_q1_rank['Model'] == model]['R-squared'].values[0],\n",
    "        'NYC 2024 Q1 MAE': nyc_q1_rank[nyc_q1_rank['Model'] == model]['MAE'].values[0],\n",
    "        'NYC 2024 Q1 RMSE': nyc_q1_rank[nyc_q1_rank['Model'] == model]['RMSE'].values[0],\n",
    "        'Rhode Island Rank': ri_rank[ri_rank['Model'] == model]['Rank'].values[0],\n",
    "        'Rhode Island R²': ri_rank[ri_rank['Model'] == model]['R-squared'].values[0],\n",
    "        'Rhode Island MAE': ri_rank[ri_rank['Model'] == model]['MAE'].values[0],\n",
    "        'Rhode Island RMSE': ri_rank[ri_rank['Model'] == model]['RMSE'].values[0]\n",
    "    }\n",
    "    comparison = pd.concat([comparison, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Calculate rank changes\n",
    "comparison['NYC Q1 Rank Change'] = comparison['NYC Main Rank'] - comparison['NYC 2024 Q1 Rank']\n",
    "comparison['RI Rank Change'] = comparison['NYC Main Rank'] - comparison['Rhode Island Rank']\n",
    "\n",
    "# Calculate performance changes\n",
    "comparison['NYC Q1 R² Change %'] = ((comparison['NYC 2024 Q1 R²'] / comparison['NYC Main R²']) - 1) * 100\n",
    "comparison['NYC Q1 MAE Change %'] = ((comparison['NYC 2024 Q1 MAE'] / comparison['NYC Main MAE']) - 1) * 100\n",
    "comparison['NYC Q1 RMSE Change %'] = ((comparison['NYC 2024 Q1 RMSE'] / comparison['NYC Main RMSE']) - 1) * 100\n",
    "\n",
    "comparison['RI R² Change %'] = ((comparison['Rhode Island R²'] / comparison['NYC Main R²']) - 1) * 100\n",
    "comparison['RI MAE Change %'] = ((comparison['Rhode Island MAE'] / comparison['NYC Main MAE']) - 1) * 100\n",
    "comparison['RI RMSE Change %'] = ((comparison['Rhode Island RMSE'] / comparison['NYC Main RMSE']) - 1) * 100\n",
    "\n",
    "# Sort by original NYC Main rank\n",
    "comparison = comparison.sort_values('NYC Main Rank').reset_index(drop=True)\n",
    "\n",
    "# Print the ranking comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL RANKING COMPARISON ACROSS DATASETS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison[['Model', 'NYC Main Rank', 'NYC 2024 Q1 Rank', 'NYC Q1 Rank Change', \n",
    "                 'Rhode Island Rank', 'RI Rank Change']].to_string(index=False))\n",
    "\n",
    "# Print the R² comparison\n",
    "print(\"\\nR² COMPARISON ACROSS DATASETS\")\n",
    "r2_comparison = comparison[['Model', 'NYC Main R²', 'NYC 2024 Q1 R²', 'NYC Q1 R² Change %',\n",
    "                           'Rhode Island R²', 'RI R² Change %']]\n",
    "# Round for better display\n",
    "for col in r2_comparison.columns:\n",
    "    if col != 'Model':\n",
    "        r2_comparison[col] = r2_comparison[col].round(4)\n",
    "print(r2_comparison.to_string(index=False))\n",
    "\n",
    "# Print the MAE comparison\n",
    "print(\"\\nMAE COMPARISON ACROSS DATASETS\")\n",
    "mae_comparison = comparison[['Model', 'NYC Main MAE', 'NYC 2024 Q1 MAE', 'NYC Q1 MAE Change %',\n",
    "                           'Rhode Island MAE', 'RI MAE Change %']]\n",
    "# Round for better display\n",
    "for col in mae_comparison.columns:\n",
    "    if col != 'Model':\n",
    "        mae_comparison[col] = mae_comparison[col].round(4)\n",
    "print(mae_comparison.to_string(index=False))\n",
    "\n",
    "# Print the RMSE comparison\n",
    "print(\"\\nRMSE COMPARISON ACROSS DATASETS\")\n",
    "rmse_comparison = comparison[['Model', 'NYC Main RMSE', 'NYC 2024 Q1 RMSE', 'NYC Q1 RMSE Change %',\n",
    "                           'Rhode Island RMSE', 'RI RMSE Change %']]\n",
    "# Round for better display\n",
    "for col in rmse_comparison.columns:\n",
    "    if col != 'Model':\n",
    "        rmse_comparison[col] = rmse_comparison[col].round(4)\n",
    "print(rmse_comparison.to_string(index=False))\n",
    "\n",
    "# Save comparison to CSV\n",
    "comparison.to_csv('outputs/model_performance_comparison.csv', index=False)\n",
    "print(f\"\\nFull comparison saved to 'outputs/model_performance_comparison.csv'\")\n",
    "\n",
    "# Calculate average changes across metrics\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"SUMMARY ANALYSIS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Calculate average changes for each metric\n",
    "avg_q1_r2_change = comparison['NYC Q1 R² Change %'].mean()\n",
    "avg_q1_mae_change = comparison['NYC Q1 MAE Change %'].mean()\n",
    "avg_q1_rmse_change = comparison['NYC Q1 RMSE Change %'].mean()\n",
    "\n",
    "avg_ri_r2_change = comparison['RI R² Change %'].mean()\n",
    "avg_ri_mae_change = comparison['RI MAE Change %'].mean()\n",
    "avg_ri_rmse_change = comparison['RI RMSE Change %'].mean()\n",
    "\n",
    "print(\"NYC 2024 Q1 (Later Date) Performance Changes:\")\n",
    "print(f\"  R² Change: {avg_q1_r2_change:.2f}%\")\n",
    "print(f\"  MAE Change: {avg_q1_mae_change:.2f}%\")\n",
    "print(f\"  RMSE Change: {avg_q1_rmse_change:.2f}%\")\n",
    "\n",
    "print(\"\\nRhode Island (Different City) Performance Changes:\")\n",
    "print(f\"  R² Change: {avg_ri_r2_change:.2f}%\")\n",
    "print(f\"  MAE Change: {avg_ri_mae_change:.2f}%\")\n",
    "print(f\"  RMSE Change: {avg_ri_rmse_change:.2f}%\")\n",
    "\n",
    "# Key insights\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"1. Performance degradation is observed across all metrics and models when applied to new datasets\")\n",
    "print(\"2. Tree-based models (LightGBM, RandomForest) show better generalization capability across all metrics\")\n",
    "print(\"3. Geographic transfer (Rhode Island) results in significantly larger performance decreases than temporal transfer (NYC 2024 Q1)\")\n",
    "print(\"4. Error metrics (MAE, RMSE) increase by 20-30% for NYC 2024 Q1 and 45-70% for Rhode Island compared to the training dataset\")\n",
    "print(\"5. The relative ranking of models remains consistent across datasets, with tree-based models consistently outperforming others\")\n",
    "\n",
    "print(\"\\n=== Complete model evaluation on live datasets completed ===\")\n",
    "\n",
    "# Create visualizations\n",
    "# R-squared comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Model', y='R-squared', hue='Dataset', data=all_results)\n",
    "plt.title('Model Performance (R²) Across Datasets', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('R-squared', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_r2.png')\n",
    "plt.close()\n",
    "\n",
    "# MAE comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Model', y='MAE', hue='Dataset', data=all_results)\n",
    "plt.title('Model Error (MAE) Across Datasets', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Mean Absolute Error', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_mae.png')\n",
    "plt.close()\n",
    "\n",
    "# RMSE comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Model', y='RMSE', hue='Dataset', data=all_results)\n",
    "plt.title('Model Error (RMSE) Across Datasets', fontsize=16)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Root Mean Squared Error', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_rmse.png')\n",
    "plt.close()\n",
    "\n",
    "# Create heatmap of R² values\n",
    "plt.figure(figsize=(12, 8))\n",
    "heatmap_data = all_results.pivot_table(index='Model', columns='Dataset', values='R-squared')\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.3f', linewidths=.5)\n",
    "plt.title('Model Performance (R²) Heatmap Across Datasets', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/model_performance_heatmap.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a9abb6-7a10-4cfd-b199-ed16a6030e93",
   "metadata": {},
   "source": [
    "The model performance was evaluated across three datasets: the NYC training data (2024 Q4), a temporally shifted NYC dataset (2024 Q1), and a geographically distinct dataset from Rhode Island. The results reveal several key insights regarding model accuracy, generalization, and robustness.\n",
    "\n",
    "First, tree-based models (LightGBM and Random Forest) consistently outperform other models across all datasets. LightGBM achieved the highest R-squared (0.7752 on training, 0.6523 on NYC Q1, 0.4256 on Rhode Island) and the lowest prediction errors (MAE and RMSE), confirming its strong predictive power and generalization capability. Random Forest followed closely, while linear models (OLS, LASSO) and KNN lagged behind, particularly in cross-domain settings.\n",
    "\n",
    "Second, performance declines are observed when models are applied to new data, especially across geographic domains. Compared to the training set, the NYC Q1 dataset resulted in an average R² drop of approximately 20.85% and an MAE increase of 25.23%. In contrast, the Rhode Island dataset caused a much more significant degradation, with R² dropping by over 50% and MAE increasing by over 60%. This indicates that spatial heterogeneity has a stronger negative impact on model performance than temporal drift, likely due to location-specific features and market conditions.\n",
    "\n",
    "Third, the relative model rankings remain unchanged across datasets, suggesting the robustness of model selection. Notably, while KNN and LASSO offer fast training times, they suffer from poor generalization and high error rates, making them less suitable for deployment.\n",
    "\n",
    "Lastly, LightGBM offers an excellent trade-off between performance and computational efficiency, with fast training and inference times, making it a strong candidate for real-world implementation, especially in dynamic markets like Airbnb.\n",
    "\n",
    "In summary, tree-based models demonstrate strong predictive accuracy and robustness, but cross-geographic deployment requires caution due to substantial performance deterioration. Future work could incorporate spatial features or hierarchical modeling to better handle regional variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ecea0-e3e4-46a1-9a38-5b3a75819448",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This study presents a comprehensive end-to-end machine learning pipeline for Airbnb price prediction, encompassing rigorous data cleaning, domain-informed feature engineering, robust model training, and cross-dataset evaluation.\n",
    "\n",
    "### Key findings:\n",
    "\n",
    "Tree-based models, especially LightGBM, consistently outperformed other approaches in both accuracy and generalization, achieving an R² of 0.775 on the NYC training set and maintaining strong performance across temporal and geographic shifts.\n",
    "Through rich feature engineering, including amenities extraction, spatial features, and review aggregation, the model was able to capture both structural and contextual determinants of pricing.\n",
    "Model interpretation via feature importance revealed that location, room type, and host experience were among the most influential drivers of price, with different models leveraging different aspects of the data.\n",
    "Performance degraded under domain shift, with larger drops observed for geographic transfer (Rhode Island) than for temporal shift (NYC Q1), confirming the need for careful generalization testing in real-world applications.\n",
    "Overall, the results demonstrate the power of modern machine learning techniques in capturing the complexity of real estate pricing in peer-to-peer rental markets. The framework developed here can be extended to other cities, time periods, and property platforms. Future work could incorporate causal inference, real-time pricing tools, or personalized pricing strategies based on guest preferences and behavioral signals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
